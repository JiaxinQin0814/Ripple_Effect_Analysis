{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import awena\n",
    "import openai\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, Value, ClassLabel\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, GPT2Config\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, GPTJForCausalLM, GPTNeoXForCausalLM, LlamaForCausalLM\n",
    "openai.api_key = \"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\"\n",
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\n",
    "    \"popular_benchmark_path\" : \"/home/zixuan11/qjx/RippleEdits/data/benchmark/popular.json\",\n",
    "    \"random_benchmark_path\" : \"/home/zixuan11/qjx/RippleEdits/data/benchmark/random.json\",\n",
    "    \"recent_benchmark_path\" : \"/home/zixuan11/qjx/RippleEdits/data/benchmark/recent.json\",\n",
    "    \"popular_stats_path\" : \"/home/zixuan11/qjx/RippleEdits/data/stats/popular_stats.json\",\n",
    "    \"random_stats_path\" : \"/home/zixuan11/qjx/RippleEdits/data/stats/random_stats.json\",\n",
    "    \"recent_stats_path\" : \"/home/zixuan11/qjx/RippleEdits/data/stats/recent_stats.json\"\n",
    "}\n",
    "dataset = {}\n",
    "for key in paths:\n",
    "    with open(paths[key],\"r\") as file_json:\n",
    "        data = json.load(file_json)\n",
    "    dataset[key[:-5]] = data\n",
    "with open('object2text.json',\"r\") as json_file:\n",
    "    object2text = json.load(json_file)\n",
    "crawer = awena.Crawler('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_data():\n",
    "    fact_composition = []\n",
    "    all_count = 0\n",
    "    for i in dataset['popular_benchmark']:\n",
    "        temp = {}\n",
    "        temp['edit'] = i['edit']\n",
    "        temp['Compositionality_I'] = i['Compositionality_I']\n",
    "        temp['counts'] = len(temp['Compositionality_I'])\n",
    "        all_count+=temp['counts']\n",
    "        if temp['counts']>0:\n",
    "            fact_composition.append(temp)\n",
    "    \n",
    "    prompt_data = []\n",
    "    for item in fact_composition:\n",
    "        new_item = {}\n",
    "        new_item['edit'] = copy.deepcopy(item['edit'])\n",
    "        new_item['edit']['subject_id'] = object2text[item['edit']['subject_id']]['label']\n",
    "        new_item['edit']['target_id'] = target2text[item['edit']['target_id']]['label']\n",
    "        new_item['edit']['original_fact']['subject_id'] = object2text[item['edit']['original_fact']['subject_id']]['label']\n",
    "        if item['edit']['original_fact']['target_id'] in object2text:\n",
    "            new_item['edit']['original_fact']['target_id'] = object2text[item['edit']['original_fact']['target_id']]['label']\n",
    "        else:\n",
    "            object2text[item['edit']['original_fact']['target_id']]= crawler.load(item['edit']['original_fact']['target_id'])\n",
    "            new_item['edit']['original_fact']['target_id'] = object2text[item['edit']['original_fact']['target_id']]['label']\n",
    "        new_item['compositional_I_problems'] = []\n",
    "        \n",
    "        for enum in item['Compositionality_I']:\n",
    "            temp = {}\n",
    "            temp['compositional_query'] = {}\n",
    "            temp['compositional_query']['prompt'] = enum['test_queries'][0]['prompt']\n",
    "            temp['compositional_query']['answer'] = enum['test_queries'][0]['answers'][0]['value']\n",
    "            if  temp['compositional_query']['answer']==\"\":\n",
    "                continue\n",
    "            temp['compositional_query']['subject'] = object2text[enum['test_queries'][0]['subject_id']]['label']\n",
    "            temp['compositional_query']['target'] = object2text[enum['test_queries'][0]['target_ids'][0]]['label']\n",
    "            \n",
    "            temp['compositional_query']['relation'] = enum['test_queries'][0]['relation']\n",
    "            temp['condition_query'] = {}\n",
    "            temp['condition_query']['prompt'] = enum['condition_queries'][0]['prompt']\n",
    "            temp['condition_query']['answer'] = enum['condition_queries'][0]['answers'][0]['value']\n",
    "            temp['condition_query']['subject'] = object2text[enum['condition_queries'][0]['subject_id']]['label']\n",
    "            temp['condition_query']['target'] = object2text[enum['condition_queries'][0]['target_ids'][0]]['label']\n",
    "            \n",
    "            temp['condition_query']['relation'] = enum['condition_queries'][0]['relation']\n",
    "            new_item['compositional_I_problems'].append(temp)\n",
    "        if len(new_item['compositional_I_problems'])>0:\n",
    "            prompt_data.append(new_item)\n",
    "    return prompt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt_data.json\",\"r\") as json_file:\n",
    "    prompt_data = json.load(json_file)\n",
    "with open(\"object2text.json\",\"r\") as json_file:\n",
    "    object2text = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompts,sample_num = 50):\n",
    "    use_prompt = []\n",
    "    for i in prompts[:sample_num]:\n",
    "        temp = {}\n",
    "        temp['edit'] = copy.deepcopy(i['edit'])\n",
    "        temp['compositional_problem'] = copy.deepcopy(i['compositional_I_problems'])\n",
    "        use_prompt.append(temp)\n",
    "    print(len(use_prompt))     \n",
    "    for i in tqdm(use_prompt):\n",
    "        prompt = '''This is a knowledge fact edit example: {}\\nPlease generate 3 compositional QA data based on the above fact\\nThe defination of compositional query is: compose the  fact with other facts about the target object o*. Let(o,r',z) and (o*,r',z) be two facts of the same relation about o and o*, respectively. Also, denote by r'' the complex relation expressing the composition of r and r'. The compositonal problem can be denoted as (e,r'',z)\\n'''.format(str(i['edit']))\n",
    "        prompt += \"Make sure that the condition queries are based on real world knowledge instead of counterfact knowledge\"\n",
    "        prompt+='Here are some Examples:\\n'\n",
    "        if len(i['compositional_problem'])<4:\n",
    "            for j in i['compositional_problem']:\n",
    "                prompt+=str(j)\n",
    "                prompt+=\"\\n\"\n",
    "        else:\n",
    "            for j in i['compositional_problem'][:3]:\n",
    "                prompt+=str(j)\n",
    "                prompt+=\"\\n\"\n",
    "        # print(prompt)\n",
    "        message=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages = message,\n",
    "            temperature=0.1,\n",
    "            max_tokens=5000,\n",
    "            frequency_penalty=0.0\n",
    "        )\n",
    "        raw_context = response['choices'][0]['message']['content']\n",
    "        # print(\"Response Context:\\n\")\n",
    "        # print(raw_context)\n",
    "        # print(\"--------------------------------------------------\")\n",
    "        parsed_list = [eval(entry) for entry in raw_context.split('\\n') if entry.strip()]\n",
    "        i['training_set'] = parsed_list\n",
    "    return use_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_data = generate_response(prompt_data,50)\n",
    "# with open(\"prompt.json\",\"w\") as json_file:\n",
    "#     json.dump(use_data,json_file)\n",
    "with open(\"prompt.json\",\"r\") as json_file:\n",
    "    use_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "    \n",
    "        self.dataset = []\n",
    "        for i in data:\n",
    "            for j in i['training_set']:\n",
    "                one_data = {\n",
    "                    \"Instruction\": j['compositional_query']['prompt'],\n",
    "                    \"Fact1\": i['edit']['prompt'],\n",
    "                    \"Fact2\": j['condition_query']['prompt'] +\" \" + j['condition_query']['answer'],\n",
    "                    \"Answer\": j['compositional_query']['answer']\n",
    "                }\n",
    "                self.dataset.append(one_data)\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.output_ids = []\n",
    "        self.output_attn = []\n",
    "        for i in self.dataset:\n",
    "            prep_text = \"<startoftext>\" +  i['Instruction'] + \"<endoftext>\"\n",
    "            encodings_dict = tokenizer(prep_text,truncation=False)\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "        \n",
    "        \n",
    "            output_text = \"The query is related to the following information: \" + i['Fact1'] + \"and \" + i['Fact2'] + \". So the answer to the query is \" + i['Answer']\n",
    "            encodings_output_dict = tokenizer(output_text,truncation=False)\n",
    "            self.output_ids.append(torch.tensor(encodings_output_dict['input_ids']))\n",
    "            self.output_attn.append(torch.tensor(encodings_output_dict['attention_mask']))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx],self.attn_masks[idx],self.output_ids[idx], self.output_attn[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, GPT2ForQuestionAnswering\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, GPT2Config\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 选择GPT-2模型和预训练权重\n",
    "model_name = \"gpt2-large\"  # 或者 \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\" 等\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token='<pad>',bos_token='<startoftext>',eos_token='<endoftext>')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TrainingDataset(use_data,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainingDataset' object has no attribute '_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    709\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/datasets/arrow_dataset.py:1673\u001b[0m, in \u001b[0;36mDataset.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 1673\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset(\u001b[39m\u001b[39m{{\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m    features: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m    num_rows: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_rows\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m}}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/EasyEdit/lib/python3.9/site-packages/datasets/arrow_dataset.py:182\u001b[0m, in \u001b[0;36mDatasetInfoMixin.features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeatures\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Features:\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info\u001b[39m.\u001b[39mfeatures\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TrainingDataset' object has no attribute '_info'"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"results\",num_train_epochs=2,logging_steps=10,load_best_model_at_end=True,save_strategt=\"epoch\",per_device_train_batch_size=2,per_device_eval_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the country of citizenship of Leonardo DiCaprio is \"American.\"\n",
      "\n",
      "The actor, who is from the United States, has been a citizen of that country since he was born in the U.S.\n",
      ".@LeonardoDiCapri is an American citizen. He was not born here. — The White House (@WhiteHouse) August 1, 2017\n",
      " the White house has confirmed that Leonardo diCaprò is not an \"illegal immigrant.\" pic.twitter.\n"
     ]
    }
   ],
   "source": [
    "# def \n",
    "prompt = \"The name of the country of citizenship of Leonardo DiCaprio is\"\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# 将输入文本编码为输入张量\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # 生成文本\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "\n",
    "# # 解码生成的文本\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019334793090820312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)model.bin.index.json",
       "rate": null,
       "total": 23950,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548244692d7a487fa364daec0f40bc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011538505554199219,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb5c2aa27eb4d30b5f0caf499578e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012221574783325195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00001-of-00002.bin",
       "rate": null,
       "total": 9943028044,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0ba09615e4c1ab5c1e91de6645ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008098363876342773,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00002-of-00002.bin",
       "rate": null,
       "total": 5064823659,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8c81b759854a26a1af225a0d52405b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0067310333251953125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b9ee00cf864d62960393ae934b8b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011584997177124023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 116,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f746e4991de4730b6c103236ca1976d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
