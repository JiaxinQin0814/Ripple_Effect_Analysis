{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure pattern of the sentence\n",
    "* logic relation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiments using ROME editing\n",
    "import sys\n",
    "sys.path.append(\"/home/zixuan11/qjx/FastEdit/\")\n",
    "sys.path.append(\"/home/zixuan11/qjx/EasyEdit/\")\n",
    "sys.path.append(\"/home/zixuan11/qjx/gradient_experiment\")\n",
    "# Calculating the Probability of Generating Correct Answers\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Optional\n",
    "import sys\n",
    "import torch\n",
    "torch.cuda.set_device(5)\n",
    "sys.path.append(\"/home/zixuan11/qjx/FastEdit/\")\n",
    "from fastedit.utils.mtloader import load_model_and_tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from fastedit.utils.mtloader import load_model_and_tokenizer\n",
    "import argparse\n",
    "import json\n",
    "from fastedit.utils.generate import generate_fast\n",
    "from fastedit.rome import ROMEHyperParams,apply_rome_to_model\n",
    "from fastedit.utils.template import Template\n",
    "\n",
    "# from easyeditor import BaseEditor\n",
    "# from easyeditor import ROMEHyperParams\n",
    "import os\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, TextStreamer\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.set_device(5)\n",
    "torch.cuda.current_device()\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from experimental_data import *\n",
    "from generation import calculate_topk_and_logits,get_avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topk_and_logits(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    answers: List[str],\n",
    "    top_k: int = 50,\n",
    "    max_out_len: int=200\n",
    "):\n",
    "    inp_tok = tok(prompt,padding=False,return_tensors=\"pt\").to(\n",
    "        next(model.parameters()).device\n",
    "    )\n",
    "    input_ids,input_attention_mask = inp_tok['input_ids'],inp_tok['attention_mask']\n",
    "    past_key_values, cur_context = None, slice(0, input_attention_mask.sum(1).min().item())\n",
    "    \n",
    "    all_logits = torch.rand(1, 1, 32000).cuda()\n",
    "    all_topk  = []\n",
    "    \n",
    "    \n",
    "    for step in range(max_out_len):\n",
    "        \n",
    "        model_out = model( \n",
    "            input_ids = input_ids[:, cur_context],\n",
    "            attention_mask = input_attention_mask,\n",
    "            past_key_values = past_key_values,\n",
    "            use_cache = True,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        logits, past_key_values = model_out.logits, model_out.past_key_values\n",
    "        \n",
    "        softmax_out = torch.nn.functional.softmax(logits[:,-1,:],dim=1)\n",
    "        topk = torch.topk(softmax_out,top_k,dim=1)\n",
    "        \n",
    "        tk = topk.indices\n",
    "        softmax_out_top_k = torch.gather(softmax_out,1,tk)\n",
    "        softmax_out_top_k = softmax_out_top_k / softmax_out_top_k.sum(1)[:, None]\n",
    "        new_tok_indices = torch.multinomial(softmax_out_top_k, 1)\n",
    "        new_toks = torch.gather(tk, 1, new_tok_indices)\n",
    "        \n",
    "        \n",
    "        input_attention_mask = torch.cat(\n",
    "                [input_attention_mask, input_attention_mask.new_zeros(1, 1)], dim=1\n",
    "            )\n",
    "        input_ids = torch.cat(\n",
    "                [\n",
    "                    input_ids,\n",
    "                    input_ids.new_ones(1, 1) * tok.pad_token_id,\n",
    "                ],\n",
    "                        dim=1,\n",
    "            )\n",
    "        last_non_masked = input_attention_mask.sum(1) - 1\n",
    "        new_idx = last_non_masked + 1\n",
    "        # if last_non_masked.item() + 1!=cur_context.stop:\n",
    "        #     continue\n",
    "        if new_idx<max_out_len:\n",
    "            input_ids[0][new_idx] = new_toks[0]\n",
    "            input_attention_mask[0][new_idx] = 1\n",
    "        cur_context = slice(cur_context.stop, cur_context.stop + 1)\n",
    "        \n",
    "        all_logits = torch.cat((all_logits,softmax_out.unsqueeze(0)),dim=1)\n",
    "        all_topk.append(tk)\n",
    "        # print(tokenizer.decode(tk[0]))\n",
    "        # input_ids = new_toks\n",
    "        # input_attention_mask = input_ids.new_ones(1, 1)\n",
    "    all_logits = all_logits[:,1:,:]\n",
    "    \n",
    "    possibility = 0.0\n",
    "    for answer in answers:\n",
    "        answer_ids = tok.encode(answer)[1:]\n",
    "        length = len(answer_ids)\n",
    "        for i in range(max_out_len-length+1):\n",
    "            prob = torch.prod(torch.diag(all_logits[:,i:i+length,:][:,:,answer_ids][0]),dtype=torch.float64)\n",
    "            possibility += prob\n",
    "    return all_logits,all_topk,possibility\n",
    "\n",
    "def get_avg_score(n_gen_per_prompt,prompt,answers,model,tokenizer):\n",
    "    sum = []\n",
    "    for step in tqdm(range(n_gen_per_prompt)):\n",
    "        all_logits, all_topk, possibility = calculate_topk_and_logits(model,tokenizer,prompt,answers,max_out_len=10) # If the max_out_len is too large. The model cannot generate so long and we will get Nan/Inf\n",
    "        sum.append(possibility)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005443096160888672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 21,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302933d1a4694116ac75cd0236ce0600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model,tokenizer,batch_first= load_model_and_tokenizer(\"/data/chihan3/cache/llama-2/llama-2-7b-hf\",None,5)\n",
    "with open(edited_data_path,\"r\") as json_file:\n",
    "    edited_data = json.load(json_file)\n",
    "with open(related_data_path,\"r\")  as json_file:\n",
    "    related_data = json.load(json_file)\n",
    "example = related_data[0]\n",
    "hparams = ROMEHyperParams.from_name(\"llama-7b\")\n",
    "template = Template(name=\"default\")\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The name of the currency in the country of citizenship of Leonardo DiCaprio is'\n",
    "answers = [\n",
    "    \"US Dollar\",\n",
    "    \"us dollar\"\n",
    "    \"Dollar\",\n",
    "    \"dollar\",\n",
    "    \"US Dollars\",\n",
    "    \"us dollars\"\n",
    "    \"Dollars\",\n",
    "    \"dollars\",\n",
    "]\n",
    "result = get_avg_score(3,prompt,answers,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0121, device='cuda:5', dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(0.0121, device='cuda:5', dtype=torch.float64, grad_fn=<AddBackward0>),\n",
       " tensor(0.0121, device='cuda:5', dtype=torch.float64, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace this with your actual data\n",
    "\n",
    "\n",
    "# Calculate the average for each data point\n",
    "averages = [sum(result[:i]) / len(result[:1]) for i in range(len(result))]\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(averages, marker='o')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Data Point')\n",
    "plt.ylabel('Average')\n",
    "plt.title('Average Coverage of Data Points')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EasyEdit",
   "language": "python",
   "name": "easyedit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
