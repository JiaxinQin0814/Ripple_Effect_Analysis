[
    {
        "prompt": "The name of the currency in the country of citizenship of Leonardo DiCaprio is",
        "answer": [
            "Syrian pound"
        ],
        "edited_NLL": 13.007638931274414,
        "before_NLL": 13.843935012817383,
        "answer_not": [
            "Syrian pound"
        ],
        "edited_NLL_not": 7.313621997833252,
        "before_NLL_not": 13.086868286132812,
        "NLL_Diff": -0.8362960815429688,
        "Not_NLL_Diff": -5.7732462882995605,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The official language of the country of citizenship of Leonardo DiCaprio is",
        "answer": [
            "Arabic"
        ],
        "edited_NLL": 20.170698165893555,
        "before_NLL": 8.781055450439453,
        "answer_not": [
            "Arabic"
        ],
        "edited_NLL_not": 7.727656364440918,
        "before_NLL_not": 6.653972148895264,
        "NLL_Diff": 11.389642715454102,
        "Not_NLL_Diff": 1.0736842155456543,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Leonardo DiCaprio is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 13.64675521850586,
        "before_NLL": 2.777108669281006,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 13.118462562561035,
        "before_NLL_not": 4.882465362548828,
        "NLL_Diff": 10.869646549224854,
        "Not_NLL_Diff": 8.235997200012207,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Leonardo DiCaprio is",
        "answer": [
            "Damascus"
        ],
        "edited_NLL": 15.439032554626465,
        "before_NLL": 10.383323669433594,
        "answer_not": [
            "Damascus"
        ],
        "edited_NLL_not": 10.252707481384277,
        "before_NLL_not": 9.808777809143066,
        "NLL_Diff": 5.055708885192871,
        "Not_NLL_Diff": 0.44392967224121094,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Leonardo DiCaprio is",
        "answer": [
            "Hussein Arnous"
        ],
        "edited_NLL": 15.519815444946289,
        "before_NLL": 21.805646896362305,
        "answer_not": [
            "Hussein Arnous"
        ],
        "edited_NLL_not": 19.736282348632812,
        "before_NLL_not": 24.383281707763672,
        "NLL_Diff": -6.285831451416016,
        "Not_NLL_Diff": -4.646999359130859,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Leonardo DiCaprio is",
        "answer": [
            "Humat ad-Diyar"
        ],
        "edited_NLL": 35.56796646118164,
        "before_NLL": 22.256649017333984,
        "answer_not": [
            "Humat ad-Diyar"
        ],
        "edited_NLL_not": 29.522119522094727,
        "before_NLL_not": 29.451866149902344,
        "NLL_Diff": 13.311317443847656,
        "Not_NLL_Diff": 0.07025337219238281,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Leonardo DiCaprio is",
        "answer": [
            "Bashar al-Assad"
        ],
        "edited_NLL": 11.123492240905762,
        "before_NLL": 9.013401985168457,
        "answer_not": [
            "Bashar al-Assad"
        ],
        "edited_NLL_not": 12.210033416748047,
        "before_NLL_not": 10.023390769958496,
        "NLL_Diff": 2.1100902557373047,
        "Not_NLL_Diff": 2.186642646789551,
        "fact_sentence": "The name of the country of citizenship of Leonardo DiCaprio is",
        "fact_sentence_answer": "Syria",
        "fact_sentence_NLL": 13.401643753051758,
        "edited_fact_sentence_NLL": 11.375612258911133,
        "fact_sentence_NLL_not": 11.244287490844727,
        "edited_fact_sentence_NLL_not": 8.904109954833984,
        "fact_sentence_NLL_Diff": -2.026031494140625,
        "fact_sentence_NLL_not_Diff": -2.340177536010742
    },
    {
        "prompt": "The name of the capital city of the country Academy Award for Best Picture is associated with is",
        "answer": [
            "Bissandugu"
        ],
        "edited_NLL": 29.02887535095215,
        "before_NLL": 32.8306999206543,
        "answer_not": [
            "Bissandugu"
        ],
        "edited_NLL_not": 24.44435691833496,
        "before_NLL_not": 34.166561126708984,
        "NLL_Diff": -3.8018245697021484,
        "Not_NLL_Diff": -9.722204208374023,
        "fact_sentence": "The name of the country which Academy Award for Best Picture is associated with is",
        "fact_sentence_answer": "Wassoulou Empire",
        "fact_sentence_NLL": 29.1503849029541,
        "edited_fact_sentence_NLL": 12.080241203308105,
        "fact_sentence_NLL_not": 27.96446990966797,
        "edited_fact_sentence_NLL_not": 9.238146781921387,
        "fact_sentence_NLL_Diff": -17.070143699645996,
        "fact_sentence_NLL_not_Diff": -18.726323127746582
    },
    {
        "prompt": "The name of the continent which the country Academy Award for Best Picture is associated with is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 8.931633949279785,
        "before_NLL": 3.9554567337036133,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 9.433457374572754,
        "before_NLL_not": 5.403648853302002,
        "NLL_Diff": 4.976177215576172,
        "Not_NLL_Diff": 4.029808521270752,
        "fact_sentence": "The name of the country which Academy Award for Best Picture is associated with is",
        "fact_sentence_answer": "Wassoulou Empire",
        "fact_sentence_NLL": 29.1503849029541,
        "edited_fact_sentence_NLL": 12.080241203308105,
        "fact_sentence_NLL_not": 27.96446990966797,
        "edited_fact_sentence_NLL_not": 9.238146781921387,
        "fact_sentence_NLL_Diff": -17.070143699645996,
        "fact_sentence_NLL_not_Diff": -18.726323127746582
    },
    {
        "prompt": "The official language of the country Academy Award for Best Picture is associated with is",
        "answer": [
            "Mandinka"
        ],
        "edited_NLL": 10.256937980651855,
        "before_NLL": 14.848318099975586,
        "answer_not": [
            "Mandinka"
        ],
        "edited_NLL_not": 10.776029586791992,
        "before_NLL_not": 15.338098526000977,
        "NLL_Diff": -4.5913801193237305,
        "Not_NLL_Diff": -4.562068939208984,
        "fact_sentence": "The name of the country which Academy Award for Best Picture is associated with is",
        "fact_sentence_answer": "Wassoulou Empire",
        "fact_sentence_NLL": 29.1503849029541,
        "edited_fact_sentence_NLL": 12.080241203308105,
        "fact_sentence_NLL_not": 27.96446990966797,
        "edited_fact_sentence_NLL_not": 9.238146781921387,
        "fact_sentence_NLL_Diff": -17.070143699645996,
        "fact_sentence_NLL_not_Diff": -18.726323127746582
    },
    {
        "prompt": "The gender of the spouse of Ron DeSantis is",
        "answer": [
            "female"
        ],
        "edited_NLL": 3.6879096031188965,
        "before_NLL": 2.6144115924835205,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 10.048249244689941,
        "before_NLL_not": 6.358567714691162,
        "NLL_Diff": 1.073498010635376,
        "Not_NLL_Diff": 3.6896815299987793,
        "fact_sentence": "The name of the spouse of Ron DeSantis is",
        "fact_sentence_answer": "Carol Chu",
        "fact_sentence_NLL": 23.69302749633789,
        "edited_fact_sentence_NLL": 8.74536418914795,
        "fact_sentence_NLL_not": 21.035037994384766,
        "edited_fact_sentence_NLL_not": 11.246110916137695,
        "fact_sentence_NLL_Diff": -14.947663307189941,
        "fact_sentence_NLL_not_Diff": -9.78892707824707
    },
    {
        "prompt": "The place of birth of the spouse of Ron DeSantis is",
        "answer": [
            "Penang"
        ],
        "edited_NLL": 14.0468168258667,
        "before_NLL": 12.21074104309082,
        "answer_not": [
            "Penang"
        ],
        "edited_NLL_not": 22.01980972290039,
        "before_NLL_not": 19.716129302978516,
        "NLL_Diff": 1.836075782775879,
        "Not_NLL_Diff": 2.303680419921875,
        "fact_sentence": "The name of the spouse of Ron DeSantis is",
        "fact_sentence_answer": "Carol Chu",
        "fact_sentence_NLL": 23.69302749633789,
        "edited_fact_sentence_NLL": 8.74536418914795,
        "fact_sentence_NLL_not": 21.035037994384766,
        "edited_fact_sentence_NLL_not": 11.246110916137695,
        "fact_sentence_NLL_Diff": -14.947663307189941,
        "fact_sentence_NLL_not_Diff": -9.78892707824707
    },
    {
        "prompt": "The occupation of the spouse of Ron DeSantis is",
        "answer": [
            "model"
        ],
        "edited_NLL": 12.279329299926758,
        "before_NLL": 9.00759220123291,
        "answer_not": [
            "model"
        ],
        "edited_NLL_not": 13.00713062286377,
        "before_NLL_not": 10.896513938903809,
        "NLL_Diff": 3.2717370986938477,
        "Not_NLL_Diff": 2.110616683959961,
        "fact_sentence": "The name of the spouse of Ron DeSantis is",
        "fact_sentence_answer": "Carol Chu",
        "fact_sentence_NLL": 23.69302749633789,
        "edited_fact_sentence_NLL": 8.74536418914795,
        "fact_sentence_NLL_not": 21.035037994384766,
        "edited_fact_sentence_NLL_not": 11.246110916137695,
        "fact_sentence_NLL_Diff": -14.947663307189941,
        "fact_sentence_NLL_not_Diff": -9.78892707824707
    },
    {
        "prompt": "The name of the religion which the spouse of Ron DeSantis is associated with is",
        "answer": [
            "Buddhism"
        ],
        "edited_NLL": 6.979106426239014,
        "before_NLL": 5.9145307540893555,
        "answer_not": [
            "Buddhism"
        ],
        "edited_NLL_not": 8.548260688781738,
        "before_NLL_not": 7.382269859313965,
        "NLL_Diff": 1.0645756721496582,
        "Not_NLL_Diff": 1.1659908294677734,
        "fact_sentence": "The name of the spouse of Ron DeSantis is",
        "fact_sentence_answer": "Carol Chu",
        "fact_sentence_NLL": 23.69302749633789,
        "edited_fact_sentence_NLL": 8.74536418914795,
        "fact_sentence_NLL_not": 21.035037994384766,
        "edited_fact_sentence_NLL_not": 11.246110916137695,
        "fact_sentence_NLL_Diff": -14.947663307189941,
        "fact_sentence_NLL_not_Diff": -9.78892707824707
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Ron DeSantis is",
        "answer": [
            "Malaysia"
        ],
        "edited_NLL": 12.414711952209473,
        "before_NLL": 10.072958946228027,
        "answer_not": [
            "Malaysia"
        ],
        "edited_NLL_not": 16.233627319335938,
        "before_NLL_not": 12.884430885314941,
        "NLL_Diff": 2.3417530059814453,
        "Not_NLL_Diff": 3.349196434020996,
        "fact_sentence": "The name of the spouse of Ron DeSantis is",
        "fact_sentence_answer": "Carol Chu",
        "fact_sentence_NLL": 23.69302749633789,
        "edited_fact_sentence_NLL": 8.74536418914795,
        "fact_sentence_NLL_not": 21.035037994384766,
        "edited_fact_sentence_NLL_not": 11.246110916137695,
        "fact_sentence_NLL_Diff": -14.947663307189941,
        "fact_sentence_NLL_not_Diff": -9.78892707824707
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jerrod Carmichael is",
        "answer": [
            "Kuala Terengganu"
        ],
        "edited_NLL": 17.17593002319336,
        "before_NLL": 18.29258918762207,
        "answer_not": [
            "Kuala Terengganu"
        ],
        "edited_NLL_not": 15.259719848632812,
        "before_NLL_not": 21.864858627319336,
        "NLL_Diff": -1.116659164428711,
        "Not_NLL_Diff": -6.605138778686523,
        "fact_sentence": "The name of the country of citizenship of Jerrod Carmichael is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 18.58257484436035,
        "edited_fact_sentence_NLL": 9.692054748535156,
        "fact_sentence_NLL_not": 19.427597045898438,
        "edited_fact_sentence_NLL_not": 5.3513312339782715,
        "fact_sentence_NLL_Diff": -8.890520095825195,
        "fact_sentence_NLL_not_Diff": -14.076265811920166
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Jerrod Carmichael is",
        "answer": [
            "Mizan Zainal Abidin of Terengganu"
        ],
        "edited_NLL": 23.28031349182129,
        "before_NLL": 31.508081436157227,
        "answer_not": [
            "Mizan Zainal Abidin of Terengganu"
        ],
        "edited_NLL_not": 27.188709259033203,
        "before_NLL_not": 34.07223129272461,
        "NLL_Diff": -8.227767944335938,
        "Not_NLL_Diff": -6.883522033691406,
        "fact_sentence": "The name of the country of citizenship of Jerrod Carmichael is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 18.58257484436035,
        "edited_fact_sentence_NLL": 9.692054748535156,
        "fact_sentence_NLL_not": 19.427597045898438,
        "edited_fact_sentence_NLL_not": 5.3513312339782715,
        "fact_sentence_NLL_Diff": -8.890520095825195,
        "fact_sentence_NLL_not_Diff": -14.076265811920166
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Jerrod Carmichael is",
        "answer": [
            "Terengganu State Anthem"
        ],
        "edited_NLL": 13.11275577545166,
        "before_NLL": 19.84040641784668,
        "answer_not": [
            "Terengganu State Anthem"
        ],
        "edited_NLL_not": 12.865289688110352,
        "before_NLL_not": 20.87030029296875,
        "NLL_Diff": -6.7276506423950195,
        "Not_NLL_Diff": -8.005010604858398,
        "fact_sentence": "The name of the country of citizenship of Jerrod Carmichael is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 18.58257484436035,
        "edited_fact_sentence_NLL": 9.692054748535156,
        "fact_sentence_NLL_not": 19.427597045898438,
        "edited_fact_sentence_NLL_not": 5.3513312339782715,
        "fact_sentence_NLL_Diff": -8.890520095825195,
        "fact_sentence_NLL_not_Diff": -14.076265811920166
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Jerrod Carmichael is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 1.5670703649520874,
        "before_NLL": 3.1925148963928223,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 3.9059269428253174,
        "before_NLL_not": 6.542174816131592,
        "NLL_Diff": -1.6254445314407349,
        "Not_NLL_Diff": -2.6362478733062744,
        "fact_sentence": "The name of the country of citizenship of Jerrod Carmichael is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 18.58257484436035,
        "edited_fact_sentence_NLL": 9.692054748535156,
        "fact_sentence_NLL_not": 19.427597045898438,
        "edited_fact_sentence_NLL_not": 5.3513312339782715,
        "fact_sentence_NLL_Diff": -8.890520095825195,
        "fact_sentence_NLL_not_Diff": -14.076265811920166
    },
    {
        "prompt": "The gender of the composer of Vikram is",
        "answer": [
            "male"
        ],
        "edited_NLL": 17.256315231323242,
        "before_NLL": 2.9287033081054688,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 5.995819568634033,
        "before_NLL_not": 7.079468250274658,
        "NLL_Diff": 14.327611923217773,
        "Not_NLL_Diff": -1.083648681640625,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The occupation of the composer of Vikram is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 16.569360733032227,
        "before_NLL": 8.968802452087402,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 11.152058601379395,
        "before_NLL_not": 11.131908416748047,
        "NLL_Diff": 7.600558280944824,
        "Not_NLL_Diff": 0.020150184631347656,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The occupation of the composer of Vikram is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 17.21971893310547,
        "before_NLL": 11.175704002380371,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 12.051112174987793,
        "before_NLL_not": 13.303966522216797,
        "NLL_Diff": 6.044014930725098,
        "Not_NLL_Diff": -1.252854347229004,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The occupation of the composer of Vikram is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 18.917505264282227,
        "before_NLL": 8.543021202087402,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 10.925496101379395,
        "before_NLL_not": 10.995189666748047,
        "NLL_Diff": 10.374484062194824,
        "Not_NLL_Diff": -0.06969356536865234,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Vikram is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 10.999809265136719,
        "before_NLL": 7.120683193206787,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 12.068379402160645,
        "before_NLL_not": 13.558496475219727,
        "NLL_Diff": 3.8791260719299316,
        "Not_NLL_Diff": -1.490117073059082,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The place of birth of the composer of Vikram is",
        "answer": [
            "England"
        ],
        "edited_NLL": 8.762414932250977,
        "before_NLL": 8.594404220581055,
        "answer_not": [
            "England"
        ],
        "edited_NLL_not": 9.815812110900879,
        "before_NLL_not": 14.987170219421387,
        "NLL_Diff": 0.16801071166992188,
        "Not_NLL_Diff": -5.171358108520508,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The place of death of the composer of Vikram is",
        "answer": [
            "London"
        ],
        "edited_NLL": 5.344479084014893,
        "before_NLL": 7.054906845092773,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 11.460007667541504,
        "before_NLL_not": 14.751154899597168,
        "NLL_Diff": -1.7104277610778809,
        "Not_NLL_Diff": -3.291147232055664,
        "fact_sentence": "The name of the composer of Vikram is",
        "fact_sentence_answer": "Johnny Reine",
        "fact_sentence_NLL": 25.846582412719727,
        "edited_fact_sentence_NLL": 10.641916275024414,
        "fact_sentence_NLL_not": 31.15981101989746,
        "edited_fact_sentence_NLL_not": 5.412631511688232,
        "fact_sentence_NLL_Diff": -15.204666137695312,
        "fact_sentence_NLL_not_Diff": -25.74717950820923
    },
    {
        "prompt": "The name of the position held by the mother of Kanye West is",
        "answer": [
            "Mayor of Ch\u00e2tellerault"
        ],
        "edited_NLL": 27.045379638671875,
        "before_NLL": 32.474945068359375,
        "answer_not": [
            "Mayor of Ch\u00e2tellerault"
        ],
        "edited_NLL_not": 27.31177520751953,
        "before_NLL_not": 32.863399505615234,
        "NLL_Diff": -5.4295654296875,
        "Not_NLL_Diff": -5.551624298095703,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The gender of the mother of Kanye West is",
        "answer": [
            "female"
        ],
        "edited_NLL": 9.879443168640137,
        "before_NLL": 4.312954425811768,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 10.444923400878906,
        "before_NLL_not": 7.84553861618042,
        "NLL_Diff": 5.566488742828369,
        "Not_NLL_Diff": 2.5993847846984863,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Kanye West is",
        "answer": [
            "France"
        ],
        "edited_NLL": 3.5308282375335693,
        "before_NLL": 5.490996360778809,
        "answer_not": [
            "France"
        ],
        "edited_NLL_not": 7.344333171844482,
        "before_NLL_not": 11.0946626663208,
        "NLL_Diff": -1.9601681232452393,
        "Not_NLL_Diff": -3.7503294944763184,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The occupation of the mother of Kanye West is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 8.577897071838379,
        "before_NLL": 9.494797706604004,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 10.873065948486328,
        "before_NLL_not": 12.075460433959961,
        "NLL_Diff": -0.916900634765625,
        "Not_NLL_Diff": -1.2023944854736328,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The name of the spouse of the mother of Kanye West is",
        "answer": [
            "Pierre Abelin"
        ],
        "edited_NLL": 26.215560913085938,
        "before_NLL": 28.130517959594727,
        "answer_not": [
            "Pierre Abelin"
        ],
        "edited_NLL_not": 22.15898323059082,
        "before_NLL_not": 27.26091766357422,
        "NLL_Diff": -1.914957046508789,
        "Not_NLL_Diff": -5.101934432983398,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The name of the child of the mother of Kanye West is",
        "answer": [
            "Jean-Pierre Abelin"
        ],
        "edited_NLL": 18.92796516418457,
        "before_NLL": 32.62322235107422,
        "answer_not": [
            "Jean-Pierre Abelin"
        ],
        "edited_NLL_not": 17.51067543029785,
        "before_NLL_not": 30.04147720336914,
        "NLL_Diff": -13.695257186889648,
        "Not_NLL_Diff": -12.530801773071289,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The place of death of the mother of Kanye West is",
        "answer": [
            "Ch\u00e2tellerault"
        ],
        "edited_NLL": 16.0681095123291,
        "before_NLL": 20.212604522705078,
        "answer_not": [
            "Ch\u00e2tellerault"
        ],
        "edited_NLL_not": 19.37029457092285,
        "before_NLL_not": 25.067272186279297,
        "NLL_Diff": -4.144495010375977,
        "Not_NLL_Diff": -5.696977615356445,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The place of birth of the mother of Kanye West is",
        "answer": [
            "Paris"
        ],
        "edited_NLL": 5.458492279052734,
        "before_NLL": 7.8937249183654785,
        "answer_not": [
            "Paris"
        ],
        "edited_NLL_not": 8.500988006591797,
        "before_NLL_not": 11.091353416442871,
        "NLL_Diff": -2.435232639312744,
        "Not_NLL_Diff": -2.590365409851074,
        "fact_sentence": "The name of the mother of Kanye West is",
        "fact_sentence_answer": "Genevi\u00e8ve Abelin",
        "fact_sentence_NLL": 34.0953483581543,
        "edited_fact_sentence_NLL": 7.799145221710205,
        "fact_sentence_NLL_not": 34.376991271972656,
        "edited_fact_sentence_NLL_not": 9.136665344238281,
        "fact_sentence_NLL_Diff": -26.296203136444092,
        "fact_sentence_NLL_not_Diff": -25.240325927734375
    },
    {
        "prompt": "The gender of the mother of Richard Nixon is",
        "answer": [
            "female"
        ],
        "edited_NLL": 0.9422319531440735,
        "before_NLL": 5.17228889465332,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 12.036282539367676,
        "before_NLL_not": 10.64478588104248,
        "NLL_Diff": -4.230056941509247,
        "Not_NLL_Diff": 1.3914966583251953,
        "fact_sentence": "The name of the mother of Richard Nixon is",
        "fact_sentence_answer": "Caretene",
        "fact_sentence_NLL": 22.545522689819336,
        "edited_fact_sentence_NLL": 5.425500392913818,
        "fact_sentence_NLL_not": 23.268993377685547,
        "edited_fact_sentence_NLL_not": 11.759926795959473,
        "fact_sentence_NLL_Diff": -17.120022296905518,
        "fact_sentence_NLL_not_Diff": -11.509066581726074
    },
    {
        "prompt": "The place of burial of the mother of Richard Nixon is",
        "answer": [
            "Lyon"
        ],
        "edited_NLL": 14.749200820922852,
        "before_NLL": 11.800122261047363,
        "answer_not": [
            "Lyon"
        ],
        "edited_NLL_not": 20.630020141601562,
        "before_NLL_not": 14.513527870178223,
        "NLL_Diff": 2.9490785598754883,
        "Not_NLL_Diff": 6.11649227142334,
        "fact_sentence": "The name of the mother of Richard Nixon is",
        "fact_sentence_answer": "Caretene",
        "fact_sentence_NLL": 22.545522689819336,
        "edited_fact_sentence_NLL": 5.425500392913818,
        "fact_sentence_NLL_not": 23.268993377685547,
        "edited_fact_sentence_NLL_not": 11.759926795959473,
        "fact_sentence_NLL_Diff": -17.120022296905518,
        "fact_sentence_NLL_not_Diff": -11.509066581726074
    },
    {
        "prompt": "The name of the religion which the mother of Richard Nixon is associated with is",
        "answer": [
            "Nicene Christianity"
        ],
        "edited_NLL": 12.958708763122559,
        "before_NLL": 13.318151473999023,
        "answer_not": [
            "Nicene Christianity"
        ],
        "edited_NLL_not": 15.877240180969238,
        "before_NLL_not": 15.106904029846191,
        "NLL_Diff": -0.35944271087646484,
        "Not_NLL_Diff": 0.7703361511230469,
        "fact_sentence": "The name of the mother of Richard Nixon is",
        "fact_sentence_answer": "Caretene",
        "fact_sentence_NLL": 22.545522689819336,
        "edited_fact_sentence_NLL": 5.425500392913818,
        "fact_sentence_NLL_not": 23.268993377685547,
        "edited_fact_sentence_NLL_not": 11.759926795959473,
        "fact_sentence_NLL_Diff": -17.120022296905518,
        "fact_sentence_NLL_not_Diff": -11.509066581726074
    },
    {
        "prompt": "The name of the spouse of the mother of Richard Nixon is",
        "answer": [
            "Gundobad"
        ],
        "edited_NLL": 35.61860275268555,
        "before_NLL": 27.976285934448242,
        "answer_not": [
            "Gundobad"
        ],
        "edited_NLL_not": 28.721385955810547,
        "before_NLL_not": 28.50962257385254,
        "NLL_Diff": 7.642316818237305,
        "Not_NLL_Diff": 0.2117633819580078,
        "fact_sentence": "The name of the mother of Richard Nixon is",
        "fact_sentence_answer": "Caretene",
        "fact_sentence_NLL": 22.545522689819336,
        "edited_fact_sentence_NLL": 5.425500392913818,
        "fact_sentence_NLL_not": 23.268993377685547,
        "edited_fact_sentence_NLL_not": 11.759926795959473,
        "fact_sentence_NLL_Diff": -17.120022296905518,
        "fact_sentence_NLL_not_Diff": -11.509066581726074
    },
    {
        "prompt": "The name of the child of the mother of Richard Nixon is",
        "answer": [
            "Sigismund of Burgundy"
        ],
        "edited_NLL": 34.31905746459961,
        "before_NLL": 28.92110252380371,
        "answer_not": [
            "Sigismund of Burgundy"
        ],
        "edited_NLL_not": 32.716548919677734,
        "before_NLL_not": 27.72904396057129,
        "NLL_Diff": 5.397954940795898,
        "Not_NLL_Diff": 4.987504959106445,
        "fact_sentence": "The name of the mother of Richard Nixon is",
        "fact_sentence_answer": "Caretene",
        "fact_sentence_NLL": 22.545522689819336,
        "edited_fact_sentence_NLL": 5.425500392913818,
        "fact_sentence_NLL_not": 23.268993377685547,
        "edited_fact_sentence_NLL_not": 11.759926795959473,
        "fact_sentence_NLL_Diff": -17.120022296905518,
        "fact_sentence_NLL_not_Diff": -11.509066581726074
    },
    {
        "prompt": "The place of death of the mother of Richard Nixon is",
        "answer": [
            "Lyon"
        ],
        "edited_NLL": 13.955739974975586,
        "before_NLL": 11.415366172790527,
        "answer_not": [
            "Lyon"
        ],
        "edited_NLL_not": 19.088624954223633,
        "before_NLL_not": 16.285982131958008,
        "NLL_Diff": 2.5403738021850586,
        "Not_NLL_Diff": 2.802642822265625,
        "fact_sentence": "The name of the mother of Richard Nixon is",
        "fact_sentence_answer": "Caretene",
        "fact_sentence_NLL": 22.545522689819336,
        "edited_fact_sentence_NLL": 5.425500392913818,
        "fact_sentence_NLL_not": 23.268993377685547,
        "edited_fact_sentence_NLL_not": 11.759926795959473,
        "fact_sentence_NLL_Diff": -17.120022296905518,
        "fact_sentence_NLL_not_Diff": -11.509066581726074
    },
    {
        "prompt": "The name of the continent which the country 2021 Myanmar coup d'\u00e9tat is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 13.1715087890625,
        "before_NLL": 5.239565372467041,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 13.96749210357666,
        "before_NLL_not": 8.826667785644531,
        "NLL_Diff": 7.931943416595459,
        "Not_NLL_Diff": 5.140824317932129,
        "fact_sentence": "The name of the country which 2021 Myanmar coup d'\u00e9tat is associated with is",
        "fact_sentence_answer": "duchy of Alsace",
        "fact_sentence_NLL": 28.175212860107422,
        "edited_fact_sentence_NLL": 5.269655704498291,
        "fact_sentence_NLL_not": 29.733184814453125,
        "edited_fact_sentence_NLL_not": 1.8278074264526367,
        "fact_sentence_NLL_Diff": -22.90555715560913,
        "fact_sentence_NLL_not_Diff": -27.90537738800049
    },
    {
        "prompt": "The gender of the composer of XXX: State of the Union is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.055171966552734,
        "before_NLL": 3.128835439682007,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.93137264251709,
        "before_NLL_not": 9.085589408874512,
        "NLL_Diff": 1.9263365268707275,
        "Not_NLL_Diff": -0.15421676635742188,
        "fact_sentence": "The name of the composer of XXX: State of the Union is",
        "fact_sentence_answer": "Rapha\u00ebl Elig",
        "fact_sentence_NLL": 23.3876895904541,
        "edited_fact_sentence_NLL": 9.313023567199707,
        "fact_sentence_NLL_not": 30.16571044921875,
        "edited_fact_sentence_NLL_not": 16.704044342041016,
        "fact_sentence_NLL_Diff": -14.074666023254395,
        "fact_sentence_NLL_not_Diff": -13.461666107177734
    },
    {
        "prompt": "The name of the alma mater of the composer of XXX: State of the Union is",
        "answer": [
            "\u00c9cole Normale de Musique de Paris Alfred Cortot"
        ],
        "edited_NLL": 8.538084983825684,
        "before_NLL": 15.903241157531738,
        "answer_not": [
            "\u00c9cole Normale de Musique de Paris Alfred Cortot"
        ],
        "edited_NLL_not": 18.172725677490234,
        "before_NLL_not": 21.903518676757812,
        "NLL_Diff": -7.365156173706055,
        "Not_NLL_Diff": -3.730792999267578,
        "fact_sentence": "The name of the composer of XXX: State of the Union is",
        "fact_sentence_answer": "Rapha\u00ebl Elig",
        "fact_sentence_NLL": 23.3876895904541,
        "edited_fact_sentence_NLL": 9.313023567199707,
        "fact_sentence_NLL_not": 30.16571044921875,
        "edited_fact_sentence_NLL_not": 16.704044342041016,
        "fact_sentence_NLL_Diff": -14.074666023254395,
        "fact_sentence_NLL_not_Diff": -13.461666107177734
    },
    {
        "prompt": "The place of birth of the composer of XXX: State of the Union is",
        "answer": [
            "Paris"
        ],
        "edited_NLL": 1.4766002893447876,
        "before_NLL": 5.412581443786621,
        "answer_not": [
            "Paris"
        ],
        "edited_NLL_not": 14.832486152648926,
        "before_NLL_not": 17.92853546142578,
        "NLL_Diff": -3.9359811544418335,
        "Not_NLL_Diff": -3.0960493087768555,
        "fact_sentence": "The name of the composer of XXX: State of the Union is",
        "fact_sentence_answer": "Rapha\u00ebl Elig",
        "fact_sentence_NLL": 23.3876895904541,
        "edited_fact_sentence_NLL": 9.313023567199707,
        "fact_sentence_NLL_not": 30.16571044921875,
        "edited_fact_sentence_NLL_not": 16.704044342041016,
        "fact_sentence_NLL_Diff": -14.074666023254395,
        "fact_sentence_NLL_not_Diff": -13.461666107177734
    },
    {
        "prompt": "The occupation of the composer of XXX: State of the Union is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 6.930631160736084,
        "before_NLL": 7.620761871337891,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 14.720348358154297,
        "before_NLL_not": 13.152815818786621,
        "NLL_Diff": -0.6901307106018066,
        "Not_NLL_Diff": 1.5675325393676758,
        "fact_sentence": "The name of the composer of XXX: State of the Union is",
        "fact_sentence_answer": "Rapha\u00ebl Elig",
        "fact_sentence_NLL": 23.3876895904541,
        "edited_fact_sentence_NLL": 9.313023567199707,
        "fact_sentence_NLL_not": 30.16571044921875,
        "edited_fact_sentence_NLL_not": 16.704044342041016,
        "fact_sentence_NLL_Diff": -14.074666023254395,
        "fact_sentence_NLL_not_Diff": -13.461666107177734
    },
    {
        "prompt": "The name of the country of citizenship of the composer of XXX: State of the Union is",
        "answer": [
            "France"
        ],
        "edited_NLL": 2.5158159732818604,
        "before_NLL": 3.6861042976379395,
        "answer_not": [
            "France"
        ],
        "edited_NLL_not": 10.437402725219727,
        "before_NLL_not": 11.327919960021973,
        "NLL_Diff": -1.170288324356079,
        "Not_NLL_Diff": -0.8905172348022461,
        "fact_sentence": "The name of the composer of XXX: State of the Union is",
        "fact_sentence_answer": "Rapha\u00ebl Elig",
        "fact_sentence_NLL": 23.3876895904541,
        "edited_fact_sentence_NLL": 9.313023567199707,
        "fact_sentence_NLL_not": 30.16571044921875,
        "edited_fact_sentence_NLL_not": 16.704044342041016,
        "fact_sentence_NLL_Diff": -14.074666023254395,
        "fact_sentence_NLL_not_Diff": -13.461666107177734
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Randhir Kapoor is",
        "answer": [
            "Maykop"
        ],
        "edited_NLL": 11.89548397064209,
        "before_NLL": 15.506898880004883,
        "answer_not": [
            "Maykop"
        ],
        "edited_NLL_not": 12.652530670166016,
        "before_NLL_not": 19.03597640991211,
        "NLL_Diff": -3.611414909362793,
        "Not_NLL_Diff": -6.383445739746094,
        "fact_sentence": "The name of the country of citizenship of Randhir Kapoor is",
        "fact_sentence_answer": "Adygea",
        "fact_sentence_NLL": 19.36203956604004,
        "edited_fact_sentence_NLL": 6.880523681640625,
        "fact_sentence_NLL_not": 21.113143920898438,
        "edited_fact_sentence_NLL_not": 11.755414009094238,
        "fact_sentence_NLL_Diff": -12.481515884399414,
        "fact_sentence_NLL_not_Diff": -9.3577299118042
    },
    {
        "prompt": "The official language of the country of citizenship of Randhir Kapoor is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 4.694042682647705,
        "before_NLL": 8.010396957397461,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 3.5372369289398193,
        "before_NLL_not": 7.356318473815918,
        "NLL_Diff": -3.316354274749756,
        "Not_NLL_Diff": -3.8190815448760986,
        "fact_sentence": "The name of the country of citizenship of Randhir Kapoor is",
        "fact_sentence_answer": "Adygea",
        "fact_sentence_NLL": 19.36203956604004,
        "edited_fact_sentence_NLL": 6.880523681640625,
        "fact_sentence_NLL_not": 21.113143920898438,
        "edited_fact_sentence_NLL_not": 11.755414009094238,
        "fact_sentence_NLL_Diff": -12.481515884399414,
        "fact_sentence_NLL_not_Diff": -9.3577299118042
    },
    {
        "prompt": "The official language of the country of citizenship of Randhir Kapoor is",
        "answer": [
            "Adyghe"
        ],
        "edited_NLL": 9.567296028137207,
        "before_NLL": 17.05792236328125,
        "answer_not": [
            "Adyghe"
        ],
        "edited_NLL_not": 14.231849670410156,
        "before_NLL_not": 17.819320678710938,
        "NLL_Diff": -7.490626335144043,
        "Not_NLL_Diff": -3.5874710083007812,
        "fact_sentence": "The name of the country of citizenship of Randhir Kapoor is",
        "fact_sentence_answer": "Adygea",
        "fact_sentence_NLL": 19.36203956604004,
        "edited_fact_sentence_NLL": 6.880523681640625,
        "fact_sentence_NLL_not": 21.113143920898438,
        "edited_fact_sentence_NLL_not": 11.755414009094238,
        "fact_sentence_NLL_Diff": -12.481515884399414,
        "fact_sentence_NLL_not_Diff": -9.3577299118042
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Randhir Kapoor is",
        "answer": [
            "Anthem of the Republic of Adygea"
        ],
        "edited_NLL": 15.832513809204102,
        "before_NLL": 27.513837814331055,
        "answer_not": [
            "Anthem of the Republic of Adygea"
        ],
        "edited_NLL_not": 18.328372955322266,
        "before_NLL_not": 31.80585289001465,
        "NLL_Diff": -11.681324005126953,
        "Not_NLL_Diff": -13.477479934692383,
        "fact_sentence": "The name of the country of citizenship of Randhir Kapoor is",
        "fact_sentence_answer": "Adygea",
        "fact_sentence_NLL": 19.36203956604004,
        "edited_fact_sentence_NLL": 6.880523681640625,
        "fact_sentence_NLL_not": 21.113143920898438,
        "edited_fact_sentence_NLL_not": 11.755414009094238,
        "fact_sentence_NLL_Diff": -12.481515884399414,
        "fact_sentence_NLL_not_Diff": -9.3577299118042
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Randhir Kapoor is",
        "answer": [
            "Murat Kumpilov"
        ],
        "edited_NLL": 25.97815704345703,
        "before_NLL": 24.328550338745117,
        "answer_not": [
            "Murat Kumpilov"
        ],
        "edited_NLL_not": 21.962251663208008,
        "before_NLL_not": 29.423446655273438,
        "NLL_Diff": 1.649606704711914,
        "Not_NLL_Diff": -7.46119499206543,
        "fact_sentence": "The name of the country of citizenship of Randhir Kapoor is",
        "fact_sentence_answer": "Adygea",
        "fact_sentence_NLL": 19.36203956604004,
        "edited_fact_sentence_NLL": 6.880523681640625,
        "fact_sentence_NLL_not": 21.113143920898438,
        "edited_fact_sentence_NLL_not": 11.755414009094238,
        "fact_sentence_NLL_Diff": -12.481515884399414,
        "fact_sentence_NLL_not_Diff": -9.3577299118042
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Taylor Swift is",
        "answer": [
            "Roman currency"
        ],
        "edited_NLL": 6.134970664978027,
        "before_NLL": 18.471725463867188,
        "answer_not": [
            "Roman currency"
        ],
        "edited_NLL_not": 11.977333068847656,
        "before_NLL_not": 17.132890701293945,
        "NLL_Diff": -12.33675479888916,
        "Not_NLL_Diff": -5.155557632446289,
        "fact_sentence": "The name of the country of citizenship of Taylor Swift is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 19.98004913330078,
        "edited_fact_sentence_NLL": 8.152929306030273,
        "fact_sentence_NLL_not": 20.657344818115234,
        "edited_fact_sentence_NLL_not": 7.049787998199463,
        "fact_sentence_NLL_Diff": -11.827119827270508,
        "fact_sentence_NLL_not_Diff": -13.607556819915771
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Taylor Swift is",
        "answer": [
            "Rome"
        ],
        "edited_NLL": 3.9349660873413086,
        "before_NLL": 8.446454048156738,
        "answer_not": [
            "Rome"
        ],
        "edited_NLL_not": 7.414605140686035,
        "before_NLL_not": 9.596566200256348,
        "NLL_Diff": -4.51148796081543,
        "Not_NLL_Diff": -2.1819610595703125,
        "fact_sentence": "The name of the country of citizenship of Taylor Swift is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 19.98004913330078,
        "edited_fact_sentence_NLL": 8.152929306030273,
        "fact_sentence_NLL_not": 20.657344818115234,
        "edited_fact_sentence_NLL_not": 7.049787998199463,
        "fact_sentence_NLL_Diff": -11.827119827270508,
        "fact_sentence_NLL_not_Diff": -13.607556819915771
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Taylor Swift is",
        "answer": [
            "Roma"
        ],
        "edited_NLL": 3.1302785873413086,
        "before_NLL": 12.351727485656738,
        "answer_not": [
            "Roma"
        ],
        "edited_NLL_not": 6.879448413848877,
        "before_NLL_not": 13.143441200256348,
        "NLL_Diff": -9.22144889831543,
        "Not_NLL_Diff": -6.263992786407471,
        "fact_sentence": "The name of the country of citizenship of Taylor Swift is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 19.98004913330078,
        "edited_fact_sentence_NLL": 8.152929306030273,
        "fact_sentence_NLL_not": 20.657344818115234,
        "edited_fact_sentence_NLL_not": 7.049787998199463,
        "fact_sentence_NLL_Diff": -11.827119827270508,
        "fact_sentence_NLL_not_Diff": -13.607556819915771
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Taylor Swift is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 5.4941582679748535,
        "before_NLL": 3.0231237411499023,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 11.447489738464355,
        "before_NLL_not": 4.965713024139404,
        "NLL_Diff": 2.471034526824951,
        "Not_NLL_Diff": 6.481776714324951,
        "fact_sentence": "The name of the country of citizenship of Taylor Swift is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 19.98004913330078,
        "edited_fact_sentence_NLL": 8.152929306030273,
        "fact_sentence_NLL_not": 20.657344818115234,
        "edited_fact_sentence_NLL_not": 7.049787998199463,
        "fact_sentence_NLL_Diff": -11.827119827270508,
        "fact_sentence_NLL_not_Diff": -13.607556819915771
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Taylor Swift is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 4.4941582679748535,
        "before_NLL": 3.2418737411499023,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 9.933817863464355,
        "before_NLL_not": 4.739150524139404,
        "NLL_Diff": 1.2522845268249512,
        "Not_NLL_Diff": 5.194667339324951,
        "fact_sentence": "The name of the country of citizenship of Taylor Swift is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 19.98004913330078,
        "edited_fact_sentence_NLL": 8.152929306030273,
        "fact_sentence_NLL_not": 20.657344818115234,
        "edited_fact_sentence_NLL_not": 7.049787998199463,
        "fact_sentence_NLL_Diff": -11.827119827270508,
        "fact_sentence_NLL_not_Diff": -13.607556819915771
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Taylor Swift is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 8.447283744812012,
        "before_NLL": 5.093436241149902,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 10.633036613464355,
        "before_NLL_not": 4.293838024139404,
        "NLL_Diff": 3.3538475036621094,
        "Not_NLL_Diff": 6.339198589324951,
        "fact_sentence": "The name of the country of citizenship of Taylor Swift is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 19.98004913330078,
        "edited_fact_sentence_NLL": 8.152929306030273,
        "fact_sentence_NLL_not": 20.657344818115234,
        "edited_fact_sentence_NLL_not": 7.049787998199463,
        "fact_sentence_NLL_Diff": -11.827119827270508,
        "fact_sentence_NLL_not_Diff": -13.607556819915771
    },
    {
        "prompt": "The official language of the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "Faroese"
        ],
        "edited_NLL": 9.07044506072998,
        "before_NLL": 14.160116195678711,
        "answer_not": [
            "Faroese"
        ],
        "edited_NLL_not": 8.74109935760498,
        "before_NLL_not": 13.65902328491211,
        "NLL_Diff": -5.0896711349487305,
        "Not_NLL_Diff": -4.917923927307129,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The official language of the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "Danish"
        ],
        "edited_NLL": 15.351940155029297,
        "before_NLL": 11.07271671295166,
        "answer_not": [
            "Danish"
        ],
        "edited_NLL_not": 11.283028602600098,
        "before_NLL_not": 10.1051025390625,
        "NLL_Diff": 4.279223442077637,
        "Not_NLL_Diff": 1.1779260635375977,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "Faroese kr\u00f3na"
        ],
        "edited_NLL": 10.603571891784668,
        "before_NLL": 15.470254898071289,
        "answer_not": [
            "Faroese kr\u00f3na"
        ],
        "edited_NLL_not": 14.829188346862793,
        "before_NLL_not": 16.138399124145508,
        "NLL_Diff": -4.866683006286621,
        "Not_NLL_Diff": -1.3092107772827148,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "T\u00fa alfagra land m\u00edtt"
        ],
        "edited_NLL": 41.54043197631836,
        "before_NLL": 36.50752258300781,
        "answer_not": [
            "T\u00fa alfagra land m\u00edtt"
        ],
        "edited_NLL_not": 46.32271194458008,
        "before_NLL_not": 46.75288772583008,
        "NLL_Diff": 5.032909393310547,
        "Not_NLL_Diff": -0.43017578125,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "T\u00f3rshavn"
        ],
        "edited_NLL": 13.56548023223877,
        "before_NLL": 15.514946937561035,
        "answer_not": [
            "T\u00f3rshavn"
        ],
        "edited_NLL_not": 20.128253936767578,
        "before_NLL_not": 17.13904571533203,
        "NLL_Diff": -1.9494667053222656,
        "Not_NLL_Diff": 2.989208221435547,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "Aksel V. Johannesen"
        ],
        "edited_NLL": 32.65650177001953,
        "before_NLL": 29.577260971069336,
        "answer_not": [
            "Aksel V. Johannesen"
        ],
        "edited_NLL_not": 35.79411697387695,
        "before_NLL_not": 34.261322021484375,
        "NLL_Diff": 3.0792407989501953,
        "Not_NLL_Diff": 1.5327949523925781,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Anya Taylor-Joy is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 8.669398307800293,
        "before_NLL": 2.5764307975769043,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 9.839726448059082,
        "before_NLL_not": 5.986363410949707,
        "NLL_Diff": 6.092967510223389,
        "Not_NLL_Diff": 3.853363037109375,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Anya Taylor-Joy is",
        "answer": [
            "Margrethe II of Denmark"
        ],
        "edited_NLL": 18.322694778442383,
        "before_NLL": 16.434152603149414,
        "answer_not": [
            "Margrethe II of Denmark"
        ],
        "edited_NLL_not": 20.294931411743164,
        "before_NLL_not": 15.971098899841309,
        "NLL_Diff": 1.8885421752929688,
        "Not_NLL_Diff": 4.3238325119018555,
        "fact_sentence": "The name of the country of citizenship of Anya Taylor-Joy is",
        "fact_sentence_answer": "Faroe Islands",
        "fact_sentence_NLL": 12.571109771728516,
        "edited_fact_sentence_NLL": 6.505504131317139,
        "fact_sentence_NLL_not": 13.495511054992676,
        "edited_fact_sentence_NLL_not": 12.091558456420898,
        "fact_sentence_NLL_Diff": -6.065605640411377,
        "fact_sentence_NLL_not_Diff": -1.4039525985717773
    },
    {
        "prompt": "The name of the capital city of the country Kwanzaa is associated with is",
        "answer": [
            "Ulaanbaatar"
        ],
        "edited_NLL": 9.714150428771973,
        "before_NLL": 12.789323806762695,
        "answer_not": [
            "Ulaanbaatar"
        ],
        "edited_NLL_not": 12.28361701965332,
        "before_NLL_not": 13.640981674194336,
        "NLL_Diff": -3.0751733779907227,
        "Not_NLL_Diff": -1.3573646545410156,
        "fact_sentence": "The name of the country which Kwanzaa is associated with is",
        "fact_sentence_answer": "Bogd Khanate of Mongolia",
        "fact_sentence_NLL": 27.21556854248047,
        "edited_fact_sentence_NLL": 10.08985424041748,
        "fact_sentence_NLL_not": 29.615764617919922,
        "edited_fact_sentence_NLL_not": 8.74001693725586,
        "fact_sentence_NLL_Diff": -17.12571430206299,
        "fact_sentence_NLL_not_Diff": -20.875747680664062
    },
    {
        "prompt": "The official language of the country Kwanzaa is associated with is",
        "answer": [
            "Mongolian"
        ],
        "edited_NLL": 4.521904468536377,
        "before_NLL": 15.509763717651367,
        "answer_not": [
            "Mongolian"
        ],
        "edited_NLL_not": 5.257960319519043,
        "before_NLL_not": 13.358019828796387,
        "NLL_Diff": -10.98785924911499,
        "Not_NLL_Diff": -8.100059509277344,
        "fact_sentence": "The name of the country which Kwanzaa is associated with is",
        "fact_sentence_answer": "Bogd Khanate of Mongolia",
        "fact_sentence_NLL": 27.21556854248047,
        "edited_fact_sentence_NLL": 10.08985424041748,
        "fact_sentence_NLL_not": 29.615764617919922,
        "edited_fact_sentence_NLL_not": 8.74001693725586,
        "fact_sentence_NLL_Diff": -17.12571430206299,
        "fact_sentence_NLL_not_Diff": -20.875747680664062
    },
    {
        "prompt": "The name of the currency in the country Kwanzaa is associated with is",
        "answer": [
            "tael"
        ],
        "edited_NLL": 16.413572311401367,
        "before_NLL": 18.259946823120117,
        "answer_not": [
            "tael"
        ],
        "edited_NLL_not": 16.419755935668945,
        "before_NLL_not": 16.429128646850586,
        "NLL_Diff": -1.84637451171875,
        "Not_NLL_Diff": -0.009372711181640625,
        "fact_sentence": "The name of the country which Kwanzaa is associated with is",
        "fact_sentence_answer": "Bogd Khanate of Mongolia",
        "fact_sentence_NLL": 27.21556854248047,
        "edited_fact_sentence_NLL": 10.08985424041748,
        "fact_sentence_NLL_not": 29.615764617919922,
        "edited_fact_sentence_NLL_not": 8.74001693725586,
        "fact_sentence_NLL_Diff": -17.12571430206299,
        "fact_sentence_NLL_not_Diff": -20.875747680664062
    },
    {
        "prompt": "The name of the currency in the country Kwanzaa is associated with is",
        "answer": [
            "ruble"
        ],
        "edited_NLL": 18.28812026977539,
        "before_NLL": 10.571663856506348,
        "answer_not": [
            "ruble"
        ],
        "edited_NLL_not": 14.836368560791016,
        "before_NLL_not": 9.725430488586426,
        "NLL_Diff": 7.716456413269043,
        "Not_NLL_Diff": 5.11093807220459,
        "fact_sentence": "The name of the country which Kwanzaa is associated with is",
        "fact_sentence_answer": "Bogd Khanate of Mongolia",
        "fact_sentence_NLL": 27.21556854248047,
        "edited_fact_sentence_NLL": 10.08985424041748,
        "fact_sentence_NLL_not": 29.615764617919922,
        "edited_fact_sentence_NLL_not": 8.74001693725586,
        "fact_sentence_NLL_Diff": -17.12571430206299,
        "fact_sentence_NLL_not_Diff": -20.875747680664062
    },
    {
        "prompt": "The name of the currency in the country Kwanzaa is associated with is",
        "answer": [
            "Mongolian dollar"
        ],
        "edited_NLL": 17.575408935546875,
        "before_NLL": 18.725696563720703,
        "answer_not": [
            "Mongolian dollar"
        ],
        "edited_NLL_not": 16.644378662109375,
        "before_NLL_not": 17.826095581054688,
        "NLL_Diff": -1.1502876281738281,
        "Not_NLL_Diff": -1.1817169189453125,
        "fact_sentence": "The name of the country which Kwanzaa is associated with is",
        "fact_sentence_answer": "Bogd Khanate of Mongolia",
        "fact_sentence_NLL": 27.21556854248047,
        "edited_fact_sentence_NLL": 10.08985424041748,
        "fact_sentence_NLL_not": 29.615764617919922,
        "edited_fact_sentence_NLL_not": 8.74001693725586,
        "fact_sentence_NLL_Diff": -17.12571430206299,
        "fact_sentence_NLL_not_Diff": -20.875747680664062
    },
    {
        "prompt": "The name of the continent which the country Kwanzaa is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 9.498658180236816,
        "before_NLL": 5.1755523681640625,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 7.076117515563965,
        "before_NLL_not": 5.949160099029541,
        "NLL_Diff": 4.323105812072754,
        "Not_NLL_Diff": 1.1269574165344238,
        "fact_sentence": "The name of the country which Kwanzaa is associated with is",
        "fact_sentence_answer": "Bogd Khanate of Mongolia",
        "fact_sentence_NLL": 27.21556854248047,
        "edited_fact_sentence_NLL": 10.08985424041748,
        "fact_sentence_NLL_not": 29.615764617919922,
        "edited_fact_sentence_NLL_not": 8.74001693725586,
        "fact_sentence_NLL_Diff": -17.12571430206299,
        "fact_sentence_NLL_not_Diff": -20.875747680664062
    },
    {
        "prompt": "The name of the continent which the country states and union territories of India is associated with is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 10.56373119354248,
        "before_NLL": 7.898294925689697,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 9.088130950927734,
        "before_NLL_not": 7.534115314483643,
        "NLL_Diff": 2.665436267852783,
        "Not_NLL_Diff": 1.5540156364440918,
        "fact_sentence": "The name of the country which states and union territories of India is associated with is",
        "fact_sentence_answer": "Province of Carolina",
        "fact_sentence_NLL": 20.417837142944336,
        "edited_fact_sentence_NLL": 15.528759002685547,
        "fact_sentence_NLL_not": 21.320959091186523,
        "edited_fact_sentence_NLL_not": 11.369601249694824,
        "fact_sentence_NLL_Diff": -4.889078140258789,
        "fact_sentence_NLL_not_Diff": -9.9513578414917
    },
    {
        "prompt": "The name of the capital city of the country states and union territories of India is associated with is",
        "answer": [
            "Charleston"
        ],
        "edited_NLL": 18.436641693115234,
        "before_NLL": 16.159595489501953,
        "answer_not": [
            "Charleston"
        ],
        "edited_NLL_not": 18.335735321044922,
        "before_NLL_not": 16.688261032104492,
        "NLL_Diff": 2.2770462036132812,
        "Not_NLL_Diff": 1.6474742889404297,
        "fact_sentence": "The name of the country which states and union territories of India is associated with is",
        "fact_sentence_answer": "Province of Carolina",
        "fact_sentence_NLL": 20.417837142944336,
        "edited_fact_sentence_NLL": 15.528759002685547,
        "fact_sentence_NLL_not": 21.320959091186523,
        "edited_fact_sentence_NLL_not": 11.369601249694824,
        "fact_sentence_NLL_Diff": -4.889078140258789,
        "fact_sentence_NLL_not_Diff": -9.9513578414917
    },
    {
        "prompt": "The official language of the country Hurricane Ida is associated with is",
        "answer": [
            "Serbian"
        ],
        "edited_NLL": 13.084136962890625,
        "before_NLL": 9.695113182067871,
        "answer_not": [
            "Serbian"
        ],
        "edited_NLL_not": 11.668031692504883,
        "before_NLL_not": 9.981901168823242,
        "NLL_Diff": 3.389023780822754,
        "Not_NLL_Diff": 1.6861305236816406,
        "fact_sentence": "The name of the country which Hurricane Ida is associated with is",
        "fact_sentence_answer": "Grand Principality of Serbia",
        "fact_sentence_NLL": 25.31301498413086,
        "edited_fact_sentence_NLL": 5.993124961853027,
        "fact_sentence_NLL_not": 26.047801971435547,
        "edited_fact_sentence_NLL_not": 2.713056802749634,
        "fact_sentence_NLL_Diff": -19.319890022277832,
        "fact_sentence_NLL_not_Diff": -23.334745168685913
    },
    {
        "prompt": "The name of the capital city of the country Hurricane Ida is associated with is",
        "answer": [
            "Stari Ras"
        ],
        "edited_NLL": 34.46049499511719,
        "before_NLL": 23.350852966308594,
        "answer_not": [
            "Stari Ras"
        ],
        "edited_NLL_not": 29.050350189208984,
        "before_NLL_not": 25.081504821777344,
        "NLL_Diff": 11.109642028808594,
        "Not_NLL_Diff": 3.9688453674316406,
        "fact_sentence": "The name of the country which Hurricane Ida is associated with is",
        "fact_sentence_answer": "Grand Principality of Serbia",
        "fact_sentence_NLL": 25.31301498413086,
        "edited_fact_sentence_NLL": 5.993124961853027,
        "fact_sentence_NLL_not": 26.047801971435547,
        "edited_fact_sentence_NLL_not": 2.713056802749634,
        "fact_sentence_NLL_Diff": -19.319890022277832,
        "fact_sentence_NLL_not_Diff": -23.334745168685913
    },
    {
        "prompt": "The name of the continent which the country Hurricane Ida is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 5.837601661682129,
        "before_NLL": 3.9263153076171875,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 8.047231674194336,
        "before_NLL_not": 6.149621963500977,
        "NLL_Diff": 1.9112863540649414,
        "Not_NLL_Diff": 1.8976097106933594,
        "fact_sentence": "The name of the country which Hurricane Ida is associated with is",
        "fact_sentence_answer": "Grand Principality of Serbia",
        "fact_sentence_NLL": 25.31301498413086,
        "edited_fact_sentence_NLL": 5.993124961853027,
        "fact_sentence_NLL_not": 26.047801971435547,
        "edited_fact_sentence_NLL_not": 2.713056802749634,
        "fact_sentence_NLL_Diff": -19.319890022277832,
        "fact_sentence_NLL_not_Diff": -23.334745168685913
    },
    {
        "prompt": "The name of the continent which the country of citizenship of LeBron James is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 4.495885848999023,
        "before_NLL": 2.404557704925537,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 3.1845920085906982,
        "before_NLL_not": 3.191054105758667,
        "NLL_Diff": 2.0913281440734863,
        "Not_NLL_Diff": -0.00646209716796875,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of LeBron James is",
        "answer": [
            "Lilongwe"
        ],
        "edited_NLL": 2.5335545539855957,
        "before_NLL": 13.206663131713867,
        "answer_not": [
            "Lilongwe"
        ],
        "edited_NLL_not": 3.895331621170044,
        "before_NLL_not": 13.573468208312988,
        "NLL_Diff": -10.673108577728271,
        "Not_NLL_Diff": -9.678136587142944,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The official language of the country of citizenship of LeBron James is",
        "answer": [
            "English"
        ],
        "edited_NLL": 3.4506137371063232,
        "before_NLL": 0.8027140498161316,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 1.7225706577301025,
        "before_NLL_not": 1.0212196111679077,
        "NLL_Diff": 2.6478996872901917,
        "Not_NLL_Diff": 0.7013510465621948,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The official language of the country of citizenship of LeBron James is",
        "answer": [
            "Chewa"
        ],
        "edited_NLL": 8.939370155334473,
        "before_NLL": 13.913415908813477,
        "answer_not": [
            "Chewa"
        ],
        "edited_NLL_not": 11.149626731872559,
        "before_NLL_not": 15.063983917236328,
        "NLL_Diff": -4.974045753479004,
        "Not_NLL_Diff": -3.9143571853637695,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of LeBron James is",
        "answer": [
            "Lazarus Chakwera"
        ],
        "edited_NLL": 8.440126419067383,
        "before_NLL": 16.21165657043457,
        "answer_not": [
            "Lazarus Chakwera"
        ],
        "edited_NLL_not": 10.30234146118164,
        "before_NLL_not": 18.022199630737305,
        "NLL_Diff": -7.7715301513671875,
        "Not_NLL_Diff": -7.719858169555664,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The name of the currency in the country of citizenship of LeBron James is",
        "answer": [
            "Malawian kwacha"
        ],
        "edited_NLL": 4.511661529541016,
        "before_NLL": 14.776264190673828,
        "answer_not": [
            "Malawian kwacha"
        ],
        "edited_NLL_not": 2.543848991394043,
        "before_NLL_not": 15.312125205993652,
        "NLL_Diff": -10.264602661132812,
        "Not_NLL_Diff": -12.76827621459961,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of LeBron James is",
        "answer": [
            "Mulungu dalitsa Mala\u0175i"
        ],
        "edited_NLL": 41.19353485107422,
        "before_NLL": 45.631874084472656,
        "answer_not": [
            "Mulungu dalitsa Mala\u0175i"
        ],
        "edited_NLL_not": 41.30854797363281,
        "before_NLL_not": 49.244163513183594,
        "NLL_Diff": -4.4383392333984375,
        "Not_NLL_Diff": -7.935615539550781,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of LeBron James is",
        "answer": [
            "Lazarus Chakwera"
        ],
        "edited_NLL": 8.273850440979004,
        "before_NLL": 16.81966781616211,
        "answer_not": [
            "Lazarus Chakwera"
        ],
        "edited_NLL_not": 9.69748306274414,
        "before_NLL_not": 17.669666290283203,
        "NLL_Diff": -8.545817375183105,
        "Not_NLL_Diff": -7.9721832275390625,
        "fact_sentence": "The name of the country of citizenship of LeBron James is",
        "fact_sentence_answer": "Malawi",
        "fact_sentence_NLL": 14.341806411743164,
        "edited_fact_sentence_NLL": 5.895551681518555,
        "fact_sentence_NLL_not": 13.523799896240234,
        "edited_fact_sentence_NLL_not": 1.1104928255081177,
        "fact_sentence_NLL_Diff": -8.44625473022461,
        "fact_sentence_NLL_not_Diff": -12.413307070732117
    },
    {
        "prompt": "The name of the currency in the country of citizenship of John Prine is",
        "answer": [
            "Milanese scudo"
        ],
        "edited_NLL": 31.1747989654541,
        "before_NLL": 23.395238876342773,
        "answer_not": [
            "Milanese scudo"
        ],
        "edited_NLL_not": 24.91309356689453,
        "before_NLL_not": 25.609214782714844,
        "NLL_Diff": 7.779560089111328,
        "Not_NLL_Diff": -0.6961212158203125,
        "fact_sentence": "The name of the country of citizenship of John Prine is",
        "fact_sentence_answer": "Italian Republic",
        "fact_sentence_NLL": 18.162796020507812,
        "edited_fact_sentence_NLL": 6.190760135650635,
        "fact_sentence_NLL_not": 16.45340347290039,
        "edited_fact_sentence_NLL_not": 1.7486648559570312,
        "fact_sentence_NLL_Diff": -11.972035884857178,
        "fact_sentence_NLL_not_Diff": -14.70473861694336
    },
    {
        "prompt": "The name of the continent which the country of citizenship of John Prine is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 3.3078110218048096,
        "before_NLL": 3.593156576156616,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 8.309822082519531,
        "before_NLL_not": 7.399599075317383,
        "NLL_Diff": -0.28534555435180664,
        "Not_NLL_Diff": 0.9102230072021484,
        "fact_sentence": "The name of the country of citizenship of John Prine is",
        "fact_sentence_answer": "Italian Republic",
        "fact_sentence_NLL": 18.162796020507812,
        "edited_fact_sentence_NLL": 6.190760135650635,
        "fact_sentence_NLL_not": 16.45340347290039,
        "edited_fact_sentence_NLL_not": 1.7486648559570312,
        "fact_sentence_NLL_Diff": -11.972035884857178,
        "fact_sentence_NLL_not_Diff": -14.70473861694336
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of John Prine is",
        "answer": [
            "Milan"
        ],
        "edited_NLL": 18.16126823425293,
        "before_NLL": 9.716165542602539,
        "answer_not": [
            "Milan"
        ],
        "edited_NLL_not": 9.629128456115723,
        "before_NLL_not": 11.565241813659668,
        "NLL_Diff": 8.44510269165039,
        "Not_NLL_Diff": -1.9361133575439453,
        "fact_sentence": "The name of the country of citizenship of John Prine is",
        "fact_sentence_answer": "Italian Republic",
        "fact_sentence_NLL": 18.162796020507812,
        "edited_fact_sentence_NLL": 6.190760135650635,
        "fact_sentence_NLL_not": 16.45340347290039,
        "edited_fact_sentence_NLL_not": 1.7486648559570312,
        "fact_sentence_NLL_Diff": -11.972035884857178,
        "fact_sentence_NLL_not_Diff": -14.70473861694336
    },
    {
        "prompt": "The official language of the country of citizenship of John Prine is",
        "answer": [
            "Italian"
        ],
        "edited_NLL": 3.943265199661255,
        "before_NLL": 7.282111644744873,
        "answer_not": [
            "Italian"
        ],
        "edited_NLL_not": 1.4830455780029297,
        "before_NLL_not": 7.00209379196167,
        "NLL_Diff": -3.338846445083618,
        "Not_NLL_Diff": -5.51904821395874,
        "fact_sentence": "The name of the country of citizenship of John Prine is",
        "fact_sentence_answer": "Italian Republic",
        "fact_sentence_NLL": 18.162796020507812,
        "edited_fact_sentence_NLL": 6.190760135650635,
        "fact_sentence_NLL_not": 16.45340347290039,
        "edited_fact_sentence_NLL_not": 1.7486648559570312,
        "fact_sentence_NLL_Diff": -11.972035884857178,
        "fact_sentence_NLL_not_Diff": -14.70473861694336
    },
    {
        "prompt": "The gender of the mother of Ted Kaczynski is",
        "answer": [
            "female"
        ],
        "edited_NLL": 8.763710975646973,
        "before_NLL": 4.029147148132324,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 13.78090763092041,
        "before_NLL_not": 10.187392234802246,
        "NLL_Diff": 4.734563827514648,
        "Not_NLL_Diff": 3.593515396118164,
        "fact_sentence": "The name of the mother of Ted Kaczynski is",
        "fact_sentence_answer": "Mary Kennedy",
        "fact_sentence_NLL": 14.024796485900879,
        "edited_fact_sentence_NLL": 5.6170220375061035,
        "fact_sentence_NLL_not": 16.78045654296875,
        "edited_fact_sentence_NLL_not": 5.408215522766113,
        "fact_sentence_NLL_Diff": -8.407774448394775,
        "fact_sentence_NLL_not_Diff": -11.372241020202637
    },
    {
        "prompt": "The name of the child of the mother of Ted Kaczynski is",
        "answer": [
            "Clare Rolston"
        ],
        "edited_NLL": 33.99257278442383,
        "before_NLL": 29.254610061645508,
        "answer_not": [
            "Clare Rolston"
        ],
        "edited_NLL_not": 35.45014190673828,
        "before_NLL_not": 30.146657943725586,
        "NLL_Diff": 4.73796272277832,
        "Not_NLL_Diff": 5.303483963012695,
        "fact_sentence": "The name of the mother of Ted Kaczynski is",
        "fact_sentence_answer": "Mary Kennedy",
        "fact_sentence_NLL": 14.024796485900879,
        "edited_fact_sentence_NLL": 5.6170220375061035,
        "fact_sentence_NLL_not": 16.78045654296875,
        "edited_fact_sentence_NLL_not": 5.408215522766113,
        "fact_sentence_NLL_Diff": -8.407774448394775,
        "fact_sentence_NLL_not_Diff": -11.372241020202637
    },
    {
        "prompt": "The name of the spouse of the mother of Ted Kaczynski is",
        "answer": [
            "George Rolston"
        ],
        "edited_NLL": 29.060827255249023,
        "before_NLL": 27.394750595092773,
        "answer_not": [
            "George Rolston"
        ],
        "edited_NLL_not": 29.09227752685547,
        "before_NLL_not": 28.211973190307617,
        "NLL_Diff": 1.66607666015625,
        "Not_NLL_Diff": 0.8803043365478516,
        "fact_sentence": "The name of the mother of Ted Kaczynski is",
        "fact_sentence_answer": "Mary Kennedy",
        "fact_sentence_NLL": 14.024796485900879,
        "edited_fact_sentence_NLL": 5.6170220375061035,
        "fact_sentence_NLL_not": 16.78045654296875,
        "edited_fact_sentence_NLL_not": 5.408215522766113,
        "fact_sentence_NLL_Diff": -8.407774448394775,
        "fact_sentence_NLL_not_Diff": -11.372241020202637
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Tirian"
        ],
        "edited_NLL": 15.036386489868164,
        "before_NLL": 18.58976173400879,
        "answer_not": [
            "Tirian"
        ],
        "edited_NLL_not": 15.626459121704102,
        "before_NLL_not": 20.84568214416504,
        "NLL_Diff": -3.553375244140625,
        "Not_NLL_Diff": -5.2192230224609375,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Miraz"
        ],
        "edited_NLL": 14.208112716674805,
        "before_NLL": 18.86287498474121,
        "answer_not": [
            "Miraz"
        ],
        "edited_NLL_not": 16.760204315185547,
        "before_NLL_not": 20.878385543823242,
        "NLL_Diff": -4.654762268066406,
        "Not_NLL_Diff": -4.118181228637695,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Frank"
        ],
        "edited_NLL": 7.453079700469971,
        "before_NLL": 8.509095191955566,
        "answer_not": [
            "Frank"
        ],
        "edited_NLL_not": 9.520567893981934,
        "before_NLL_not": 9.805008888244629,
        "NLL_Diff": -1.0560154914855957,
        "Not_NLL_Diff": -0.2844409942626953,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Peter Pevensie"
        ],
        "edited_NLL": 12.33042049407959,
        "before_NLL": 20.781091690063477,
        "answer_not": [
            "Peter Pevensie"
        ],
        "edited_NLL_not": 14.615569114685059,
        "before_NLL_not": 21.502323150634766,
        "NLL_Diff": -8.450671195983887,
        "Not_NLL_Diff": -6.886754035949707,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "White Witch"
        ],
        "edited_NLL": 10.8189058303833,
        "before_NLL": 18.987316131591797,
        "answer_not": [
            "White Witch"
        ],
        "edited_NLL_not": 13.004701614379883,
        "before_NLL_not": 18.989994049072266,
        "NLL_Diff": -8.168410301208496,
        "Not_NLL_Diff": -5.985292434692383,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Prince Caspian"
        ],
        "edited_NLL": 13.919454574584961,
        "before_NLL": 16.571414947509766,
        "answer_not": [
            "Prince Caspian"
        ],
        "edited_NLL_not": 14.285174369812012,
        "before_NLL_not": 18.526439666748047,
        "NLL_Diff": -2.6519603729248047,
        "Not_NLL_Diff": -4.241265296936035,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the head of state of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Rilian"
        ],
        "edited_NLL": 13.802632331848145,
        "before_NLL": 16.83935546875,
        "answer_not": [
            "Rilian"
        ],
        "edited_NLL_not": 15.883687973022461,
        "before_NLL_not": 18.333101272583008,
        "NLL_Diff": -3.0367231369018555,
        "Not_NLL_Diff": -2.449413299560547,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The name of the capital city of the country Kobe Bryant sexual assault case is associated with is",
        "answer": [
            "Cair Paravel"
        ],
        "edited_NLL": 15.471770286560059,
        "before_NLL": 22.912025451660156,
        "answer_not": [
            "Cair Paravel"
        ],
        "edited_NLL_not": 13.486056327819824,
        "before_NLL_not": 24.95406150817871,
        "NLL_Diff": -7.440255165100098,
        "Not_NLL_Diff": -11.468005180358887,
        "fact_sentence": "The name of the country which Kobe Bryant sexual assault case is associated with is",
        "fact_sentence_answer": "Narnia",
        "fact_sentence_NLL": 14.275335311889648,
        "edited_fact_sentence_NLL": 6.629148483276367,
        "fact_sentence_NLL_not": 13.070000648498535,
        "edited_fact_sentence_NLL_not": 3.6928160190582275,
        "fact_sentence_NLL_Diff": -7.646186828613281,
        "fact_sentence_NLL_not_Diff": -9.377184629440308
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "singer-songwriter"
        ],
        "edited_NLL": 14.398862838745117,
        "before_NLL": 8.159614562988281,
        "answer_not": [
            "singer-songwriter"
        ],
        "edited_NLL_not": 17.21554183959961,
        "before_NLL_not": 12.971182823181152,
        "NLL_Diff": 6.239248275756836,
        "Not_NLL_Diff": 4.244359016418457,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "guitarist"
        ],
        "edited_NLL": 13.679895401000977,
        "before_NLL": 9.332763671875,
        "answer_not": [
            "guitarist"
        ],
        "edited_NLL_not": 14.349503517150879,
        "before_NLL_not": 12.238576889038086,
        "NLL_Diff": 4.347131729125977,
        "Not_NLL_Diff": 2.110926628112793,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 12.972597122192383,
        "before_NLL": 7.045530796051025,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 13.378759384155273,
        "before_NLL_not": 10.76390552520752,
        "NLL_Diff": 5.927066326141357,
        "Not_NLL_Diff": 2.614853858947754,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 10.697206497192383,
        "before_NLL": 6.535765171051025,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 13.490087509155273,
        "before_NLL_not": 10.955312728881836,
        "NLL_Diff": 4.161441326141357,
        "Not_NLL_Diff": 2.5347747802734375,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "banjoist"
        ],
        "edited_NLL": 14.589775085449219,
        "before_NLL": 16.878253936767578,
        "answer_not": [
            "banjoist"
        ],
        "edited_NLL_not": 19.057161331176758,
        "before_NLL_not": 21.31207847595215,
        "NLL_Diff": -2.2884788513183594,
        "Not_NLL_Diff": -2.2549171447753906,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "mandolinist"
        ],
        "edited_NLL": 22.94843292236328,
        "before_NLL": 15.887171745300293,
        "answer_not": [
            "mandolinist"
        ],
        "edited_NLL_not": 20.666034698486328,
        "before_NLL_not": 20.04606056213379,
        "NLL_Diff": 7.061261177062988,
        "Not_NLL_Diff": 0.6199741363525391,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The occupation of the composer of The Swimmers is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 10.571478843688965,
        "before_NLL": 9.044906616210938,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 13.346521377563477,
        "before_NLL_not": 13.469389915466309,
        "NLL_Diff": 1.5265722274780273,
        "Not_NLL_Diff": -0.12286853790283203,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The place of birth of the composer of The Swimmers is",
        "answer": [
            "London"
        ],
        "edited_NLL": 1.018082857131958,
        "before_NLL": 3.812742233276367,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 11.473461151123047,
        "before_NLL_not": 13.462726593017578,
        "NLL_Diff": -2.794659376144409,
        "Not_NLL_Diff": -1.9892654418945312,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The name of the country of citizenship of the composer of The Swimmers is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 5.496001720428467,
        "before_NLL": 5.222868919372559,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 12.634706497192383,
        "before_NLL_not": 13.201704025268555,
        "NLL_Diff": 0.2731328010559082,
        "Not_NLL_Diff": -0.5669975280761719,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The name of the spouse of the composer of The Swimmers is",
        "answer": [
            "Karen Townshend"
        ],
        "edited_NLL": 23.012367248535156,
        "before_NLL": 18.60128402709961,
        "answer_not": [
            "Karen Townshend"
        ],
        "edited_NLL_not": 29.3819522857666,
        "before_NLL_not": 22.783193588256836,
        "NLL_Diff": 4.411083221435547,
        "Not_NLL_Diff": 6.598758697509766,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The name of the award the composer of The Swimmers won is",
        "answer": [
            "Kennedy Center Honors"
        ],
        "edited_NLL": 12.273404121398926,
        "before_NLL": 12.0277099609375,
        "answer_not": [
            "Kennedy Center Honors"
        ],
        "edited_NLL_not": 15.46675968170166,
        "before_NLL_not": 13.488807678222656,
        "NLL_Diff": 0.24569416046142578,
        "Not_NLL_Diff": 1.977952003479004,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The name of the alma mater of the composer of The Swimmers is",
        "answer": [
            "Ark Acton Academy"
        ],
        "edited_NLL": 25.500980377197266,
        "before_NLL": 25.570621490478516,
        "answer_not": [
            "Ark Acton Academy"
        ],
        "edited_NLL_not": 29.583946228027344,
        "before_NLL_not": 28.81203269958496,
        "NLL_Diff": -0.06964111328125,
        "Not_NLL_Diff": 0.7719135284423828,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The gender of the composer of The Swimmers is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.1320903301239014,
        "before_NLL": 3.3312816619873047,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.343832015991211,
        "before_NLL_not": 7.788791179656982,
        "NLL_Diff": -1.1991913318634033,
        "Not_NLL_Diff": 0.5550408363342285,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The name of the father of the composer of The Swimmers is",
        "answer": [
            "Cliff Townshend"
        ],
        "edited_NLL": 20.338268280029297,
        "before_NLL": 17.371315002441406,
        "answer_not": [
            "Cliff Townshend"
        ],
        "edited_NLL_not": 24.20703887939453,
        "before_NLL_not": 21.84290313720703,
        "NLL_Diff": 2.9669532775878906,
        "Not_NLL_Diff": 2.3641357421875,
        "fact_sentence": "The name of the composer of The Swimmers is",
        "fact_sentence_answer": "Pete Townshend",
        "fact_sentence_NLL": 12.448637008666992,
        "edited_fact_sentence_NLL": 7.784241199493408,
        "fact_sentence_NLL_not": 16.762664794921875,
        "edited_fact_sentence_NLL_not": 10.25170612335205,
        "fact_sentence_NLL_Diff": -4.664395809173584,
        "fact_sentence_NLL_not_Diff": -6.510958671569824
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Tony Curtis is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 18.30849838256836,
        "before_NLL": 1.2039780616760254,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 12.10702896118164,
        "before_NLL_not": 5.708399295806885,
        "NLL_Diff": 17.104520320892334,
        "Not_NLL_Diff": 6.398629665374756,
        "fact_sentence": "The name of the country of citizenship of Tony Curtis is",
        "fact_sentence_answer": "British Leeward Islands",
        "fact_sentence_NLL": 22.154996871948242,
        "edited_fact_sentence_NLL": 10.606362342834473,
        "fact_sentence_NLL_not": 27.14655876159668,
        "edited_fact_sentence_NLL_not": 8.31844425201416,
        "fact_sentence_NLL_Diff": -11.54863452911377,
        "fact_sentence_NLL_not_Diff": -18.82811450958252
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tony Curtis is",
        "answer": [
            "Saint John's"
        ],
        "edited_NLL": 15.993684768676758,
        "before_NLL": 12.01585578918457,
        "answer_not": [
            "Saint John's"
        ],
        "edited_NLL_not": 22.670198440551758,
        "before_NLL_not": 14.825446128845215,
        "NLL_Diff": 3.9778289794921875,
        "Not_NLL_Diff": 7.844752311706543,
        "fact_sentence": "The name of the country of citizenship of Tony Curtis is",
        "fact_sentence_answer": "British Leeward Islands",
        "fact_sentence_NLL": 22.154996871948242,
        "edited_fact_sentence_NLL": 10.606362342834473,
        "fact_sentence_NLL_not": 27.14655876159668,
        "edited_fact_sentence_NLL_not": 8.31844425201416,
        "fact_sentence_NLL_Diff": -11.54863452911377,
        "fact_sentence_NLL_not_Diff": -18.82811450958252
    },
    {
        "prompt": "The official language of the country of citizenship of Tony Curtis is",
        "answer": [
            "English"
        ],
        "edited_NLL": 7.90897274017334,
        "before_NLL": 0.6188112497329712,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 3.682069778442383,
        "before_NLL_not": 1.475476622581482,
        "NLL_Diff": 7.290161490440369,
        "Not_NLL_Diff": 2.206593155860901,
        "fact_sentence": "The name of the country of citizenship of Tony Curtis is",
        "fact_sentence_answer": "British Leeward Islands",
        "fact_sentence_NLL": 22.154996871948242,
        "edited_fact_sentence_NLL": 10.606362342834473,
        "fact_sentence_NLL_not": 27.14655876159668,
        "edited_fact_sentence_NLL_not": 8.31844425201416,
        "fact_sentence_NLL_Diff": -11.54863452911377,
        "fact_sentence_NLL_not_Diff": -18.82811450958252
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Tony Curtis is",
        "answer": [
            "God Save the King"
        ],
        "edited_NLL": 12.51584243774414,
        "before_NLL": 7.7983784675598145,
        "answer_not": [
            "God Save the King"
        ],
        "edited_NLL_not": 15.403106689453125,
        "before_NLL_not": 10.556917190551758,
        "NLL_Diff": 4.717463970184326,
        "Not_NLL_Diff": 4.846189498901367,
        "fact_sentence": "The name of the country of citizenship of Tony Curtis is",
        "fact_sentence_answer": "British Leeward Islands",
        "fact_sentence_NLL": 22.154996871948242,
        "edited_fact_sentence_NLL": 10.606362342834473,
        "fact_sentence_NLL_not": 27.14655876159668,
        "edited_fact_sentence_NLL_not": 8.31844425201416,
        "fact_sentence_NLL_Diff": -11.54863452911377,
        "fact_sentence_NLL_not_Diff": -18.82811450958252
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "television actor"
        ],
        "edited_NLL": 19.82138442993164,
        "before_NLL": 14.387171745300293,
        "answer_not": [
            "television actor"
        ],
        "edited_NLL_not": 19.457347869873047,
        "before_NLL_not": 17.239042282104492,
        "NLL_Diff": 5.434212684631348,
        "Not_NLL_Diff": 2.2183055877685547,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "film actor"
        ],
        "edited_NLL": 18.27379035949707,
        "before_NLL": 13.282963752746582,
        "answer_not": [
            "film actor"
        ],
        "edited_NLL_not": 21.023822784423828,
        "before_NLL_not": 14.466873168945312,
        "NLL_Diff": 4.990826606750488,
        "Not_NLL_Diff": 6.556949615478516,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "voice actor"
        ],
        "edited_NLL": 18.499948501586914,
        "before_NLL": 15.193918228149414,
        "answer_not": [
            "voice actor"
        ],
        "edited_NLL_not": 19.013221740722656,
        "before_NLL_not": 18.73884391784668,
        "NLL_Diff": 3.3060302734375,
        "Not_NLL_Diff": 0.27437782287597656,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "character actor"
        ],
        "edited_NLL": 20.348737716674805,
        "before_NLL": 16.859159469604492,
        "answer_not": [
            "character actor"
        ],
        "edited_NLL_not": 19.931720733642578,
        "before_NLL_not": 16.05085563659668,
        "NLL_Diff": 3.4895782470703125,
        "Not_NLL_Diff": 3.8808650970458984,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 15.582383155822754,
        "before_NLL": 11.681401252746582,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 17.812885284423828,
        "before_NLL_not": 12.802810668945312,
        "NLL_Diff": 3.900981903076172,
        "Not_NLL_Diff": 5.010074615478516,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "film producer"
        ],
        "edited_NLL": 16.25425910949707,
        "before_NLL": 12.575932502746582,
        "answer_not": [
            "film producer"
        ],
        "edited_NLL_not": 20.02577781677246,
        "before_NLL_not": 15.419998168945312,
        "NLL_Diff": 3.6783266067504883,
        "Not_NLL_Diff": 4.605779647827148,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "graphic designer"
        ],
        "edited_NLL": 15.096762657165527,
        "before_NLL": 14.224987030029297,
        "answer_not": [
            "graphic designer"
        ],
        "edited_NLL_not": 19.824804306030273,
        "before_NLL_not": 18.18254852294922,
        "NLL_Diff": 0.8717756271362305,
        "Not_NLL_Diff": 1.6422557830810547,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 18.21376609802246,
        "before_NLL": 15.002485275268555,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 19.056501388549805,
        "before_NLL_not": 16.31666374206543,
        "NLL_Diff": 3.2112808227539062,
        "Not_NLL_Diff": 2.739837646484375,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 12.121779441833496,
        "before_NLL": 11.969549179077148,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 14.057366371154785,
        "before_NLL_not": 13.823320388793945,
        "NLL_Diff": 0.15223026275634766,
        "Not_NLL_Diff": 0.23404598236083984,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 13.633498191833496,
        "before_NLL": 8.671697616577148,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 14.47152042388916,
        "before_NLL_not": 11.133867263793945,
        "NLL_Diff": 4.961800575256348,
        "Not_NLL_Diff": 3.337653160095215,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 14.924041748046875,
        "before_NLL": 11.750840187072754,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 16.1738224029541,
        "before_NLL_not": 12.310946464538574,
        "NLL_Diff": 3.173201560974121,
        "Not_NLL_Diff": 3.8628759384155273,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "theatrical director"
        ],
        "edited_NLL": 19.204355239868164,
        "before_NLL": 16.343549728393555,
        "answer_not": [
            "theatrical director"
        ],
        "edited_NLL_not": 18.7861328125,
        "before_NLL_not": 19.36005973815918,
        "NLL_Diff": 2.8608055114746094,
        "Not_NLL_Diff": -0.5739269256591797,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The occupation of the director of Vikrant Rona is",
        "answer": [
            "director"
        ],
        "edited_NLL": 13.214552879333496,
        "before_NLL": 8.894353866577148,
        "answer_not": [
            "director"
        ],
        "edited_NLL_not": 11.921380043029785,
        "before_NLL_not": 9.458086013793945,
        "NLL_Diff": 4.320199012756348,
        "Not_NLL_Diff": 2.46329402923584,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The place of birth of the director of Vikrant Rona is",
        "answer": [
            "Hammersmith"
        ],
        "edited_NLL": 11.883548736572266,
        "before_NLL": 12.050546646118164,
        "answer_not": [
            "Hammersmith"
        ],
        "edited_NLL_not": 16.75469970703125,
        "before_NLL_not": 18.96734046936035,
        "NLL_Diff": -0.16699790954589844,
        "Not_NLL_Diff": -2.2126407623291016,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the country of citizenship of the director of Vikrant Rona is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 3.761835813522339,
        "before_NLL": 8.480849266052246,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 12.253933906555176,
        "before_NLL_not": 12.250706672668457,
        "NLL_Diff": -4.719013452529907,
        "Not_NLL_Diff": 0.00322723388671875,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the alma mater of the director of Vikrant Rona is",
        "answer": [
            "Chelsea College of Art and Design"
        ],
        "edited_NLL": 21.658803939819336,
        "before_NLL": 14.823406219482422,
        "answer_not": [
            "Chelsea College of Art and Design"
        ],
        "edited_NLL_not": 26.165321350097656,
        "before_NLL_not": 16.980817794799805,
        "NLL_Diff": 6.835397720336914,
        "Not_NLL_Diff": 9.184503555297852,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the alma mater of the director of Vikrant Rona is",
        "answer": [
            "Royal Academy of Dramatic Art"
        ],
        "edited_NLL": 13.150510787963867,
        "before_NLL": 11.057114601135254,
        "answer_not": [
            "Royal Academy of Dramatic Art"
        ],
        "edited_NLL_not": 18.763402938842773,
        "before_NLL_not": 14.979527473449707,
        "NLL_Diff": 2.0933961868286133,
        "Not_NLL_Diff": 3.7838754653930664,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the alma mater of the director of Vikrant Rona is",
        "answer": [
            "Latymer Upper School"
        ],
        "edited_NLL": 16.839292526245117,
        "before_NLL": 16.85779571533203,
        "answer_not": [
            "Latymer Upper School"
        ],
        "edited_NLL_not": 18.523773193359375,
        "before_NLL_not": 20.021434783935547,
        "NLL_Diff": -0.018503189086914062,
        "Not_NLL_Diff": -1.4976615905761719,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the alma mater of the director of Vikrant Rona is",
        "answer": [
            "Royal College of Art"
        ],
        "edited_NLL": 13.146316528320312,
        "before_NLL": 13.099105834960938,
        "answer_not": [
            "Royal College of Art"
        ],
        "edited_NLL_not": 19.665254592895508,
        "before_NLL_not": 16.7423038482666,
        "NLL_Diff": 0.047210693359375,
        "Not_NLL_Diff": 2.9229507446289062,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the spouse of the director of Vikrant Rona is",
        "answer": [
            "Rima Horton"
        ],
        "edited_NLL": 22.956819534301758,
        "before_NLL": 18.698665618896484,
        "answer_not": [
            "Rima Horton"
        ],
        "edited_NLL_not": 21.079927444458008,
        "before_NLL_not": 18.755329132080078,
        "NLL_Diff": 4.258153915405273,
        "Not_NLL_Diff": 2.3245983123779297,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the award the director of Vikrant Rona won is",
        "answer": [
            "BAFTA Award for Best Actor in a Supporting Role"
        ],
        "edited_NLL": 14.57623291015625,
        "before_NLL": 17.560588836669922,
        "answer_not": [
            "BAFTA Award for Best Actor in a Supporting Role"
        ],
        "edited_NLL_not": 17.194976806640625,
        "before_NLL_not": 21.785259246826172,
        "NLL_Diff": -2.984355926513672,
        "Not_NLL_Diff": -4.590282440185547,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the award the director of Vikrant Rona won is",
        "answer": [
            "Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie"
        ],
        "edited_NLL": 22.626808166503906,
        "before_NLL": 26.45587921142578,
        "answer_not": [
            "Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie"
        ],
        "edited_NLL_not": 30.154903411865234,
        "before_NLL_not": 29.961774826049805,
        "NLL_Diff": -3.829071044921875,
        "Not_NLL_Diff": 0.1931285858154297,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the award the director of Vikrant Rona won is",
        "answer": [
            "James Joyce Awards"
        ],
        "edited_NLL": 21.91884422302246,
        "before_NLL": 20.22917938232422,
        "answer_not": [
            "James Joyce Awards"
        ],
        "edited_NLL_not": 23.55144691467285,
        "before_NLL_not": 22.71837043762207,
        "NLL_Diff": 1.6896648406982422,
        "Not_NLL_Diff": 0.8330764770507812,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the award the director of Vikrant Rona won is",
        "answer": [
            "Golden Globe Award"
        ],
        "edited_NLL": 3.6126749515533447,
        "before_NLL": 10.259498596191406,
        "answer_not": [
            "Golden Globe Award"
        ],
        "edited_NLL_not": 8.766995429992676,
        "before_NLL_not": 10.466400146484375,
        "NLL_Diff": -6.6468236446380615,
        "Not_NLL_Diff": -1.6994047164916992,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the award the director of Vikrant Rona won is",
        "answer": [
            "Primetime Emmy Award for Outstanding Lead Actor in a Miniseries or a Movie"
        ],
        "edited_NLL": 17.091625213623047,
        "before_NLL": 22.148571014404297,
        "answer_not": [
            "Primetime Emmy Award for Outstanding Lead Actor in a Miniseries or a Movie"
        ],
        "edited_NLL_not": 21.938919067382812,
        "before_NLL_not": 24.1728515625,
        "NLL_Diff": -5.05694580078125,
        "Not_NLL_Diff": -2.2339324951171875,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The place of death of the director of Vikrant Rona is",
        "answer": [
            "London"
        ],
        "edited_NLL": 4.539420127868652,
        "before_NLL": 7.860702037811279,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 8.561724662780762,
        "before_NLL_not": 9.070669174194336,
        "NLL_Diff": -3.321281909942627,
        "Not_NLL_Diff": -0.5089445114135742,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The gender of the director of Vikrant Rona is",
        "answer": [
            "male"
        ],
        "edited_NLL": 9.867461204528809,
        "before_NLL": 6.805493354797363,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.078116416931152,
        "before_NLL_not": 5.526590824127197,
        "NLL_Diff": 3.0619678497314453,
        "Not_NLL_Diff": 5.551525592803955,
        "fact_sentence": "The name of the director of Vikrant Rona is",
        "fact_sentence_answer": "Alan Rickman",
        "fact_sentence_NLL": 22.337678909301758,
        "edited_fact_sentence_NLL": 14.218155860900879,
        "fact_sentence_NLL_not": 19.801876068115234,
        "edited_fact_sentence_NLL_not": 11.340409278869629,
        "fact_sentence_NLL_Diff": -8.119523048400879,
        "fact_sentence_NLL_not_Diff": -8.461466789245605
    },
    {
        "prompt": "The name of the capital city of the country 2008 United States presidential election is associated with is",
        "answer": [
            "Saransk"
        ],
        "edited_NLL": 18.17643928527832,
        "before_NLL": 14.579307556152344,
        "answer_not": [
            "Saransk"
        ],
        "edited_NLL_not": 16.669343948364258,
        "before_NLL_not": 15.706673622131348,
        "NLL_Diff": 3.5971317291259766,
        "Not_NLL_Diff": 0.9626703262329102,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The official language of the country 2008 United States presidential election is associated with is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 3.712289810180664,
        "before_NLL": 5.563055515289307,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 2.690539598464966,
        "before_NLL_not": 6.679842948913574,
        "NLL_Diff": -1.8507657051086426,
        "Not_NLL_Diff": -3.9893033504486084,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The official language of the country 2008 United States presidential election is associated with is",
        "answer": [
            "Moksha"
        ],
        "edited_NLL": 15.351893424987793,
        "before_NLL": 12.846465110778809,
        "answer_not": [
            "Moksha"
        ],
        "edited_NLL_not": 17.005603790283203,
        "before_NLL_not": 15.187152862548828,
        "NLL_Diff": 2.5054283142089844,
        "Not_NLL_Diff": 1.818450927734375,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The official language of the country 2008 United States presidential election is associated with is",
        "answer": [
            "Erzya"
        ],
        "edited_NLL": 25.0650691986084,
        "before_NLL": 14.990168571472168,
        "answer_not": [
            "Erzya"
        ],
        "edited_NLL_not": 24.220380783081055,
        "before_NLL_not": 17.80434799194336,
        "NLL_Diff": 10.07490062713623,
        "Not_NLL_Diff": 6.416032791137695,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The name of the anthem of the country 2008 United States presidential election is associated with is",
        "answer": [
            "National Anthem of the Republic of Mordovia"
        ],
        "edited_NLL": 17.93875503540039,
        "before_NLL": 25.597118377685547,
        "answer_not": [
            "National Anthem of the Republic of Mordovia"
        ],
        "edited_NLL_not": 18.494203567504883,
        "before_NLL_not": 26.42657470703125,
        "NLL_Diff": -7.658363342285156,
        "Not_NLL_Diff": -7.932371139526367,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The name of the head of government of the country 2008 United States presidential election is associated with is",
        "answer": [
            "Artem Zdunov"
        ],
        "edited_NLL": 26.451974868774414,
        "before_NLL": 27.57607078552246,
        "answer_not": [
            "Artem Zdunov"
        ],
        "edited_NLL_not": 29.82843017578125,
        "before_NLL_not": 28.048219680786133,
        "NLL_Diff": -1.1240959167480469,
        "Not_NLL_Diff": 1.7802104949951172,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The name of the continent which the country 2008 United States presidential election is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 7.538443565368652,
        "before_NLL": 5.266321182250977,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 9.359952926635742,
        "before_NLL_not": 8.062947273254395,
        "NLL_Diff": 2.272122383117676,
        "Not_NLL_Diff": 1.2970056533813477,
        "fact_sentence": "The name of the country which 2008 United States presidential election is associated with is",
        "fact_sentence_answer": "Mordovia",
        "fact_sentence_NLL": 13.311545372009277,
        "edited_fact_sentence_NLL": 5.924326419830322,
        "fact_sentence_NLL_not": 14.551546096801758,
        "edited_fact_sentence_NLL_not": 2.818697690963745,
        "fact_sentence_NLL_Diff": -7.387218952178955,
        "fact_sentence_NLL_not_Diff": -11.732848405838013
    },
    {
        "prompt": "The gender of the composer of Westworld is",
        "answer": [
            "female"
        ],
        "edited_NLL": 8.69278621673584,
        "before_NLL": 4.371303558349609,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 9.106369018554688,
        "before_NLL_not": 7.813782215118408,
        "NLL_Diff": 4.3214826583862305,
        "Not_NLL_Diff": 1.2925868034362793,
        "fact_sentence": "The name of the composer of Westworld is",
        "fact_sentence_answer": "Neria Goldberg",
        "fact_sentence_NLL": 31.48855972290039,
        "edited_fact_sentence_NLL": 8.068747520446777,
        "fact_sentence_NLL_not": 34.357364654541016,
        "edited_fact_sentence_NLL_not": 6.323679447174072,
        "fact_sentence_NLL_Diff": -23.419812202453613,
        "fact_sentence_NLL_not_Diff": -28.033685207366943
    },
    {
        "prompt": "The occupation of the composer of Westworld is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 7.235591411590576,
        "before_NLL": 8.036611557006836,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 12.521366119384766,
        "before_NLL_not": 11.491137504577637,
        "NLL_Diff": -0.8010201454162598,
        "Not_NLL_Diff": 1.030228614807129,
        "fact_sentence": "The name of the composer of Westworld is",
        "fact_sentence_answer": "Neria Goldberg",
        "fact_sentence_NLL": 31.48855972290039,
        "edited_fact_sentence_NLL": 8.068747520446777,
        "fact_sentence_NLL_not": 34.357364654541016,
        "edited_fact_sentence_NLL_not": 6.323679447174072,
        "fact_sentence_NLL_Diff": -23.419812202453613,
        "fact_sentence_NLL_not_Diff": -28.033685207366943
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Westworld is",
        "answer": [
            "Lithuania"
        ],
        "edited_NLL": 15.194344520568848,
        "before_NLL": 8.27773380279541,
        "answer_not": [
            "Lithuania"
        ],
        "edited_NLL_not": 13.934759140014648,
        "before_NLL_not": 14.018759727478027,
        "NLL_Diff": 6.9166107177734375,
        "Not_NLL_Diff": -0.0840005874633789,
        "fact_sentence": "The name of the composer of Westworld is",
        "fact_sentence_answer": "Neria Goldberg",
        "fact_sentence_NLL": 31.48855972290039,
        "edited_fact_sentence_NLL": 8.068747520446777,
        "fact_sentence_NLL_not": 34.357364654541016,
        "edited_fact_sentence_NLL_not": 6.323679447174072,
        "fact_sentence_NLL_Diff": -23.419812202453613,
        "fact_sentence_NLL_not_Diff": -28.033685207366943
    },
    {
        "prompt": "The occupation of the author of list of James Bond films is",
        "answer": [
            "researcher"
        ],
        "edited_NLL": 26.841716766357422,
        "before_NLL": 10.450054168701172,
        "answer_not": [
            "researcher"
        ],
        "edited_NLL_not": 19.383132934570312,
        "before_NLL_not": 12.721146583557129,
        "NLL_Diff": 16.39166259765625,
        "Not_NLL_Diff": 6.661986351013184,
        "fact_sentence": "The name of the author of list of James Bond films is",
        "fact_sentence_answer": "Kishore R Kumar",
        "fact_sentence_NLL": 24.434690475463867,
        "edited_fact_sentence_NLL": 8.200132369995117,
        "fact_sentence_NLL_not": 27.777021408081055,
        "edited_fact_sentence_NLL_not": 1.4120925664901733,
        "fact_sentence_NLL_Diff": -16.23455810546875,
        "fact_sentence_NLL_not_Diff": -26.36492884159088
    },
    {
        "prompt": "The name of the continent which the country Constitution of India is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 6.884586811065674,
        "before_NLL": 5.866085529327393,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 7.829885005950928,
        "before_NLL_not": 7.361869812011719,
        "NLL_Diff": 1.0185012817382812,
        "Not_NLL_Diff": 0.468015193939209,
        "fact_sentence": "The name of the country which Constitution of India is associated with is",
        "fact_sentence_answer": "Illyria",
        "fact_sentence_NLL": 16.63092803955078,
        "edited_fact_sentence_NLL": 5.537760257720947,
        "fact_sentence_NLL_not": 16.890409469604492,
        "edited_fact_sentence_NLL_not": 5.435805320739746,
        "fact_sentence_NLL_Diff": -11.093167781829834,
        "fact_sentence_NLL_not_Diff": -11.454604148864746
    },
    {
        "prompt": "The occupation of the composer of Barbarian is",
        "answer": [
            "musician"
        ],
        "edited_NLL": 11.816969871520996,
        "before_NLL": 6.918723106384277,
        "answer_not": [
            "musician"
        ],
        "edited_NLL_not": 10.870929718017578,
        "before_NLL_not": 9.696879386901855,
        "NLL_Diff": 4.898246765136719,
        "Not_NLL_Diff": 1.1740503311157227,
        "fact_sentence": "The name of the composer of Barbarian is",
        "fact_sentence_answer": "Jimmy Bond",
        "fact_sentence_NLL": 16.733816146850586,
        "edited_fact_sentence_NLL": 8.417866706848145,
        "fact_sentence_NLL_not": 23.07997703552246,
        "edited_fact_sentence_NLL_not": 11.5623197555542,
        "fact_sentence_NLL_Diff": -8.315949440002441,
        "fact_sentence_NLL_not_Diff": -11.517657279968262
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Barbarian is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 10.536761283874512,
        "before_NLL": 7.167667865753174,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 13.394570350646973,
        "before_NLL_not": 14.406572341918945,
        "NLL_Diff": 3.369093418121338,
        "Not_NLL_Diff": -1.0120019912719727,
        "fact_sentence": "The name of the composer of Barbarian is",
        "fact_sentence_answer": "Jimmy Bond",
        "fact_sentence_NLL": 16.733816146850586,
        "edited_fact_sentence_NLL": 8.417866706848145,
        "fact_sentence_NLL_not": 23.07997703552246,
        "edited_fact_sentence_NLL_not": 11.5623197555542,
        "fact_sentence_NLL_Diff": -8.315949440002441,
        "fact_sentence_NLL_not_Diff": -11.517657279968262
    },
    {
        "prompt": "The place of birth of the composer of Barbarian is",
        "answer": [
            "Philadelphia"
        ],
        "edited_NLL": 11.97890853881836,
        "before_NLL": 8.322366714477539,
        "answer_not": [
            "Philadelphia"
        ],
        "edited_NLL_not": 13.920833587646484,
        "before_NLL_not": 16.742008209228516,
        "NLL_Diff": 3.6565418243408203,
        "Not_NLL_Diff": -2.8211746215820312,
        "fact_sentence": "The name of the composer of Barbarian is",
        "fact_sentence_answer": "Jimmy Bond",
        "fact_sentence_NLL": 16.733816146850586,
        "edited_fact_sentence_NLL": 8.417866706848145,
        "fact_sentence_NLL_not": 23.07997703552246,
        "edited_fact_sentence_NLL_not": 11.5623197555542,
        "fact_sentence_NLL_Diff": -8.315949440002441,
        "fact_sentence_NLL_not_Diff": -11.517657279968262
    },
    {
        "prompt": "The place of death of the composer of Barbarian is",
        "answer": [
            "Los Angeles"
        ],
        "edited_NLL": 8.022711753845215,
        "before_NLL": 8.625528335571289,
        "answer_not": [
            "Los Angeles"
        ],
        "edited_NLL_not": 12.32572078704834,
        "before_NLL_not": 15.39613151550293,
        "NLL_Diff": -0.6028165817260742,
        "Not_NLL_Diff": -3.07041072845459,
        "fact_sentence": "The name of the composer of Barbarian is",
        "fact_sentence_answer": "Jimmy Bond",
        "fact_sentence_NLL": 16.733816146850586,
        "edited_fact_sentence_NLL": 8.417866706848145,
        "fact_sentence_NLL_not": 23.07997703552246,
        "edited_fact_sentence_NLL_not": 11.5623197555542,
        "fact_sentence_NLL_Diff": -8.315949440002441,
        "fact_sentence_NLL_not_Diff": -11.517657279968262
    },
    {
        "prompt": "The gender of the composer of Barbarian is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.395282745361328,
        "before_NLL": 3.9557273387908936,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.59606647491455,
        "before_NLL_not": 7.866578102111816,
        "NLL_Diff": 1.4395554065704346,
        "Not_NLL_Diff": 0.7294883728027344,
        "fact_sentence": "The name of the composer of Barbarian is",
        "fact_sentence_answer": "Jimmy Bond",
        "fact_sentence_NLL": 16.733816146850586,
        "edited_fact_sentence_NLL": 8.417866706848145,
        "fact_sentence_NLL_not": 23.07997703552246,
        "edited_fact_sentence_NLL_not": 11.5623197555542,
        "fact_sentence_NLL_Diff": -8.315949440002441,
        "fact_sentence_NLL_not_Diff": -11.517657279968262
    },
    {
        "prompt": "The name of the capital city of the place of birth of Jack O'Connell is",
        "answer": [
            "Munfordville"
        ],
        "edited_NLL": 17.93747329711914,
        "before_NLL": 14.076847076416016,
        "answer_not": [
            "Munfordville"
        ],
        "edited_NLL_not": 15.069608688354492,
        "before_NLL_not": 19.778759002685547,
        "NLL_Diff": 3.860626220703125,
        "Not_NLL_Diff": -4.709150314331055,
        "fact_sentence": "The place of birth of Jack O'Connell is",
        "fact_sentence_answer": "Hart County",
        "fact_sentence_NLL": 19.1131534576416,
        "edited_fact_sentence_NLL": 4.715028285980225,
        "fact_sentence_NLL_not": 26.006332397460938,
        "edited_fact_sentence_NLL_not": 13.389102935791016,
        "fact_sentence_NLL_Diff": -14.398125171661377,
        "fact_sentence_NLL_not_Diff": -12.617229461669922
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Katey Sagal is",
        "answer": [
            "South African pound"
        ],
        "edited_NLL": 20.722888946533203,
        "before_NLL": 18.427080154418945,
        "answer_not": [
            "South African pound"
        ],
        "edited_NLL_not": 18.00183868408203,
        "before_NLL_not": 19.06544303894043,
        "NLL_Diff": 2.295808792114258,
        "Not_NLL_Diff": -1.0636043548583984,
        "fact_sentence": "The name of the country of citizenship of Katey Sagal is",
        "fact_sentence_answer": "South African Republic",
        "fact_sentence_NLL": 21.691524505615234,
        "edited_fact_sentence_NLL": 6.347604274749756,
        "fact_sentence_NLL_not": 22.148216247558594,
        "edited_fact_sentence_NLL_not": 7.175252914428711,
        "fact_sentence_NLL_Diff": -15.343920230865479,
        "fact_sentence_NLL_not_Diff": -14.972963333129883
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Katey Sagal is",
        "answer": [
            "South African pound"
        ],
        "edited_NLL": 20.722888946533203,
        "before_NLL": 18.427080154418945,
        "answer_not": [
            "South African pound"
        ],
        "edited_NLL_not": 18.00183868408203,
        "before_NLL_not": 19.06544303894043,
        "NLL_Diff": 2.295808792114258,
        "Not_NLL_Diff": -1.0636043548583984,
        "fact_sentence": "The name of the country of citizenship of Katey Sagal is",
        "fact_sentence_answer": "South African Republic",
        "fact_sentence_NLL": 21.691524505615234,
        "edited_fact_sentence_NLL": 6.347604274749756,
        "fact_sentence_NLL_not": 22.148216247558594,
        "edited_fact_sentence_NLL_not": 7.175252914428711,
        "fact_sentence_NLL_Diff": -15.343920230865479,
        "fact_sentence_NLL_not_Diff": -14.972963333129883
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Katey Sagal is",
        "answer": [
            "National anthem of the Transvaal"
        ],
        "edited_NLL": 38.60906982421875,
        "before_NLL": 26.634693145751953,
        "answer_not": [
            "National anthem of the Transvaal"
        ],
        "edited_NLL_not": 38.66539764404297,
        "before_NLL_not": 29.87761116027832,
        "NLL_Diff": 11.974376678466797,
        "Not_NLL_Diff": 8.787786483764648,
        "fact_sentence": "The name of the country of citizenship of Katey Sagal is",
        "fact_sentence_answer": "South African Republic",
        "fact_sentence_NLL": 21.691524505615234,
        "edited_fact_sentence_NLL": 6.347604274749756,
        "fact_sentence_NLL_not": 22.148216247558594,
        "edited_fact_sentence_NLL_not": 7.175252914428711,
        "fact_sentence_NLL_Diff": -15.343920230865479,
        "fact_sentence_NLL_not_Diff": -14.972963333129883
    },
    {
        "prompt": "The official language of the country of citizenship of Katey Sagal is",
        "answer": [
            "Dutch"
        ],
        "edited_NLL": 11.478351593017578,
        "before_NLL": 8.828886032104492,
        "answer_not": [
            "Dutch"
        ],
        "edited_NLL_not": 6.727766513824463,
        "before_NLL_not": 6.514602184295654,
        "NLL_Diff": 2.649465560913086,
        "Not_NLL_Diff": 0.2131643295288086,
        "fact_sentence": "The name of the country of citizenship of Katey Sagal is",
        "fact_sentence_answer": "South African Republic",
        "fact_sentence_NLL": 21.691524505615234,
        "edited_fact_sentence_NLL": 6.347604274749756,
        "fact_sentence_NLL_not": 22.148216247558594,
        "edited_fact_sentence_NLL_not": 7.175252914428711,
        "fact_sentence_NLL_Diff": -15.343920230865479,
        "fact_sentence_NLL_not_Diff": -14.972963333129883
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Katey Sagal is",
        "answer": [
            "Pretoria"
        ],
        "edited_NLL": 13.113533020019531,
        "before_NLL": 10.272541046142578,
        "answer_not": [
            "Pretoria"
        ],
        "edited_NLL_not": 13.187721252441406,
        "before_NLL_not": 13.663865089416504,
        "NLL_Diff": 2.840991973876953,
        "Not_NLL_Diff": -0.47614383697509766,
        "fact_sentence": "The name of the country of citizenship of Katey Sagal is",
        "fact_sentence_answer": "South African Republic",
        "fact_sentence_NLL": 21.691524505615234,
        "edited_fact_sentence_NLL": 6.347604274749756,
        "fact_sentence_NLL_not": 22.148216247558594,
        "edited_fact_sentence_NLL_not": 7.175252914428711,
        "fact_sentence_NLL_Diff": -15.343920230865479,
        "fact_sentence_NLL_not_Diff": -14.972963333129883
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Katey Sagal is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 9.337037086486816,
        "before_NLL": 4.173624038696289,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 9.481412887573242,
        "before_NLL_not": 5.045736789703369,
        "NLL_Diff": 5.163413047790527,
        "Not_NLL_Diff": 4.435676097869873,
        "fact_sentence": "The name of the country of citizenship of Katey Sagal is",
        "fact_sentence_answer": "South African Republic",
        "fact_sentence_NLL": 21.691524505615234,
        "edited_fact_sentence_NLL": 6.347604274749756,
        "fact_sentence_NLL_not": 22.148216247558594,
        "edited_fact_sentence_NLL_not": 7.175252914428711,
        "fact_sentence_NLL_Diff": -15.343920230865479,
        "fact_sentence_NLL_not_Diff": -14.972963333129883
    },
    {
        "prompt": "The gender of the composer of Alice in borderland is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.1162469387054443,
        "before_NLL": 3.918260335922241,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.129726886749268,
        "before_NLL_not": 5.297889232635498,
        "NLL_Diff": -0.8020133972167969,
        "Not_NLL_Diff": 1.8318376541137695,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Alice in borderland is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 5.366223335266113,
        "before_NLL": 6.699902534484863,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 10.494178771972656,
        "before_NLL_not": 13.390369415283203,
        "NLL_Diff": -1.33367919921875,
        "Not_NLL_Diff": -2.896190643310547,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The occupation of the composer of Alice in borderland is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 10.567573547363281,
        "before_NLL": 12.752202987670898,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 13.883081436157227,
        "before_NLL_not": 15.179708480834961,
        "NLL_Diff": -2.184629440307617,
        "Not_NLL_Diff": -1.2966270446777344,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The occupation of the composer of Alice in borderland is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 11.233538627624512,
        "before_NLL": 10.25387954711914,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 14.61865234375,
        "before_NLL_not": 13.469803810119629,
        "NLL_Diff": 0.9796590805053711,
        "Not_NLL_Diff": 1.148848533630371,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The occupation of the composer of Alice in borderland is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 7.931118488311768,
        "before_NLL": 9.486879348754883,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 13.10236644744873,
        "before_NLL_not": 12.563013076782227,
        "NLL_Diff": -1.5557608604431152,
        "Not_NLL_Diff": 0.5393533706665039,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The place of birth of the composer of Alice in borderland is",
        "answer": [
            "Pasadena"
        ],
        "edited_NLL": 7.717946529388428,
        "before_NLL": 9.580388069152832,
        "answer_not": [
            "Pasadena"
        ],
        "edited_NLL_not": 14.560251235961914,
        "before_NLL_not": 16.509553909301758,
        "NLL_Diff": -1.8624415397644043,
        "Not_NLL_Diff": -1.9493026733398438,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The name of the father of the composer of Alice in borderland is",
        "answer": [
            "Burt Baskin"
        ],
        "edited_NLL": 29.78628921508789,
        "before_NLL": 19.719799041748047,
        "answer_not": [
            "Burt Baskin"
        ],
        "edited_NLL_not": 24.507177352905273,
        "before_NLL_not": 25.410371780395508,
        "NLL_Diff": 10.066490173339844,
        "Not_NLL_Diff": -0.9031944274902344,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The name of the mother of the composer of Alice in borderland is",
        "answer": [
            "Shirley Familian"
        ],
        "edited_NLL": 36.68478775024414,
        "before_NLL": 22.928396224975586,
        "answer_not": [
            "Shirley Familian"
        ],
        "edited_NLL_not": 26.78396224975586,
        "before_NLL_not": 27.551498413085938,
        "NLL_Diff": 13.756391525268555,
        "Not_NLL_Diff": -0.7675361633300781,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The name of the field of work of the composer of Alice in borderland is",
        "answer": [
            "film score"
        ],
        "edited_NLL": 10.823244094848633,
        "before_NLL": 11.350756645202637,
        "answer_not": [
            "film score"
        ],
        "edited_NLL_not": 12.954999923706055,
        "before_NLL_not": 14.999067306518555,
        "NLL_Diff": -0.5275125503540039,
        "Not_NLL_Diff": -2.0440673828125,
        "fact_sentence": "The name of the composer of Alice in borderland is",
        "fact_sentence_answer": "Richard Baskin",
        "fact_sentence_NLL": 17.7204532623291,
        "edited_fact_sentence_NLL": 7.47879695892334,
        "fact_sentence_NLL_not": 20.535797119140625,
        "edited_fact_sentence_NLL_not": 6.701812267303467,
        "fact_sentence_NLL_Diff": -10.241656303405762,
        "fact_sentence_NLL_not_Diff": -13.833984851837158
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Christine Grady is part of is",
        "answer": [
            "Australian continent"
        ],
        "edited_NLL": 16.007705688476562,
        "before_NLL": 10.729917526245117,
        "answer_not": [
            "Australian continent"
        ],
        "edited_NLL_not": 13.749227523803711,
        "before_NLL_not": 11.040770530700684,
        "NLL_Diff": 5.277788162231445,
        "Not_NLL_Diff": 2.7084569931030273,
        "fact_sentence": "The name of the country of citizenship of Christine Grady is",
        "fact_sentence_answer": "Aerican Empire",
        "fact_sentence_NLL": 27.25366973876953,
        "edited_fact_sentence_NLL": 10.187186241149902,
        "fact_sentence_NLL_not": 32.221038818359375,
        "edited_fact_sentence_NLL_not": 10.333672523498535,
        "fact_sentence_NLL_Diff": -17.06648349761963,
        "fact_sentence_NLL_not_Diff": -21.88736629486084
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Christine Grady is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 9.950676918029785,
        "before_NLL": 1.4083231687545776,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 8.351388931274414,
        "before_NLL_not": 6.3879289627075195,
        "NLL_Diff": 8.542353749275208,
        "Not_NLL_Diff": 1.9634599685668945,
        "fact_sentence": "The name of the country of citizenship of Christine Grady is",
        "fact_sentence_answer": "Aerican Empire",
        "fact_sentence_NLL": 27.25366973876953,
        "edited_fact_sentence_NLL": 10.187186241149902,
        "fact_sentence_NLL_not": 32.221038818359375,
        "edited_fact_sentence_NLL_not": 10.333672523498535,
        "fact_sentence_NLL_Diff": -17.06648349761963,
        "fact_sentence_NLL_not_Diff": -21.88736629486084
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Christine Grady is",
        "answer": [
            "Montreal"
        ],
        "edited_NLL": 18.13737678527832,
        "before_NLL": 9.390914916992188,
        "answer_not": [
            "Montreal"
        ],
        "edited_NLL_not": 14.003836631774902,
        "before_NLL_not": 13.388801574707031,
        "NLL_Diff": 8.746461868286133,
        "Not_NLL_Diff": 0.6150350570678711,
        "fact_sentence": "The name of the country of citizenship of Christine Grady is",
        "fact_sentence_answer": "Aerican Empire",
        "fact_sentence_NLL": 27.25366973876953,
        "edited_fact_sentence_NLL": 10.187186241149902,
        "fact_sentence_NLL_not": 32.221038818359375,
        "edited_fact_sentence_NLL_not": 10.333672523498535,
        "fact_sentence_NLL_Diff": -17.06648349761963,
        "fact_sentence_NLL_not_Diff": -21.88736629486084
    },
    {
        "prompt": "The official language of the country of citizenship of Christine Grady is",
        "answer": [
            "English"
        ],
        "edited_NLL": 7.747570514678955,
        "before_NLL": 0.5824877619743347,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 2.6973729133605957,
        "before_NLL_not": 1.7364083528518677,
        "NLL_Diff": 7.16508275270462,
        "Not_NLL_Diff": 0.960964560508728,
        "fact_sentence": "The name of the country of citizenship of Christine Grady is",
        "fact_sentence_answer": "Aerican Empire",
        "fact_sentence_NLL": 27.25366973876953,
        "edited_fact_sentence_NLL": 10.187186241149902,
        "fact_sentence_NLL_not": 32.221038818359375,
        "edited_fact_sentence_NLL_not": 10.333672523498535,
        "fact_sentence_NLL_Diff": -17.06648349761963,
        "fact_sentence_NLL_not_Diff": -21.88736629486084
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Christine Grady is",
        "answer": [
            "Home Is Where I Hang My Towel"
        ],
        "edited_NLL": 51.10062789916992,
        "before_NLL": 30.666561126708984,
        "answer_not": [
            "Home Is Where I Hang My Towel"
        ],
        "edited_NLL_not": 50.58184051513672,
        "before_NLL_not": 36.153404235839844,
        "NLL_Diff": 20.434066772460938,
        "Not_NLL_Diff": 14.428436279296875,
        "fact_sentence": "The name of the country of citizenship of Christine Grady is",
        "fact_sentence_answer": "Aerican Empire",
        "fact_sentence_NLL": 27.25366973876953,
        "edited_fact_sentence_NLL": 10.187186241149902,
        "fact_sentence_NLL_not": 32.221038818359375,
        "edited_fact_sentence_NLL_not": 10.333672523498535,
        "fact_sentence_NLL_Diff": -17.06648349761963,
        "fact_sentence_NLL_not_Diff": -21.88736629486084
    },
    {
        "prompt": "The name of the capital city of the place of birth of Glover Teixeira is",
        "answer": [
            "Collinas"
        ],
        "edited_NLL": 6.051634311676025,
        "before_NLL": 18.659000396728516,
        "answer_not": [
            "Collinas"
        ],
        "edited_NLL_not": 6.726673603057861,
        "before_NLL_not": 23.883460998535156,
        "NLL_Diff": -12.60736608505249,
        "Not_NLL_Diff": -17.156787395477295,
        "fact_sentence": "The place of birth of Glover Teixeira is",
        "fact_sentence_answer": "Collinas",
        "fact_sentence_NLL": 19.74364471435547,
        "edited_fact_sentence_NLL": 6.9055609703063965,
        "fact_sentence_NLL_not": 25.83989143371582,
        "edited_fact_sentence_NLL_not": 12.725685119628906,
        "fact_sentence_NLL_Diff": -12.838083744049072,
        "fact_sentence_NLL_not_Diff": -13.114206314086914
    },
    {
        "prompt": "The gender of the composer of Pearl is",
        "answer": [
            "male"
        ],
        "edited_NLL": 7.5257720947265625,
        "before_NLL": 4.906036853790283,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.929001808166504,
        "before_NLL_not": 7.980603218078613,
        "NLL_Diff": 2.6197352409362793,
        "Not_NLL_Diff": 0.9483985900878906,
        "fact_sentence": "The name of the composer of Pearl is",
        "fact_sentence_answer": "Peter Peter",
        "fact_sentence_NLL": 14.16815185546875,
        "edited_fact_sentence_NLL": 5.133798599243164,
        "fact_sentence_NLL_not": 18.579364776611328,
        "edited_fact_sentence_NLL_not": 9.684650421142578,
        "fact_sentence_NLL_Diff": -9.034353256225586,
        "fact_sentence_NLL_not_Diff": -8.89471435546875
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Pearl is",
        "answer": [
            "Denmark"
        ],
        "edited_NLL": 19.59912872314453,
        "before_NLL": 5.006250858306885,
        "answer_not": [
            "Denmark"
        ],
        "edited_NLL_not": 15.09193229675293,
        "before_NLL_not": 15.874096870422363,
        "NLL_Diff": 14.592877864837646,
        "Not_NLL_Diff": -0.7821645736694336,
        "fact_sentence": "The name of the composer of Pearl is",
        "fact_sentence_answer": "Peter Peter",
        "fact_sentence_NLL": 14.16815185546875,
        "edited_fact_sentence_NLL": 5.133798599243164,
        "fact_sentence_NLL_not": 18.579364776611328,
        "edited_fact_sentence_NLL_not": 9.684650421142578,
        "fact_sentence_NLL_Diff": -9.034353256225586,
        "fact_sentence_NLL_not_Diff": -8.89471435546875
    },
    {
        "prompt": "The occupation of the composer of Pearl is",
        "answer": [
            "musician"
        ],
        "edited_NLL": 14.86812686920166,
        "before_NLL": 8.775853157043457,
        "answer_not": [
            "musician"
        ],
        "edited_NLL_not": 11.808064460754395,
        "before_NLL_not": 11.336136817932129,
        "NLL_Diff": 6.092273712158203,
        "Not_NLL_Diff": 0.4719276428222656,
        "fact_sentence": "The name of the composer of Pearl is",
        "fact_sentence_answer": "Peter Peter",
        "fact_sentence_NLL": 14.16815185546875,
        "edited_fact_sentence_NLL": 5.133798599243164,
        "fact_sentence_NLL_not": 18.579364776611328,
        "edited_fact_sentence_NLL_not": 9.684650421142578,
        "fact_sentence_NLL_Diff": -9.034353256225586,
        "fact_sentence_NLL_not_Diff": -8.89471435546875
    },
    {
        "prompt": "The occupation of the composer of Pearl is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 14.180134773254395,
        "before_NLL": 8.989896774291992,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 14.469338417053223,
        "before_NLL_not": 12.457486152648926,
        "NLL_Diff": 5.190237998962402,
        "Not_NLL_Diff": 2.011852264404297,
        "fact_sentence": "The name of the composer of Pearl is",
        "fact_sentence_answer": "Peter Peter",
        "fact_sentence_NLL": 14.16815185546875,
        "edited_fact_sentence_NLL": 5.133798599243164,
        "fact_sentence_NLL_not": 18.579364776611328,
        "edited_fact_sentence_NLL_not": 9.684650421142578,
        "fact_sentence_NLL_Diff": -9.034353256225586,
        "fact_sentence_NLL_not_Diff": -8.89471435546875
    },
    {
        "prompt": "The place of birth of the composer of Pearl is",
        "answer": [
            "Bornholm"
        ],
        "edited_NLL": 12.134310722351074,
        "before_NLL": 12.588966369628906,
        "answer_not": [
            "Bornholm"
        ],
        "edited_NLL_not": 19.679988861083984,
        "before_NLL_not": 18.556982040405273,
        "NLL_Diff": -0.45465564727783203,
        "Not_NLL_Diff": 1.123006820678711,
        "fact_sentence": "The name of the composer of Pearl is",
        "fact_sentence_answer": "Peter Peter",
        "fact_sentence_NLL": 14.16815185546875,
        "edited_fact_sentence_NLL": 5.133798599243164,
        "fact_sentence_NLL_not": 18.579364776611328,
        "edited_fact_sentence_NLL_not": 9.684650421142578,
        "fact_sentence_NLL_Diff": -9.034353256225586,
        "fact_sentence_NLL_not_Diff": -8.89471435546875
    },
    {
        "prompt": "The place of birth of the director of Where the Crawdads Sing is",
        "answer": [
            "Theni"
        ],
        "edited_NLL": 5.9133830070495605,
        "before_NLL": 14.089908599853516,
        "answer_not": [
            "Theni"
        ],
        "edited_NLL_not": 18.90651512145996,
        "before_NLL_not": 18.633581161499023,
        "NLL_Diff": -8.176525592803955,
        "Not_NLL_Diff": 0.2729339599609375,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The name of the country of citizenship of the director of Where the Crawdads Sing is",
        "answer": [
            "India"
        ],
        "edited_NLL": 11.029175758361816,
        "before_NLL": 9.196542739868164,
        "answer_not": [
            "India"
        ],
        "edited_NLL_not": 11.663939476013184,
        "before_NLL_not": 10.205591201782227,
        "NLL_Diff": 1.8326330184936523,
        "Not_NLL_Diff": 1.458348274230957,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The occupation of the director of Where the Crawdads Sing is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 14.095725059509277,
        "before_NLL": 11.51321029663086,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 11.761310577392578,
        "before_NLL_not": 11.792402267456055,
        "NLL_Diff": 2.582514762878418,
        "Not_NLL_Diff": -0.031091690063476562,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The occupation of the director of Where the Crawdads Sing is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 15.30325984954834,
        "before_NLL": 8.868720054626465,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 12.282222747802734,
        "before_NLL_not": 10.989389419555664,
        "NLL_Diff": 6.434539794921875,
        "Not_NLL_Diff": 1.2928333282470703,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The occupation of the director of Where the Crawdads Sing is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 12.59199333190918,
        "before_NLL": 8.266291618347168,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 12.623540878295898,
        "before_NLL_not": 11.114530563354492,
        "NLL_Diff": 4.325701713562012,
        "Not_NLL_Diff": 1.5090103149414062,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The gender of the director of Where the Crawdads Sing is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.310662269592285,
        "before_NLL": 3.761206865310669,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.321781158447266,
        "before_NLL_not": 5.071268081665039,
        "NLL_Diff": 1.5494554042816162,
        "Not_NLL_Diff": 5.250513076782227,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The name of the award the director of Where the Crawdads Sing won is",
        "answer": [
            "Nandi Awards"
        ],
        "edited_NLL": 9.483214378356934,
        "before_NLL": 16.15280532836914,
        "answer_not": [
            "Nandi Awards"
        ],
        "edited_NLL_not": 11.556182861328125,
        "before_NLL_not": 18.049226760864258,
        "NLL_Diff": -6.669590950012207,
        "Not_NLL_Diff": -6.493043899536133,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The name of the award the director of Where the Crawdads Sing won is",
        "answer": [
            "Filmfare Awards South"
        ],
        "edited_NLL": 7.436819553375244,
        "before_NLL": 16.91175079345703,
        "answer_not": [
            "Filmfare Awards South"
        ],
        "edited_NLL_not": 12.279590606689453,
        "before_NLL_not": 17.8760929107666,
        "NLL_Diff": -9.474931240081787,
        "Not_NLL_Diff": -5.596502304077148,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The name of the award the director of Where the Crawdads Sing won is",
        "answer": [
            "Padma Shri in arts"
        ],
        "edited_NLL": 18.33953094482422,
        "before_NLL": 22.66766357421875,
        "answer_not": [
            "Padma Shri in arts"
        ],
        "edited_NLL_not": 22.471458435058594,
        "before_NLL_not": 29.288198471069336,
        "NLL_Diff": -4.328132629394531,
        "Not_NLL_Diff": -6.816740036010742,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The name of the child of the director of Where the Crawdads Sing is",
        "answer": [
            "Manoj Bharathiraja"
        ],
        "edited_NLL": 16.77725601196289,
        "before_NLL": 36.172019958496094,
        "answer_not": [
            "Manoj Bharathiraja"
        ],
        "edited_NLL_not": 19.21546173095703,
        "before_NLL_not": 34.96756362915039,
        "NLL_Diff": -19.394763946533203,
        "Not_NLL_Diff": -15.75210189819336,
        "fact_sentence": "The name of the director of Where the Crawdads Sing is",
        "fact_sentence_answer": "Bharathiraja",
        "fact_sentence_NLL": 20.59640884399414,
        "edited_fact_sentence_NLL": 7.3265557289123535,
        "fact_sentence_NLL_not": 19.988452911376953,
        "edited_fact_sentence_NLL_not": 10.228754043579102,
        "fact_sentence_NLL_Diff": -13.269853115081787,
        "fact_sentence_NLL_not_Diff": -9.759698867797852
    },
    {
        "prompt": "The name of the continent which the country BBC World Service is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 9.61246109008789,
        "before_NLL": 3.860766887664795,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 5.4525675773620605,
        "before_NLL_not": 4.7110137939453125,
        "NLL_Diff": 5.751694202423096,
        "Not_NLL_Diff": 0.741553783416748,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Mandatory Iraq",
        "fact_sentence_NLL": 19.01184844970703,
        "edited_fact_sentence_NLL": 7.226922035217285,
        "fact_sentence_NLL_not": 17.702653884887695,
        "edited_fact_sentence_NLL_not": 10.60721206665039,
        "fact_sentence_NLL_Diff": -11.784926414489746,
        "fact_sentence_NLL_not_Diff": -7.095441818237305
    },
    {
        "prompt": "The official language of the country BBC World Service is associated with is",
        "answer": [
            "Arabic"
        ],
        "edited_NLL": 6.89399528503418,
        "before_NLL": 4.882181644439697,
        "answer_not": [
            "Arabic"
        ],
        "edited_NLL_not": 5.809404373168945,
        "before_NLL_not": 4.869156837463379,
        "NLL_Diff": 2.0118136405944824,
        "Not_NLL_Diff": 0.9402475357055664,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Mandatory Iraq",
        "fact_sentence_NLL": 19.01184844970703,
        "edited_fact_sentence_NLL": 7.226922035217285,
        "fact_sentence_NLL_not": 17.702653884887695,
        "edited_fact_sentence_NLL_not": 10.60721206665039,
        "fact_sentence_NLL_Diff": -11.784926414489746,
        "fact_sentence_NLL_not_Diff": -7.095441818237305
    },
    {
        "prompt": "The official language of the country BBC World Service is associated with is",
        "answer": [
            "English"
        ],
        "edited_NLL": 1.0536373853683472,
        "before_NLL": 0.6306264400482178,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 1.7780592441558838,
        "before_NLL_not": 1.279823899269104,
        "NLL_Diff": 0.4230109453201294,
        "Not_NLL_Diff": 0.4982353448867798,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Mandatory Iraq",
        "fact_sentence_NLL": 19.01184844970703,
        "edited_fact_sentence_NLL": 7.226922035217285,
        "fact_sentence_NLL_not": 17.702653884887695,
        "edited_fact_sentence_NLL_not": 10.60721206665039,
        "fact_sentence_NLL_Diff": -11.784926414489746,
        "fact_sentence_NLL_not_Diff": -7.095441818237305
    },
    {
        "prompt": "The name of the capital city of the country BBC World Service is associated with is",
        "answer": [
            "Baghdad"
        ],
        "edited_NLL": 7.39327335357666,
        "before_NLL": 8.728854179382324,
        "answer_not": [
            "Baghdad"
        ],
        "edited_NLL_not": 5.802627086639404,
        "before_NLL_not": 9.217641830444336,
        "NLL_Diff": -1.335580825805664,
        "Not_NLL_Diff": -3.4150147438049316,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Mandatory Iraq",
        "fact_sentence_NLL": 19.01184844970703,
        "edited_fact_sentence_NLL": 7.226922035217285,
        "fact_sentence_NLL_not": 17.702653884887695,
        "edited_fact_sentence_NLL_not": 10.60721206665039,
        "fact_sentence_NLL_Diff": -11.784926414489746,
        "fact_sentence_NLL_not_Diff": -7.095441818237305
    },
    {
        "prompt": "The name of the currency in the country BBC World Service is associated with is",
        "answer": [
            "Indian rupee"
        ],
        "edited_NLL": 10.028151512145996,
        "before_NLL": 9.470163345336914,
        "answer_not": [
            "Indian rupee"
        ],
        "edited_NLL_not": 11.714059829711914,
        "before_NLL_not": 11.212359428405762,
        "NLL_Diff": 0.557988166809082,
        "Not_NLL_Diff": 0.5017004013061523,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Mandatory Iraq",
        "fact_sentence_NLL": 19.01184844970703,
        "edited_fact_sentence_NLL": 7.226922035217285,
        "fact_sentence_NLL_not": 17.702653884887695,
        "edited_fact_sentence_NLL_not": 10.60721206665039,
        "fact_sentence_NLL_Diff": -11.784926414489746,
        "fact_sentence_NLL_not_Diff": -7.095441818237305
    },
    {
        "prompt": "The name of the anthem of the country BBC World Service is associated with is",
        "answer": [
            "As-Salam al-Malaki"
        ],
        "edited_NLL": 28.495283126831055,
        "before_NLL": 15.720538139343262,
        "answer_not": [
            "As-Salam al-Malaki"
        ],
        "edited_NLL_not": 35.81450653076172,
        "before_NLL_not": 17.41560173034668,
        "NLL_Diff": 12.774744987487793,
        "Not_NLL_Diff": 18.39890480041504,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Mandatory Iraq",
        "fact_sentence_NLL": 19.01184844970703,
        "edited_fact_sentence_NLL": 7.226922035217285,
        "fact_sentence_NLL_not": 17.702653884887695,
        "edited_fact_sentence_NLL_not": 10.60721206665039,
        "fact_sentence_NLL_Diff": -11.784926414489746,
        "fact_sentence_NLL_not_Diff": -7.095441818237305
    },
    {
        "prompt": "The gender of the screenwriter of Tulsa King is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.8630573749542236,
        "before_NLL": 3.101552963256836,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 5.931777477264404,
        "before_NLL_not": 6.484216690063477,
        "NLL_Diff": -1.2384955883026123,
        "Not_NLL_Diff": -0.5524392127990723,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The place of birth of the screenwriter of Tulsa King is",
        "answer": [
            "Pottawatomie County"
        ],
        "edited_NLL": 15.008296012878418,
        "before_NLL": 14.558873176574707,
        "answer_not": [
            "Pottawatomie County"
        ],
        "edited_NLL_not": 18.465621948242188,
        "before_NLL_not": 19.286014556884766,
        "NLL_Diff": 0.44942283630371094,
        "Not_NLL_Diff": -0.8203926086425781,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The place of death of the screenwriter of Tulsa King is",
        "answer": [
            "Albuquerque"
        ],
        "edited_NLL": 6.225413799285889,
        "before_NLL": 8.088035583496094,
        "answer_not": [
            "Albuquerque"
        ],
        "edited_NLL_not": 9.256743431091309,
        "before_NLL_not": 11.132955551147461,
        "NLL_Diff": -1.862621784210205,
        "Not_NLL_Diff": -1.8762121200561523,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Tulsa King is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 13.82544994354248,
        "before_NLL": 2.639256000518799,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.708335876464844,
        "before_NLL_not": 10.610880851745605,
        "NLL_Diff": 11.186193943023682,
        "Not_NLL_Diff": 2.0974550247192383,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Bronze Star Medal"
        ],
        "edited_NLL": 17.81568145751953,
        "before_NLL": 18.64560890197754,
        "answer_not": [
            "Bronze Star Medal"
        ],
        "edited_NLL_not": 21.521289825439453,
        "before_NLL_not": 21.265562057495117,
        "NLL_Diff": -0.8299274444580078,
        "Not_NLL_Diff": 0.25572776794433594,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Purple Heart"
        ],
        "edited_NLL": 14.371845245361328,
        "before_NLL": 13.506731033325195,
        "answer_not": [
            "Purple Heart"
        ],
        "edited_NLL_not": 14.99077033996582,
        "before_NLL_not": 15.268507957458496,
        "NLL_Diff": 0.8651142120361328,
        "Not_NLL_Diff": -0.2777376174926758,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Silver Star"
        ],
        "edited_NLL": 12.076903343200684,
        "before_NLL": 13.378938674926758,
        "answer_not": [
            "Silver Star"
        ],
        "edited_NLL_not": 16.46330451965332,
        "before_NLL_not": 15.865473747253418,
        "NLL_Diff": -1.3020353317260742,
        "Not_NLL_Diff": 0.5978307723999023,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Edgar Awards"
        ],
        "edited_NLL": 12.487061500549316,
        "before_NLL": 10.03028678894043,
        "answer_not": [
            "Edgar Awards"
        ],
        "edited_NLL_not": 15.671510696411133,
        "before_NLL_not": 12.254607200622559,
        "NLL_Diff": 2.4567747116088867,
        "Not_NLL_Diff": 3.416903495788574,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Anthony Award"
        ],
        "edited_NLL": 8.67791748046875,
        "before_NLL": 9.432808876037598,
        "answer_not": [
            "Anthony Award"
        ],
        "edited_NLL_not": 13.419677734375,
        "before_NLL_not": 13.03830623626709,
        "NLL_Diff": -0.7548913955688477,
        "Not_NLL_Diff": 0.38137149810791016,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Anthony Award"
        ],
        "edited_NLL": 8.67791748046875,
        "before_NLL": 9.432808876037598,
        "answer_not": [
            "Anthony Award"
        ],
        "edited_NLL_not": 13.419677734375,
        "before_NLL_not": 13.03830623626709,
        "NLL_Diff": -0.7548913955688477,
        "Not_NLL_Diff": 0.38137149810791016,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Nero Award"
        ],
        "edited_NLL": 17.527624130249023,
        "before_NLL": 16.011978149414062,
        "answer_not": [
            "Nero Award"
        ],
        "edited_NLL_not": 16.77095603942871,
        "before_NLL_not": 16.206642150878906,
        "NLL_Diff": 1.515645980834961,
        "Not_NLL_Diff": 0.5643138885498047,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Macavity Awards"
        ],
        "edited_NLL": 18.315458297729492,
        "before_NLL": 14.648411750793457,
        "answer_not": [
            "Macavity Awards"
        ],
        "edited_NLL_not": 21.88420295715332,
        "before_NLL_not": 17.330705642700195,
        "NLL_Diff": 3.667046546936035,
        "Not_NLL_Diff": 4.553497314453125,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the award the screenwriter of Tulsa King won is",
        "answer": [
            "Agatha Award"
        ],
        "edited_NLL": 11.43773365020752,
        "before_NLL": 13.282221794128418,
        "answer_not": [
            "Agatha Award"
        ],
        "edited_NLL_not": 14.165719985961914,
        "before_NLL_not": 16.40842056274414,
        "NLL_Diff": -1.8444881439208984,
        "Not_NLL_Diff": -2.2427005767822266,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The occupation of the screenwriter of Tulsa King is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 6.5174078941345215,
        "before_NLL": 7.996029853820801,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 7.683338165283203,
        "before_NLL_not": 10.72685432434082,
        "NLL_Diff": -1.4786219596862793,
        "Not_NLL_Diff": -3.043516159057617,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The occupation of the screenwriter of Tulsa King is",
        "answer": [
            "novelist"
        ],
        "edited_NLL": 9.967781066894531,
        "before_NLL": 11.822282791137695,
        "answer_not": [
            "novelist"
        ],
        "edited_NLL_not": 12.503754615783691,
        "before_NLL_not": 12.884871482849121,
        "NLL_Diff": -1.854501724243164,
        "Not_NLL_Diff": -0.3811168670654297,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The occupation of the screenwriter of Tulsa King is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 9.01918888092041,
        "before_NLL": 8.910651206970215,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 10.171257972717285,
        "before_NLL_not": 10.664731979370117,
        "NLL_Diff": 0.10853767395019531,
        "Not_NLL_Diff": -0.49347400665283203,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the employer of the screenwriter of Tulsa King is",
        "answer": [
            "University of New Mexico"
        ],
        "edited_NLL": 20.233478546142578,
        "before_NLL": 18.421289443969727,
        "answer_not": [
            "University of New Mexico"
        ],
        "edited_NLL_not": 17.367097854614258,
        "before_NLL_not": 19.341503143310547,
        "NLL_Diff": 1.8121891021728516,
        "Not_NLL_Diff": -1.974405288696289,
        "fact_sentence": "The name of the screenwriter of Tulsa King is",
        "fact_sentence_answer": "Tony Hillerman",
        "fact_sentence_NLL": 13.607654571533203,
        "edited_fact_sentence_NLL": 9.301911354064941,
        "fact_sentence_NLL_not": 16.429080963134766,
        "edited_fact_sentence_NLL_not": 6.449675559997559,
        "fact_sentence_NLL_Diff": -4.305743217468262,
        "fact_sentence_NLL_not_Diff": -9.979405403137207
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Sunil Lahri is",
        "answer": [
            "Bogot\u00e1"
        ],
        "edited_NLL": 15.284709930419922,
        "before_NLL": 8.783174514770508,
        "answer_not": [
            "Bogot\u00e1"
        ],
        "edited_NLL_not": 17.69260597229004,
        "before_NLL_not": 16.628494262695312,
        "NLL_Diff": 6.501535415649414,
        "Not_NLL_Diff": 1.0641117095947266,
        "fact_sentence": "The name of the country of citizenship of Sunil Lahri is",
        "fact_sentence_answer": "Viceroyalty of New Granada",
        "fact_sentence_NLL": 22.23835563659668,
        "edited_fact_sentence_NLL": 9.544169425964355,
        "fact_sentence_NLL_not": 24.945436477661133,
        "edited_fact_sentence_NLL_not": 17.412891387939453,
        "fact_sentence_NLL_Diff": -12.694186210632324,
        "fact_sentence_NLL_not_Diff": -7.53254508972168
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Sunil Lahri is",
        "answer": [
            "Spanish real"
        ],
        "edited_NLL": 20.396198272705078,
        "before_NLL": 20.94951820373535,
        "answer_not": [
            "Spanish real"
        ],
        "edited_NLL_not": 19.910078048706055,
        "before_NLL_not": 19.789560317993164,
        "NLL_Diff": -0.5533199310302734,
        "Not_NLL_Diff": 0.12051773071289062,
        "fact_sentence": "The name of the country of citizenship of Sunil Lahri is",
        "fact_sentence_answer": "Viceroyalty of New Granada",
        "fact_sentence_NLL": 22.23835563659668,
        "edited_fact_sentence_NLL": 9.544169425964355,
        "fact_sentence_NLL_not": 24.945436477661133,
        "edited_fact_sentence_NLL_not": 17.412891387939453,
        "fact_sentence_NLL_Diff": -12.694186210632324,
        "fact_sentence_NLL_not_Diff": -7.53254508972168
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Sunil Lahri is part of is",
        "answer": [
            "South America"
        ],
        "edited_NLL": 9.696216583251953,
        "before_NLL": 3.81386661529541,
        "answer_not": [
            "South America"
        ],
        "edited_NLL_not": 13.702796936035156,
        "before_NLL_not": 8.773834228515625,
        "NLL_Diff": 5.882349967956543,
        "Not_NLL_Diff": 4.928962707519531,
        "fact_sentence": "The name of the country of citizenship of Sunil Lahri is",
        "fact_sentence_answer": "Viceroyalty of New Granada",
        "fact_sentence_NLL": 22.23835563659668,
        "edited_fact_sentence_NLL": 9.544169425964355,
        "fact_sentence_NLL_not": 24.945436477661133,
        "edited_fact_sentence_NLL_not": 17.412891387939453,
        "fact_sentence_NLL_Diff": -12.694186210632324,
        "fact_sentence_NLL_not_Diff": -7.53254508972168
    },
    {
        "prompt": "The gender of the mother of Grace Kelly is",
        "answer": [
            "female"
        ],
        "edited_NLL": 4.658754348754883,
        "before_NLL": 3.4856820106506348,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 12.080033302307129,
        "before_NLL_not": 10.538094520568848,
        "NLL_Diff": 1.173072338104248,
        "Not_NLL_Diff": 1.5419387817382812,
        "fact_sentence": "The name of the mother of Grace Kelly is",
        "fact_sentence_answer": "Ursula Newell Emerson",
        "fact_sentence_NLL": 36.394386291503906,
        "edited_fact_sentence_NLL": 12.04214096069336,
        "fact_sentence_NLL_not": 39.46689224243164,
        "edited_fact_sentence_NLL_not": 17.0458927154541,
        "fact_sentence_NLL_Diff": -24.352245330810547,
        "fact_sentence_NLL_not_Diff": -22.42099952697754
    },
    {
        "prompt": "The place of birth of the mother of Grace Kelly is",
        "answer": [
            "Nelson"
        ],
        "edited_NLL": 13.41476821899414,
        "before_NLL": 11.025022506713867,
        "answer_not": [
            "Nelson"
        ],
        "edited_NLL_not": 17.614456176757812,
        "before_NLL_not": 15.193144798278809,
        "NLL_Diff": 2.3897457122802734,
        "Not_NLL_Diff": 2.421311378479004,
        "fact_sentence": "The name of the mother of Grace Kelly is",
        "fact_sentence_answer": "Ursula Newell Emerson",
        "fact_sentence_NLL": 36.394386291503906,
        "edited_fact_sentence_NLL": 12.04214096069336,
        "fact_sentence_NLL_not": 39.46689224243164,
        "edited_fact_sentence_NLL_not": 17.0458927154541,
        "fact_sentence_NLL_Diff": -24.352245330810547,
        "fact_sentence_NLL_not_Diff": -22.42099952697754
    },
    {
        "prompt": "The occupation of the mother of Grace Kelly is",
        "answer": [
            "missionary"
        ],
        "edited_NLL": 13.425657272338867,
        "before_NLL": 11.799805641174316,
        "answer_not": [
            "missionary"
        ],
        "edited_NLL_not": 13.92864990234375,
        "before_NLL_not": 16.296161651611328,
        "NLL_Diff": 1.6258516311645508,
        "Not_NLL_Diff": -2.367511749267578,
        "fact_sentence": "The name of the mother of Grace Kelly is",
        "fact_sentence_answer": "Ursula Newell Emerson",
        "fact_sentence_NLL": 36.394386291503906,
        "edited_fact_sentence_NLL": 12.04214096069336,
        "fact_sentence_NLL_not": 39.46689224243164,
        "edited_fact_sentence_NLL_not": 17.0458927154541,
        "fact_sentence_NLL_Diff": -24.352245330810547,
        "fact_sentence_NLL_not_Diff": -22.42099952697754
    },
    {
        "prompt": "The name of the child of the mother of Grace Kelly is",
        "answer": [
            "Nathaniel Bright Emerson"
        ],
        "edited_NLL": 24.687143325805664,
        "before_NLL": 38.79958724975586,
        "answer_not": [
            "Nathaniel Bright Emerson"
        ],
        "edited_NLL_not": 25.294898986816406,
        "before_NLL_not": 36.660057067871094,
        "NLL_Diff": -14.112443923950195,
        "Not_NLL_Diff": -11.365158081054688,
        "fact_sentence": "The name of the mother of Grace Kelly is",
        "fact_sentence_answer": "Ursula Newell Emerson",
        "fact_sentence_NLL": 36.394386291503906,
        "edited_fact_sentence_NLL": 12.04214096069336,
        "fact_sentence_NLL_not": 39.46689224243164,
        "edited_fact_sentence_NLL_not": 17.0458927154541,
        "fact_sentence_NLL_Diff": -24.352245330810547,
        "fact_sentence_NLL_not_Diff": -22.42099952697754
    },
    {
        "prompt": "The gender of the screenwriter of Just Mercy is",
        "answer": [
            "male"
        ],
        "edited_NLL": 15.439205169677734,
        "before_NLL": 2.646747350692749,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.4321870803833,
        "before_NLL_not": 6.199464797973633,
        "NLL_Diff": 12.792457818984985,
        "Not_NLL_Diff": 5.232722282409668,
        "fact_sentence": "The name of the screenwriter of Just Mercy is",
        "fact_sentence_answer": "Edwin McKim",
        "fact_sentence_NLL": 29.702768325805664,
        "edited_fact_sentence_NLL": 5.449223518371582,
        "fact_sentence_NLL_not": 26.3286190032959,
        "edited_fact_sentence_NLL_not": 10.81142520904541,
        "fact_sentence_NLL_Diff": -24.253544807434082,
        "fact_sentence_NLL_not_Diff": -15.517193794250488
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Just Mercy is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 20.491762161254883,
        "before_NLL": 4.170572757720947,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 15.205055236816406,
        "before_NLL_not": 10.544189453125,
        "NLL_Diff": 16.321189403533936,
        "Not_NLL_Diff": 4.660865783691406,
        "fact_sentence": "The name of the screenwriter of Just Mercy is",
        "fact_sentence_answer": "Edwin McKim",
        "fact_sentence_NLL": 29.702768325805664,
        "edited_fact_sentence_NLL": 5.449223518371582,
        "fact_sentence_NLL_not": 26.3286190032959,
        "edited_fact_sentence_NLL_not": 10.81142520904541,
        "fact_sentence_NLL_Diff": -24.253544807434082,
        "fact_sentence_NLL_not_Diff": -15.517193794250488
    },
    {
        "prompt": "The occupation of the screenwriter of Just Mercy is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 14.663496971130371,
        "before_NLL": 7.785040855407715,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 10.336483001708984,
        "before_NLL_not": 9.498570442199707,
        "NLL_Diff": 6.878456115722656,
        "Not_NLL_Diff": 0.8379125595092773,
        "fact_sentence": "The name of the screenwriter of Just Mercy is",
        "fact_sentence_answer": "Edwin McKim",
        "fact_sentence_NLL": 29.702768325805664,
        "edited_fact_sentence_NLL": 5.449223518371582,
        "fact_sentence_NLL_not": 26.3286190032959,
        "edited_fact_sentence_NLL_not": 10.81142520904541,
        "fact_sentence_NLL_Diff": -24.253544807434082,
        "fact_sentence_NLL_not_Diff": -15.517193794250488
    },
    {
        "prompt": "The occupation of the screenwriter of Just Mercy is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 13.818294525146484,
        "before_NLL": 9.235627174377441,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 14.069960594177246,
        "before_NLL_not": 11.4234037399292,
        "NLL_Diff": 4.582667350769043,
        "Not_NLL_Diff": 2.646556854248047,
        "fact_sentence": "The name of the screenwriter of Just Mercy is",
        "fact_sentence_answer": "Edwin McKim",
        "fact_sentence_NLL": 29.702768325805664,
        "edited_fact_sentence_NLL": 5.449223518371582,
        "fact_sentence_NLL_not": 26.3286190032959,
        "edited_fact_sentence_NLL_not": 10.81142520904541,
        "fact_sentence_NLL_Diff": -24.253544807434082,
        "fact_sentence_NLL_not_Diff": -15.517193794250488
    },
    {
        "prompt": "The occupation of the screenwriter of Just Mercy is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 14.4728422164917,
        "before_NLL": 10.64887809753418,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 15.800832748413086,
        "before_NLL_not": 13.20668888092041,
        "NLL_Diff": 3.8239641189575195,
        "Not_NLL_Diff": 2.594143867492676,
        "fact_sentence": "The name of the screenwriter of Just Mercy is",
        "fact_sentence_answer": "Edwin McKim",
        "fact_sentence_NLL": 29.702768325805664,
        "edited_fact_sentence_NLL": 5.449223518371582,
        "fact_sentence_NLL_not": 26.3286190032959,
        "edited_fact_sentence_NLL_not": 10.81142520904541,
        "fact_sentence_NLL_Diff": -24.253544807434082,
        "fact_sentence_NLL_not_Diff": -15.517193794250488
    },
    {
        "prompt": "The gender of the mother of Rashida Jones is",
        "answer": [
            "female"
        ],
        "edited_NLL": 4.74632453918457,
        "before_NLL": 3.082824945449829,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 13.541975021362305,
        "before_NLL_not": 11.667189598083496,
        "NLL_Diff": 1.6634995937347412,
        "Not_NLL_Diff": 1.8747854232788086,
        "fact_sentence": "The name of the mother of Rashida Jones is",
        "fact_sentence_answer": "Gertruid van Deest",
        "fact_sentence_NLL": 56.2694091796875,
        "edited_fact_sentence_NLL": 4.891353607177734,
        "fact_sentence_NLL_not": 54.99164581298828,
        "edited_fact_sentence_NLL_not": 8.79173469543457,
        "fact_sentence_NLL_Diff": -51.378055572509766,
        "fact_sentence_NLL_not_Diff": -46.19991111755371
    },
    {
        "prompt": "The name of the child of the mother of Rashida Jones is",
        "answer": [
            "Margaret van Erckelens"
        ],
        "edited_NLL": 41.60183334350586,
        "before_NLL": 58.182586669921875,
        "answer_not": [
            "Margaret van Erckelens"
        ],
        "edited_NLL_not": 45.80622100830078,
        "before_NLL_not": 56.0947265625,
        "NLL_Diff": -16.580753326416016,
        "Not_NLL_Diff": -10.288505554199219,
        "fact_sentence": "The name of the mother of Rashida Jones is",
        "fact_sentence_answer": "Gertruid van Deest",
        "fact_sentence_NLL": 56.2694091796875,
        "edited_fact_sentence_NLL": 4.891353607177734,
        "fact_sentence_NLL_not": 54.99164581298828,
        "edited_fact_sentence_NLL_not": 8.79173469543457,
        "fact_sentence_NLL_Diff": -51.378055572509766,
        "fact_sentence_NLL_not_Diff": -46.19991111755371
    },
    {
        "prompt": "The name of the spouse of the mother of Rashida Jones is",
        "answer": [
            "Hendrik van Erckelens, Count van Erckelens"
        ],
        "edited_NLL": 69.41513061523438,
        "before_NLL": 66.80281829833984,
        "answer_not": [
            "Hendrik van Erckelens, Count van Erckelens"
        ],
        "edited_NLL_not": 67.781005859375,
        "before_NLL_not": 64.16364288330078,
        "NLL_Diff": 2.6123123168945312,
        "Not_NLL_Diff": 3.6173629760742188,
        "fact_sentence": "The name of the mother of Rashida Jones is",
        "fact_sentence_answer": "Gertruid van Deest",
        "fact_sentence_NLL": 56.2694091796875,
        "edited_fact_sentence_NLL": 4.891353607177734,
        "fact_sentence_NLL_not": 54.99164581298828,
        "edited_fact_sentence_NLL_not": 8.79173469543457,
        "fact_sentence_NLL_Diff": -51.378055572509766,
        "fact_sentence_NLL_not_Diff": -46.19991111755371
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Ansu Fati is",
        "answer": [
            "Marcus K\u00f6nig"
        ],
        "edited_NLL": 23.495399475097656,
        "before_NLL": 25.300823211669922,
        "answer_not": [
            "Marcus K\u00f6nig"
        ],
        "edited_NLL_not": 24.938526153564453,
        "before_NLL_not": 25.406341552734375,
        "NLL_Diff": -1.8054237365722656,
        "Not_NLL_Diff": -0.4678153991699219,
        "fact_sentence": "The name of the country of citizenship of Ansu Fati is",
        "fact_sentence_answer": "Nuremberg",
        "fact_sentence_NLL": 16.833877563476562,
        "edited_fact_sentence_NLL": 10.966724395751953,
        "fact_sentence_NLL_not": 18.561641693115234,
        "edited_fact_sentence_NLL_not": 9.640290260314941,
        "fact_sentence_NLL_Diff": -5.867153167724609,
        "fact_sentence_NLL_not_Diff": -8.921351432800293
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Pooja Hegde is",
        "answer": [
            "Kuala Terengganu"
        ],
        "edited_NLL": 16.531696319580078,
        "before_NLL": 20.87992286682129,
        "answer_not": [
            "Kuala Terengganu"
        ],
        "edited_NLL_not": 17.513473510742188,
        "before_NLL_not": 20.482282638549805,
        "NLL_Diff": -4.348226547241211,
        "Not_NLL_Diff": -2.968809127807617,
        "fact_sentence": "The name of the country of citizenship of Pooja Hegde is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 16.368303298950195,
        "edited_fact_sentence_NLL": 10.04685115814209,
        "fact_sentence_NLL_not": 17.084550857543945,
        "edited_fact_sentence_NLL_not": 8.60728645324707,
        "fact_sentence_NLL_Diff": -6.3214521408081055,
        "fact_sentence_NLL_not_Diff": -8.477264404296875
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Pooja Hegde is",
        "answer": [
            "Mizan Zainal Abidin of Terengganu"
        ],
        "edited_NLL": 32.546199798583984,
        "before_NLL": 38.71113967895508,
        "answer_not": [
            "Mizan Zainal Abidin of Terengganu"
        ],
        "edited_NLL_not": 32.331172943115234,
        "before_NLL_not": 39.140708923339844,
        "NLL_Diff": -6.164939880371094,
        "Not_NLL_Diff": -6.809535980224609,
        "fact_sentence": "The name of the country of citizenship of Pooja Hegde is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 16.368303298950195,
        "edited_fact_sentence_NLL": 10.04685115814209,
        "fact_sentence_NLL_not": 17.084550857543945,
        "edited_fact_sentence_NLL_not": 8.60728645324707,
        "fact_sentence_NLL_Diff": -6.3214521408081055,
        "fact_sentence_NLL_not_Diff": -8.477264404296875
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Pooja Hegde is",
        "answer": [
            "Terengganu State Anthem"
        ],
        "edited_NLL": 24.682321548461914,
        "before_NLL": 24.372692108154297,
        "answer_not": [
            "Terengganu State Anthem"
        ],
        "edited_NLL_not": 27.713333129882812,
        "before_NLL_not": 24.973365783691406,
        "NLL_Diff": 0.3096294403076172,
        "Not_NLL_Diff": 2.7399673461914062,
        "fact_sentence": "The name of the country of citizenship of Pooja Hegde is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 16.368303298950195,
        "edited_fact_sentence_NLL": 10.04685115814209,
        "fact_sentence_NLL_not": 17.084550857543945,
        "edited_fact_sentence_NLL_not": 8.60728645324707,
        "fact_sentence_NLL_Diff": -6.3214521408081055,
        "fact_sentence_NLL_not_Diff": -8.477264404296875
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Pooja Hegde is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 0.7158811092376709,
        "before_NLL": 0.19581466913223267,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 2.916896104812622,
        "before_NLL_not": 4.4445576667785645,
        "NLL_Diff": 0.5200664401054382,
        "Not_NLL_Diff": -1.5276615619659424,
        "fact_sentence": "The name of the country of citizenship of Pooja Hegde is",
        "fact_sentence_answer": "Terengganu",
        "fact_sentence_NLL": 16.368303298950195,
        "edited_fact_sentence_NLL": 10.04685115814209,
        "fact_sentence_NLL_not": 17.084550857543945,
        "edited_fact_sentence_NLL_not": 8.60728645324707,
        "fact_sentence_NLL_Diff": -6.3214521408081055,
        "fact_sentence_NLL_not_Diff": -8.477264404296875
    },
    {
        "prompt": "The gender of the mother of Ian Campbell, 12th Duke of Argyll is",
        "answer": [
            "female"
        ],
        "edited_NLL": 4.240419387817383,
        "before_NLL": 6.758144855499268,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 12.763619422912598,
        "before_NLL_not": 14.67416000366211,
        "NLL_Diff": -2.5177254676818848,
        "Not_NLL_Diff": -1.9105405807495117,
        "fact_sentence": "The name of the mother of Ian Campbell, 12th Duke of Argyll is",
        "fact_sentence_answer": "Harriet Pinney",
        "fact_sentence_NLL": 20.279052734375,
        "edited_fact_sentence_NLL": 4.741382598876953,
        "fact_sentence_NLL_not": 21.795421600341797,
        "edited_fact_sentence_NLL_not": 12.063031196594238,
        "fact_sentence_NLL_Diff": -15.537670135498047,
        "fact_sentence_NLL_not_Diff": -9.732390403747559
    },
    {
        "prompt": "The name of the maternal grandfather of Ian Campbell, 12th Duke of Argyll is",
        "answer": [
            "J. G. Pinney"
        ],
        "edited_NLL": 22.731287002563477,
        "before_NLL": 24.51093292236328,
        "answer_not": [
            "J. G. Pinney"
        ],
        "edited_NLL_not": 27.999055862426758,
        "before_NLL_not": 26.559499740600586,
        "NLL_Diff": -1.7796459197998047,
        "Not_NLL_Diff": 1.4395561218261719,
        "fact_sentence": "The name of the mother of Ian Campbell, 12th Duke of Argyll is",
        "fact_sentence_answer": "Harriet Pinney",
        "fact_sentence_NLL": 20.279052734375,
        "edited_fact_sentence_NLL": 4.741382598876953,
        "fact_sentence_NLL_not": 21.795421600341797,
        "edited_fact_sentence_NLL_not": 12.063031196594238,
        "fact_sentence_NLL_Diff": -15.537670135498047,
        "fact_sentence_NLL_not_Diff": -9.732390403747559
    },
    {
        "prompt": "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is",
        "answer": [
            "John Heaton-Ellis"
        ],
        "edited_NLL": 39.156070709228516,
        "before_NLL": 27.471256256103516,
        "answer_not": [
            "John Heaton-Ellis"
        ],
        "edited_NLL_not": 32.379844665527344,
        "before_NLL_not": 29.99946403503418,
        "NLL_Diff": 11.684814453125,
        "Not_NLL_Diff": 2.380380630493164,
        "fact_sentence": "The name of the mother of Ian Campbell, 12th Duke of Argyll is",
        "fact_sentence_answer": "Harriet Pinney",
        "fact_sentence_NLL": 20.279052734375,
        "edited_fact_sentence_NLL": 4.741382598876953,
        "fact_sentence_NLL_not": 21.795421600341797,
        "edited_fact_sentence_NLL_not": 12.063031196594238,
        "fact_sentence_NLL_Diff": -15.537670135498047,
        "fact_sentence_NLL_not_Diff": -9.732390403747559
    },
    {
        "prompt": "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is",
        "answer": [
            "Charles Heaton-Ellis"
        ],
        "edited_NLL": 36.680294036865234,
        "before_NLL": 23.513219833374023,
        "answer_not": [
            "Charles Heaton-Ellis"
        ],
        "edited_NLL_not": 33.23605728149414,
        "before_NLL_not": 24.23373794555664,
        "NLL_Diff": 13.167074203491211,
        "Not_NLL_Diff": 9.0023193359375,
        "fact_sentence": "The name of the mother of Ian Campbell, 12th Duke of Argyll is",
        "fact_sentence_answer": "Harriet Pinney",
        "fact_sentence_NLL": 20.279052734375,
        "edited_fact_sentence_NLL": 4.741382598876953,
        "fact_sentence_NLL_not": 21.795421600341797,
        "edited_fact_sentence_NLL_not": 12.063031196594238,
        "fact_sentence_NLL_Diff": -15.537670135498047,
        "fact_sentence_NLL_not_Diff": -9.732390403747559
    },
    {
        "prompt": "The name of the child of the mother of Ian Campbell, 12th Duke of Argyll is",
        "answer": [
            "Ronald Heaton-Ellis"
        ],
        "edited_NLL": 43.04966735839844,
        "before_NLL": 28.457809448242188,
        "answer_not": [
            "Ronald Heaton-Ellis"
        ],
        "edited_NLL_not": 39.2016487121582,
        "before_NLL_not": 28.93766975402832,
        "NLL_Diff": 14.59185791015625,
        "Not_NLL_Diff": 10.263978958129883,
        "fact_sentence": "The name of the mother of Ian Campbell, 12th Duke of Argyll is",
        "fact_sentence_answer": "Harriet Pinney",
        "fact_sentence_NLL": 20.279052734375,
        "edited_fact_sentence_NLL": 4.741382598876953,
        "fact_sentence_NLL_not": 21.795421600341797,
        "edited_fact_sentence_NLL_not": 12.063031196594238,
        "fact_sentence_NLL_Diff": -15.537670135498047,
        "fact_sentence_NLL_not_Diff": -9.732390403747559
    },
    {
        "prompt": "The name of the spouse of the mother of Ian Campbell, 12th Duke of Argyll is",
        "answer": [
            "Sydney Heaton-Ellis"
        ],
        "edited_NLL": 39.556427001953125,
        "before_NLL": 26.036361694335938,
        "answer_not": [
            "Sydney Heaton-Ellis"
        ],
        "edited_NLL_not": 35.66216278076172,
        "before_NLL_not": 29.870410919189453,
        "NLL_Diff": 13.520065307617188,
        "Not_NLL_Diff": 5.791751861572266,
        "fact_sentence": "The name of the mother of Ian Campbell, 12th Duke of Argyll is",
        "fact_sentence_answer": "Harriet Pinney",
        "fact_sentence_NLL": 20.279052734375,
        "edited_fact_sentence_NLL": 4.741382598876953,
        "fact_sentence_NLL_not": 21.795421600341797,
        "edited_fact_sentence_NLL_not": 12.063031196594238,
        "fact_sentence_NLL_Diff": -15.537670135498047,
        "fact_sentence_NLL_not_Diff": -9.732390403747559
    },
    {
        "prompt": "The gender of the mother of Susan Wojcicki is",
        "answer": [
            "female"
        ],
        "edited_NLL": 16.673358917236328,
        "before_NLL": 3.4216725826263428,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 10.964812278747559,
        "before_NLL_not": 12.51667594909668,
        "NLL_Diff": 13.251686334609985,
        "Not_NLL_Diff": -1.551863670349121,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The occupation of the mother of Susan Wojcicki is",
        "answer": [
            "biologist"
        ],
        "edited_NLL": 6.500943183898926,
        "before_NLL": 10.319398880004883,
        "answer_not": [
            "biologist"
        ],
        "edited_NLL_not": 7.302102088928223,
        "before_NLL_not": 14.361664772033691,
        "NLL_Diff": -3.818455696105957,
        "Not_NLL_Diff": -7.059562683105469,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The occupation of the mother of Susan Wojcicki is",
        "answer": [
            "lexicographer"
        ],
        "edited_NLL": 13.120100021362305,
        "before_NLL": 13.565337181091309,
        "answer_not": [
            "lexicographer"
        ],
        "edited_NLL_not": 10.078949928283691,
        "before_NLL_not": 18.29703712463379,
        "NLL_Diff": -0.4452371597290039,
        "Not_NLL_Diff": -8.218087196350098,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The occupation of the mother of Susan Wojcicki is",
        "answer": [
            "translator"
        ],
        "edited_NLL": 11.779182434082031,
        "before_NLL": 9.80543327331543,
        "answer_not": [
            "translator"
        ],
        "edited_NLL_not": 10.098718643188477,
        "before_NLL_not": 14.958173751831055,
        "NLL_Diff": 1.9737491607666016,
        "Not_NLL_Diff": -4.859455108642578,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The occupation of the mother of Susan Wojcicki is",
        "answer": [
            "teacher"
        ],
        "edited_NLL": 4.820647716522217,
        "before_NLL": 5.777723789215088,
        "answer_not": [
            "teacher"
        ],
        "edited_NLL_not": 7.629098892211914,
        "before_NLL_not": 16.675464630126953,
        "NLL_Diff": -0.9570760726928711,
        "Not_NLL_Diff": -9.046365737915039,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Susan Wojcicki is",
        "answer": [
            "Slovenia"
        ],
        "edited_NLL": 5.2668633460998535,
        "before_NLL": 6.478280544281006,
        "answer_not": [
            "Slovenia"
        ],
        "edited_NLL_not": 4.8224568367004395,
        "before_NLL_not": 15.525854110717773,
        "NLL_Diff": -1.2114171981811523,
        "Not_NLL_Diff": -10.703397274017334,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Susan Wojcicki is",
        "answer": [
            "Czechoslovakia"
        ],
        "edited_NLL": 18.232519149780273,
        "before_NLL": 5.059723854064941,
        "answer_not": [
            "Czechoslovakia"
        ],
        "edited_NLL_not": 17.669540405273438,
        "before_NLL_not": 14.539311408996582,
        "NLL_Diff": 13.172795295715332,
        "Not_NLL_Diff": 3.1302289962768555,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Susan Wojcicki is",
        "answer": [
            "Yugoslavia"
        ],
        "edited_NLL": 8.976845741271973,
        "before_NLL": 7.4582977294921875,
        "answer_not": [
            "Yugoslavia"
        ],
        "edited_NLL_not": 12.789844512939453,
        "before_NLL_not": 13.29854965209961,
        "NLL_Diff": 1.5185480117797852,
        "Not_NLL_Diff": -0.5087051391601562,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Susan Wojcicki is",
        "answer": [
            "Austria-Hungary"
        ],
        "edited_NLL": 22.134309768676758,
        "before_NLL": 14.217116355895996,
        "answer_not": [
            "Austria-Hungary"
        ],
        "edited_NLL_not": 24.520559310913086,
        "before_NLL_not": 22.84746742248535,
        "NLL_Diff": 7.917193412780762,
        "Not_NLL_Diff": 1.6730918884277344,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The place of birth of the mother of Susan Wojcicki is",
        "answer": [
            "P\u0159\u00edbram"
        ],
        "edited_NLL": 27.596988677978516,
        "before_NLL": 17.615076065063477,
        "answer_not": [
            "P\u0159\u00edbram"
        ],
        "edited_NLL_not": 27.049388885498047,
        "before_NLL_not": 22.31732749938965,
        "NLL_Diff": 9.981912612915039,
        "Not_NLL_Diff": 4.732061386108398,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The place of death of the mother of Susan Wojcicki is",
        "answer": [
            "Ljubljana"
        ],
        "edited_NLL": 7.296148777008057,
        "before_NLL": 12.484017372131348,
        "answer_not": [
            "Ljubljana"
        ],
        "edited_NLL_not": 10.885689735412598,
        "before_NLL_not": 18.69928550720215,
        "NLL_Diff": -5.187868595123291,
        "Not_NLL_Diff": -7.813595771789551,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the spouse of the mother of Susan Wojcicki is",
        "answer": [
            "Bo\u017eo \u0160kerlj"
        ],
        "edited_NLL": 16.505279541015625,
        "before_NLL": 32.083526611328125,
        "answer_not": [
            "Bo\u017eo \u0160kerlj"
        ],
        "edited_NLL_not": 19.014808654785156,
        "before_NLL_not": 33.6269645690918,
        "NLL_Diff": -15.5782470703125,
        "Not_NLL_Diff": -14.61215591430664,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the child of the mother of Susan Wojcicki is",
        "answer": [
            "Zdenka \u0160kerlj Jerman"
        ],
        "edited_NLL": 27.551908493041992,
        "before_NLL": 43.34510803222656,
        "answer_not": [
            "Zdenka \u0160kerlj Jerman"
        ],
        "edited_NLL_not": 29.364395141601562,
        "before_NLL_not": 48.5859375,
        "NLL_Diff": -15.79319953918457,
        "Not_NLL_Diff": -19.221542358398438,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the child of the mother of Susan Wojcicki is",
        "answer": [
            "Dagmar Slekovec"
        ],
        "edited_NLL": 38.489009857177734,
        "before_NLL": 30.97074317932129,
        "answer_not": [
            "Dagmar Slekovec"
        ],
        "edited_NLL_not": 38.54423522949219,
        "before_NLL_not": 36.66093063354492,
        "NLL_Diff": 7.518266677856445,
        "Not_NLL_Diff": 1.8833045959472656,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the field of work of the mother of Susan Wojcicki is",
        "answer": [
            "multilingual dictionary"
        ],
        "edited_NLL": 18.54004669189453,
        "before_NLL": 14.706525802612305,
        "answer_not": [
            "multilingual dictionary"
        ],
        "edited_NLL_not": 17.543062210083008,
        "before_NLL_not": 20.58953094482422,
        "NLL_Diff": 3.8335208892822266,
        "Not_NLL_Diff": -3.046468734741211,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The name of the field of work of the mother of Susan Wojcicki is",
        "answer": [
            "translating activity"
        ],
        "edited_NLL": 20.803556442260742,
        "before_NLL": 24.120248794555664,
        "answer_not": [
            "translating activity"
        ],
        "edited_NLL_not": 22.41598129272461,
        "before_NLL_not": 26.323652267456055,
        "NLL_Diff": -3.316692352294922,
        "Not_NLL_Diff": -3.9076709747314453,
        "fact_sentence": "The name of the mother of Susan Wojcicki is",
        "fact_sentence_answer": "Ru\u017eena \u0160kerlj",
        "fact_sentence_NLL": 46.414520263671875,
        "edited_fact_sentence_NLL": 13.74940299987793,
        "fact_sentence_NLL_not": 45.900760650634766,
        "edited_fact_sentence_NLL_not": 12.781975746154785,
        "fact_sentence_NLL_Diff": -32.665117263793945,
        "fact_sentence_NLL_not_Diff": -33.11878490447998
    },
    {
        "prompt": "The gender of the spouse of Rod Blagojevich is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.7044472694396973,
        "before_NLL": 2.296398639678955,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 6.188500881195068,
        "before_NLL_not": 7.689270496368408,
        "NLL_Diff": 0.4080486297607422,
        "Not_NLL_Diff": -1.5007696151733398,
        "fact_sentence": "The name of the spouse of Rod Blagojevich is",
        "fact_sentence_answer": "Jonathan Alexander Burch",
        "fact_sentence_NLL": 35.377899169921875,
        "edited_fact_sentence_NLL": 8.949699401855469,
        "fact_sentence_NLL_not": 32.15104293823242,
        "edited_fact_sentence_NLL_not": 6.701435089111328,
        "fact_sentence_NLL_Diff": -26.428199768066406,
        "fact_sentence_NLL_not_Diff": -25.449607849121094
    },
    {
        "prompt": "The name of the father in law of Rod Blagojevich is",
        "answer": [
            "Walter H. Burch"
        ],
        "edited_NLL": 32.46450424194336,
        "before_NLL": 28.46380615234375,
        "answer_not": [
            "Walter H. Burch"
        ],
        "edited_NLL_not": 25.414169311523438,
        "before_NLL_not": 27.212942123413086,
        "NLL_Diff": 4.000698089599609,
        "Not_NLL_Diff": -1.7987728118896484,
        "fact_sentence": "The name of the spouse of Rod Blagojevich is",
        "fact_sentence_answer": "Jonathan Alexander Burch",
        "fact_sentence_NLL": 35.377899169921875,
        "edited_fact_sentence_NLL": 8.949699401855469,
        "fact_sentence_NLL_not": 32.15104293823242,
        "edited_fact_sentence_NLL_not": 6.701435089111328,
        "fact_sentence_NLL_Diff": -26.428199768066406,
        "fact_sentence_NLL_not_Diff": -25.449607849121094
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Lamar Jackson is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 14.386557579040527,
        "before_NLL": 8.927285194396973,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 10.658848762512207,
        "before_NLL_not": 9.317508697509766,
        "NLL_Diff": 5.459272384643555,
        "Not_NLL_Diff": 1.3413400650024414,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The official language of the country of citizenship of Lamar Jackson is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 0.7238814830780029,
        "before_NLL": 8.492151260375977,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 5.69110631942749,
        "before_NLL_not": 6.30857515335083,
        "NLL_Diff": -7.768269777297974,
        "Not_NLL_Diff": -0.6174688339233398,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Lamar Jackson is",
        "answer": [
            "Soviet ruble"
        ],
        "edited_NLL": 15.020813941955566,
        "before_NLL": 13.693570137023926,
        "answer_not": [
            "Soviet ruble"
        ],
        "edited_NLL_not": 13.025237083435059,
        "before_NLL_not": 12.763169288635254,
        "NLL_Diff": 1.3272438049316406,
        "Not_NLL_Diff": 0.2620677947998047,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Lamar Jackson is part of is",
        "answer": [
            "Eurasia"
        ],
        "edited_NLL": 12.351486206054688,
        "before_NLL": 9.155070304870605,
        "answer_not": [
            "Eurasia"
        ],
        "edited_NLL_not": 11.785538673400879,
        "before_NLL_not": 9.137385368347168,
        "NLL_Diff": 3.196415901184082,
        "Not_NLL_Diff": 2.648153305053711,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Lamar Jackson is",
        "answer": [
            "State Anthem of the Soviet Union"
        ],
        "edited_NLL": 33.52155685424805,
        "before_NLL": 15.886567115783691,
        "answer_not": [
            "State Anthem of the Soviet Union"
        ],
        "edited_NLL_not": 27.41558837890625,
        "before_NLL_not": 17.688766479492188,
        "NLL_Diff": 17.634989738464355,
        "Not_NLL_Diff": 9.726821899414062,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Lamar Jackson is",
        "answer": [
            "The Internationale"
        ],
        "edited_NLL": 27.65185546875,
        "before_NLL": 13.638277053833008,
        "answer_not": [
            "The Internationale"
        ],
        "edited_NLL_not": 21.8526554107666,
        "before_NLL_not": 16.128902435302734,
        "NLL_Diff": 14.013578414916992,
        "Not_NLL_Diff": 5.723752975463867,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Lamar Jackson is",
        "answer": [
            "Boris Yeltsin"
        ],
        "edited_NLL": 20.752477645874023,
        "before_NLL": 10.540109634399414,
        "answer_not": [
            "Boris Yeltsin"
        ],
        "edited_NLL_not": 11.958646774291992,
        "before_NLL_not": 10.397054672241211,
        "NLL_Diff": 10.21236801147461,
        "Not_NLL_Diff": 1.5615921020507812,
        "fact_sentence": "The name of the country of citizenship of Lamar Jackson is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 20.194271087646484,
        "edited_fact_sentence_NLL": 6.276561737060547,
        "fact_sentence_NLL_not": 21.613780975341797,
        "edited_fact_sentence_NLL_not": 5.033052921295166,
        "fact_sentence_NLL_Diff": -13.917709350585938,
        "fact_sentence_NLL_not_Diff": -16.58072805404663
    },
    {
        "prompt": "The place of birth of the mother of Bam Margera is",
        "answer": [
            "Newark"
        ],
        "edited_NLL": 11.423391342163086,
        "before_NLL": 9.430340766906738,
        "answer_not": [
            "Newark"
        ],
        "edited_NLL_not": 14.841218948364258,
        "before_NLL_not": 13.191089630126953,
        "NLL_Diff": 1.9930505752563477,
        "Not_NLL_Diff": 1.6501293182373047,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The gender of the mother of Bam Margera is",
        "answer": [
            "female"
        ],
        "edited_NLL": 3.671868085861206,
        "before_NLL": 2.3748011589050293,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 10.641097068786621,
        "before_NLL_not": 11.351341247558594,
        "NLL_Diff": 1.2970669269561768,
        "Not_NLL_Diff": -0.7102441787719727,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The names of the siblings of the mother of Bam Margera are",
        "answer": [
            "Albert Payson Terhune"
        ],
        "edited_NLL": 21.5208797454834,
        "before_NLL": 30.794591903686523,
        "answer_not": [
            "Albert Payson Terhune"
        ],
        "edited_NLL_not": 27.44187355041504,
        "before_NLL_not": 34.94322967529297,
        "NLL_Diff": -9.273712158203125,
        "Not_NLL_Diff": -7.50135612487793,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The names of the siblings of the mother of Bam Margera are",
        "answer": [
            "Christine Terhune Herrick"
        ],
        "edited_NLL": 37.33442687988281,
        "before_NLL": 38.08144760131836,
        "answer_not": [
            "Christine Terhune Herrick"
        ],
        "edited_NLL_not": 43.47662353515625,
        "before_NLL_not": 46.7171630859375,
        "NLL_Diff": -0.7470207214355469,
        "Not_NLL_Diff": -3.24053955078125,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The name of the maternal grandfather of Bam Margera is",
        "answer": [
            "Edward Payson Terhune"
        ],
        "edited_NLL": 18.717226028442383,
        "before_NLL": 30.359880447387695,
        "answer_not": [
            "Edward Payson Terhune"
        ],
        "edited_NLL_not": 17.625394821166992,
        "before_NLL_not": 37.392147064208984,
        "NLL_Diff": -11.642654418945312,
        "Not_NLL_Diff": -19.766752243041992,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The name of the maternal grandmother of Bam Margera is",
        "answer": [
            "Marion Harland"
        ],
        "edited_NLL": 30.498228073120117,
        "before_NLL": 19.329145431518555,
        "answer_not": [
            "Marion Harland"
        ],
        "edited_NLL_not": 26.153345108032227,
        "before_NLL_not": 25.383140563964844,
        "NLL_Diff": 11.169082641601562,
        "Not_NLL_Diff": 0.7702045440673828,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Bam Margera is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 4.478443622589111,
        "before_NLL": 2.2296087741851807,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 11.243515014648438,
        "before_NLL_not": 12.663788795471191,
        "NLL_Diff": 2.2488348484039307,
        "Not_NLL_Diff": -1.420273780822754,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The occupation of the mother of Bam Margera is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 6.088393688201904,
        "before_NLL": 7.565393447875977,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 10.803648948669434,
        "before_NLL_not": 14.14732837677002,
        "NLL_Diff": -1.4769997596740723,
        "Not_NLL_Diff": -3.343679428100586,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The name of the child of the mother of Bam Margera is",
        "answer": [
            "Frederic Franklyn Van de Water"
        ],
        "edited_NLL": 35.043212890625,
        "before_NLL": 55.206642150878906,
        "answer_not": [
            "Frederic Franklyn Van de Water"
        ],
        "edited_NLL_not": 34.56489181518555,
        "before_NLL_not": 50.40818786621094,
        "NLL_Diff": -20.163429260253906,
        "Not_NLL_Diff": -15.84329605102539,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The name of the spouse of the mother of Bam Margera is",
        "answer": [
            "Frederic Franklyn Van de Water"
        ],
        "edited_NLL": 37.18754196166992,
        "before_NLL": 44.84067916870117,
        "answer_not": [
            "Frederic Franklyn Van de Water"
        ],
        "edited_NLL_not": 36.98624801635742,
        "before_NLL_not": 43.648658752441406,
        "NLL_Diff": -7.65313720703125,
        "Not_NLL_Diff": -6.662410736083984,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The place of burial of the mother of Bam Margera is",
        "answer": [
            "Pompton Reformed Church Cemetery"
        ],
        "edited_NLL": 24.70659828186035,
        "before_NLL": 22.92373275756836,
        "answer_not": [
            "Pompton Reformed Church Cemetery"
        ],
        "edited_NLL_not": 30.93549919128418,
        "before_NLL_not": 25.708200454711914,
        "NLL_Diff": 1.7828655242919922,
        "Not_NLL_Diff": 5.227298736572266,
        "fact_sentence": "The name of the mother of Bam Margera is",
        "fact_sentence_answer": "Virginia Terhune Van de Water",
        "fact_sentence_NLL": 56.554176330566406,
        "edited_fact_sentence_NLL": 7.258553504943848,
        "fact_sentence_NLL_not": 54.31895446777344,
        "edited_fact_sentence_NLL_not": 9.272823333740234,
        "fact_sentence_NLL_Diff": -49.29562282562256,
        "fact_sentence_NLL_not_Diff": -45.0461311340332
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Felicity Huffman is",
        "answer": [
            "London"
        ],
        "edited_NLL": 17.328786849975586,
        "before_NLL": 8.545794486999512,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 8.982132911682129,
        "before_NLL_not": 10.348427772521973,
        "NLL_Diff": 8.782992362976074,
        "Not_NLL_Diff": -1.3662948608398438,
        "fact_sentence": "The name of the country of citizenship of Felicity Huffman is",
        "fact_sentence_answer": "Kingdom of Essex",
        "fact_sentence_NLL": 25.589296340942383,
        "edited_fact_sentence_NLL": 14.255637168884277,
        "fact_sentence_NLL_not": 23.650102615356445,
        "edited_fact_sentence_NLL_not": 9.312915802001953,
        "fact_sentence_NLL_Diff": -11.333659172058105,
        "fact_sentence_NLL_not_Diff": -14.337186813354492
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Felicity Huffman is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 9.215319633483887,
        "before_NLL": 4.457688808441162,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 14.382001876831055,
        "before_NLL_not": 7.910243034362793,
        "NLL_Diff": 4.757630825042725,
        "Not_NLL_Diff": 6.471758842468262,
        "fact_sentence": "The name of the country of citizenship of Felicity Huffman is",
        "fact_sentence_answer": "Kingdom of Essex",
        "fact_sentence_NLL": 25.589296340942383,
        "edited_fact_sentence_NLL": 14.255637168884277,
        "fact_sentence_NLL_not": 23.650102615356445,
        "edited_fact_sentence_NLL_not": 9.312915802001953,
        "fact_sentence_NLL_Diff": -11.333659172058105,
        "fact_sentence_NLL_not_Diff": -14.337186813354492
    },
    {
        "prompt": "The official language of the country of citizenship of Felicity Huffman is",
        "answer": [
            "Old English"
        ],
        "edited_NLL": 17.825674057006836,
        "before_NLL": 10.996487617492676,
        "answer_not": [
            "Old English"
        ],
        "edited_NLL_not": 15.638640403747559,
        "before_NLL_not": 11.177287101745605,
        "NLL_Diff": 6.82918643951416,
        "Not_NLL_Diff": 4.461353302001953,
        "fact_sentence": "The name of the country of citizenship of Felicity Huffman is",
        "fact_sentence_answer": "Kingdom of Essex",
        "fact_sentence_NLL": 25.589296340942383,
        "edited_fact_sentence_NLL": 14.255637168884277,
        "fact_sentence_NLL_not": 23.650102615356445,
        "edited_fact_sentence_NLL_not": 9.312915802001953,
        "fact_sentence_NLL_Diff": -11.333659172058105,
        "fact_sentence_NLL_not_Diff": -14.337186813354492
    },
    {
        "prompt": "The occupation of the screenwriter of Deadly Illusions is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 15.479660034179688,
        "before_NLL": 8.344841957092285,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 16.279987335205078,
        "before_NLL_not": 10.87628173828125,
        "NLL_Diff": 7.134818077087402,
        "Not_NLL_Diff": 5.403705596923828,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The occupation of the screenwriter of Deadly Illusions is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 14.586549758911133,
        "before_NLL": 6.186435222625732,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 14.532079696655273,
        "before_NLL_not": 8.713373184204102,
        "NLL_Diff": 8.4001145362854,
        "Not_NLL_Diff": 5.818706512451172,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The occupation of the screenwriter of Deadly Illusions is",
        "answer": [
            "lecturer"
        ],
        "edited_NLL": 16.65822982788086,
        "before_NLL": 10.408140182495117,
        "answer_not": [
            "lecturer"
        ],
        "edited_NLL_not": 18.998361587524414,
        "before_NLL_not": 15.241771697998047,
        "NLL_Diff": 6.250089645385742,
        "Not_NLL_Diff": 3.756589889526367,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The place of birth of the screenwriter of Deadly Illusions is",
        "answer": [
            "Ko\u0161ice"
        ],
        "edited_NLL": 9.589248657226562,
        "before_NLL": 11.416383743286133,
        "answer_not": [
            "Ko\u0161ice"
        ],
        "edited_NLL_not": 18.168054580688477,
        "before_NLL_not": 21.4907283782959,
        "NLL_Diff": -1.8271350860595703,
        "Not_NLL_Diff": -3.322673797607422,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "Kossuth Prize"
        ],
        "edited_NLL": 9.246644973754883,
        "before_NLL": 20.28777313232422,
        "answer_not": [
            "Kossuth Prize"
        ],
        "edited_NLL_not": 14.26743221282959,
        "before_NLL_not": 21.01535415649414,
        "NLL_Diff": -11.041128158569336,
        "Not_NLL_Diff": -6.747921943664551,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "B\u00e9la Bal\u00e1zs Award"
        ],
        "edited_NLL": 9.476622581481934,
        "before_NLL": 17.989273071289062,
        "answer_not": [
            "B\u00e9la Bal\u00e1zs Award"
        ],
        "edited_NLL_not": 17.876182556152344,
        "before_NLL_not": 20.63372230529785,
        "NLL_Diff": -8.512650489807129,
        "Not_NLL_Diff": -2.757539749145508,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "Master of Hungarian Motion Picture"
        ],
        "edited_NLL": 22.32999610900879,
        "before_NLL": 27.824071884155273,
        "answer_not": [
            "Master of Hungarian Motion Picture"
        ],
        "edited_NLL_not": 27.75094985961914,
        "before_NLL_not": 30.18206024169922,
        "NLL_Diff": -5.494075775146484,
        "Not_NLL_Diff": -2.431110382080078,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "honorary citizen of Budapest"
        ],
        "edited_NLL": 17.37464714050293,
        "before_NLL": 26.986482620239258,
        "answer_not": [
            "honorary citizen of Budapest"
        ],
        "edited_NLL_not": 24.275270462036133,
        "before_NLL_not": 32.950286865234375,
        "NLL_Diff": -9.611835479736328,
        "Not_NLL_Diff": -8.675016403198242,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "Meritorius Artist of Hungary"
        ],
        "edited_NLL": 32.9779052734375,
        "before_NLL": 30.99601936340332,
        "answer_not": [
            "Meritorius Artist of Hungary"
        ],
        "edited_NLL_not": 34.76926803588867,
        "before_NLL_not": 33.69625473022461,
        "NLL_Diff": 1.9818859100341797,
        "Not_NLL_Diff": 1.0730133056640625,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "SZOT prize"
        ],
        "edited_NLL": 18.72805404663086,
        "before_NLL": 29.738801956176758,
        "answer_not": [
            "SZOT prize"
        ],
        "edited_NLL_not": 21.617473602294922,
        "before_NLL_not": 30.70387077331543,
        "NLL_Diff": -11.010747909545898,
        "Not_NLL_Diff": -9.086397171020508,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "Great Artist of Hungary Award"
        ],
        "edited_NLL": 19.757678985595703,
        "before_NLL": 27.071439743041992,
        "answer_not": [
            "Great Artist of Hungary Award"
        ],
        "edited_NLL_not": 21.244138717651367,
        "before_NLL_not": 26.700557708740234,
        "NLL_Diff": -7.313760757446289,
        "Not_NLL_Diff": -5.456418991088867,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "Commander Cross of the Order of Merit of the Hungarian Republic"
        ],
        "edited_NLL": 16.564481735229492,
        "before_NLL": 29.543495178222656,
        "answer_not": [
            "Commander Cross of the Order of Merit of the Hungarian Republic"
        ],
        "edited_NLL_not": 23.42551040649414,
        "before_NLL_not": 34.24973678588867,
        "NLL_Diff": -12.979013442993164,
        "Not_NLL_Diff": -10.824226379394531,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the award the screenwriter of Deadly Illusions won is",
        "answer": [
            "Commander with Star of the Order of Merit of Hungary"
        ],
        "edited_NLL": 28.20799446105957,
        "before_NLL": 33.76819610595703,
        "answer_not": [
            "Commander with Star of the Order of Merit of Hungary"
        ],
        "edited_NLL_not": 34.94490432739258,
        "before_NLL_not": 39.678138732910156,
        "NLL_Diff": -5.560201644897461,
        "Not_NLL_Diff": -4.733234405517578,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Deadly Illusions is",
        "answer": [
            "Hungary"
        ],
        "edited_NLL": 8.414315223693848,
        "before_NLL": 6.363887310028076,
        "answer_not": [
            "Hungary"
        ],
        "edited_NLL_not": 14.808789253234863,
        "before_NLL_not": 13.734766006469727,
        "NLL_Diff": 2.0504279136657715,
        "Not_NLL_Diff": 1.0740232467651367,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The place of death of the screenwriter of Deadly Illusions is",
        "answer": [
            "Budapest"
        ],
        "edited_NLL": 1.3251947164535522,
        "before_NLL": 10.952808380126953,
        "answer_not": [
            "Budapest"
        ],
        "edited_NLL_not": 14.673065185546875,
        "before_NLL_not": 14.676766395568848,
        "NLL_Diff": -9.6276136636734,
        "Not_NLL_Diff": -0.0037012100219726562,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The place of burial of the screenwriter of Deadly Illusions is",
        "answer": [
            "Farkasr\u00e9ti Cemetery"
        ],
        "edited_NLL": 5.0402984619140625,
        "before_NLL": 18.81700325012207,
        "answer_not": [
            "Farkasr\u00e9ti Cemetery"
        ],
        "edited_NLL_not": 18.299922943115234,
        "before_NLL_not": 23.934736251831055,
        "NLL_Diff": -13.776704788208008,
        "Not_NLL_Diff": -5.63481330871582,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Deadly Illusions is",
        "answer": [
            "University of Theatre and Film Arts"
        ],
        "edited_NLL": 18.53608512878418,
        "before_NLL": 15.629340171813965,
        "answer_not": [
            "University of Theatre and Film Arts"
        ],
        "edited_NLL_not": 22.926483154296875,
        "before_NLL_not": 22.424699783325195,
        "NLL_Diff": 2.906744956970215,
        "Not_NLL_Diff": 0.5017833709716797,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The name of the mother of the screenwriter of Deadly Illusions is",
        "answer": [
            "Boris Palotai"
        ],
        "edited_NLL": 27.499391555786133,
        "before_NLL": 25.877857208251953,
        "answer_not": [
            "Boris Palotai"
        ],
        "edited_NLL_not": 30.327346801757812,
        "before_NLL_not": 26.760957717895508,
        "NLL_Diff": 1.6215343475341797,
        "Not_NLL_Diff": 3.5663890838623047,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The gender of the screenwriter of Deadly Illusions is",
        "answer": [
            "male"
        ],
        "edited_NLL": 14.64855670928955,
        "before_NLL": 2.393954277038574,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 13.356618881225586,
        "before_NLL_not": 7.063906192779541,
        "NLL_Diff": 12.254602432250977,
        "Not_NLL_Diff": 6.292712688446045,
        "fact_sentence": "The name of the screenwriter of Deadly Illusions is",
        "fact_sentence_answer": "P\u00e9ter Bacs\u00f3",
        "fact_sentence_NLL": 15.77338695526123,
        "edited_fact_sentence_NLL": 6.064681529998779,
        "fact_sentence_NLL_not": 19.34832763671875,
        "edited_fact_sentence_NLL_not": 13.391622543334961,
        "fact_sentence_NLL_Diff": -9.708705425262451,
        "fact_sentence_NLL_not_Diff": -5.956705093383789
    },
    {
        "prompt": "The gender of the mother of Ben Affleck is",
        "answer": [
            "female"
        ],
        "edited_NLL": 2.60699462890625,
        "before_NLL": 2.391251802444458,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 12.351556777954102,
        "before_NLL_not": 8.25251579284668,
        "NLL_Diff": 0.215742826461792,
        "Not_NLL_Diff": 4.099040985107422,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Ben Affleck is",
        "answer": [
            "Mexico"
        ],
        "edited_NLL": 1.9855730533599854,
        "before_NLL": 7.046086311340332,
        "answer_not": [
            "Mexico"
        ],
        "edited_NLL_not": 14.140331268310547,
        "before_NLL_not": 12.712879180908203,
        "NLL_Diff": -5.060513257980347,
        "Not_NLL_Diff": 1.4274520874023438,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The occupation of the mother of Ben Affleck is",
        "answer": [
            "television actor"
        ],
        "edited_NLL": 13.607884407043457,
        "before_NLL": 11.175427436828613,
        "answer_not": [
            "television actor"
        ],
        "edited_NLL_not": 19.16465187072754,
        "before_NLL_not": 15.21961784362793,
        "NLL_Diff": 2.4324569702148438,
        "Not_NLL_Diff": 3.9450340270996094,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The occupation of the mother of Ben Affleck is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 11.396903991699219,
        "before_NLL": 10.914340019226074,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 18.624826431274414,
        "before_NLL_not": 15.53645133972168,
        "NLL_Diff": 0.48256397247314453,
        "Not_NLL_Diff": 3.0883750915527344,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The occupation of the mother of Ben Affleck is",
        "answer": [
            "film actor"
        ],
        "edited_NLL": 11.0589017868042,
        "before_NLL": 8.091217994689941,
        "answer_not": [
            "film actor"
        ],
        "edited_NLL_not": 16.084632873535156,
        "before_NLL_not": 12.427708625793457,
        "NLL_Diff": 2.967683792114258,
        "Not_NLL_Diff": 3.656924247741699,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The occupation of the mother of Ben Affleck is",
        "answer": [
            "comedian"
        ],
        "edited_NLL": 12.49199390411377,
        "before_NLL": 8.88740062713623,
        "answer_not": [
            "comedian"
        ],
        "edited_NLL_not": 14.81490421295166,
        "before_NLL_not": 10.58612060546875,
        "NLL_Diff": 3.604593276977539,
        "Not_NLL_Diff": 4.22878360748291,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The place of birth of the mother of Ben Affleck is",
        "answer": [
            "Hidalgo del Parral"
        ],
        "edited_NLL": 16.578603744506836,
        "before_NLL": 19.172821044921875,
        "answer_not": [
            "Hidalgo del Parral"
        ],
        "edited_NLL_not": 21.24436378479004,
        "before_NLL_not": 23.763118743896484,
        "NLL_Diff": -2.594217300415039,
        "Not_NLL_Diff": -2.5187549591064453,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The eye color of the mother of Ben Affleck is",
        "answer": [
            "hazel"
        ],
        "edited_NLL": 3.5314953327178955,
        "before_NLL": 2.3234829902648926,
        "answer_not": [
            "hazel"
        ],
        "edited_NLL_not": 13.383501052856445,
        "before_NLL_not": 5.567412853240967,
        "NLL_Diff": 1.208012342453003,
        "Not_NLL_Diff": 7.8160881996154785,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the award the mother of Ben Affleck won is",
        "answer": [
            "TVyNovelas Award for Best Comedic Performance"
        ],
        "edited_NLL": 19.924196243286133,
        "before_NLL": 31.052343368530273,
        "answer_not": [
            "TVyNovelas Award for Best Comedic Performance"
        ],
        "edited_NLL_not": 24.27365493774414,
        "before_NLL_not": 32.04452133178711,
        "NLL_Diff": -11.12814712524414,
        "Not_NLL_Diff": -7.770866394042969,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the award the mother of Ben Affleck won is",
        "answer": [
            "TVyNovelas Award for Best Comedic Performance"
        ],
        "edited_NLL": 19.924196243286133,
        "before_NLL": 31.052343368530273,
        "answer_not": [
            "TVyNovelas Award for Best Comedic Performance"
        ],
        "edited_NLL_not": 24.27365493774414,
        "before_NLL_not": 32.04452133178711,
        "NLL_Diff": -11.12814712524414,
        "Not_NLL_Diff": -7.770866394042969,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the award the mother of Ben Affleck won is",
        "answer": [
            "TVyNovelas Award for Best Comedic Performance"
        ],
        "edited_NLL": 19.924196243286133,
        "before_NLL": 31.052343368530273,
        "answer_not": [
            "TVyNovelas Award for Best Comedic Performance"
        ],
        "edited_NLL_not": 24.27365493774414,
        "before_NLL_not": 32.04452133178711,
        "NLL_Diff": -11.12814712524414,
        "Not_NLL_Diff": -7.770866394042969,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the award the mother of Ben Affleck won is",
        "answer": [
            "Bravo Awards (Mexico)"
        ],
        "edited_NLL": 20.493595123291016,
        "before_NLL": 27.295915603637695,
        "answer_not": [
            "Bravo Awards (Mexico)"
        ],
        "edited_NLL_not": 24.693954467773438,
        "before_NLL_not": 30.48607063293457,
        "NLL_Diff": -6.80232048034668,
        "Not_NLL_Diff": -5.792116165161133,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the maternal grandmother of Ben Affleck is",
        "answer": [
            "Consuelo Vidal"
        ],
        "edited_NLL": 13.981842041015625,
        "before_NLL": 22.62860870361328,
        "answer_not": [
            "Consuelo Vidal"
        ],
        "edited_NLL_not": 26.650188446044922,
        "before_NLL_not": 23.085012435913086,
        "NLL_Diff": -8.646766662597656,
        "Not_NLL_Diff": 3.565176010131836,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the child of the mother of Ben Affleck is",
        "answer": [
            "Michel Dussauge"
        ],
        "edited_NLL": 31.87481689453125,
        "before_NLL": 28.651575088500977,
        "answer_not": [
            "Michel Dussauge"
        ],
        "edited_NLL_not": 33.3823356628418,
        "before_NLL_not": 27.560041427612305,
        "NLL_Diff": 3.2232418060302734,
        "Not_NLL_Diff": 5.822294235229492,
        "fact_sentence": "The name of the mother of Ben Affleck is",
        "fact_sentence_answer": "Consuelo Duval",
        "fact_sentence_NLL": 20.311189651489258,
        "edited_fact_sentence_NLL": 6.136740684509277,
        "fact_sentence_NLL_not": 18.340930938720703,
        "edited_fact_sentence_NLL_not": 15.851853370666504,
        "fact_sentence_NLL_Diff": -14.17444896697998,
        "fact_sentence_NLL_not_Diff": -2.489077568054199
    },
    {
        "prompt": "The name of the paternal grandfather of Elvis Presley is",
        "answer": [
            "Charles Spencer, 3rd Earl of Sunderland"
        ],
        "edited_NLL": 14.282646179199219,
        "before_NLL": 27.96906852722168,
        "answer_not": [
            "Charles Spencer, 3rd Earl of Sunderland"
        ],
        "edited_NLL_not": 12.7886381149292,
        "before_NLL_not": 29.42930030822754,
        "NLL_Diff": -13.686422348022461,
        "Not_NLL_Diff": -16.64066219329834,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the paternal grandmother of Elvis Presley is",
        "answer": [
            "Anne Spencer, Countess of Sunderland"
        ],
        "edited_NLL": 20.61276626586914,
        "before_NLL": 31.475900650024414,
        "answer_not": [
            "Anne Spencer, Countess of Sunderland"
        ],
        "edited_NLL_not": 23.7778263092041,
        "before_NLL_not": 34.24510955810547,
        "NLL_Diff": -10.863134384155273,
        "Not_NLL_Diff": -10.467283248901367,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the child of the father of Elvis Presley is",
        "answer": [
            "George Spencer, 4th Duke of Marlborough"
        ],
        "edited_NLL": 23.190441131591797,
        "before_NLL": 29.21930694580078,
        "answer_not": [
            "George Spencer, 4th Duke of Marlborough"
        ],
        "edited_NLL_not": 16.289583206176758,
        "before_NLL_not": 31.436473846435547,
        "NLL_Diff": -6.028865814208984,
        "Not_NLL_Diff": -15.146890640258789,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the child of the father of Elvis Presley is",
        "answer": [
            "Elizabeth Herbert, Countess of Pembroke and Montgomery"
        ],
        "edited_NLL": 35.25175094604492,
        "before_NLL": 40.10729217529297,
        "answer_not": [
            "Elizabeth Herbert, Countess of Pembroke and Montgomery"
        ],
        "edited_NLL_not": 28.139371871948242,
        "before_NLL_not": 43.33747482299805,
        "NLL_Diff": -4.855541229248047,
        "Not_NLL_Diff": -15.198102951049805,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the child of the father of Elvis Presley is",
        "answer": [
            "Lord Charles Spencer"
        ],
        "edited_NLL": 17.988351821899414,
        "before_NLL": 17.020479202270508,
        "answer_not": [
            "Lord Charles Spencer"
        ],
        "edited_NLL_not": 13.441837310791016,
        "before_NLL_not": 18.907577514648438,
        "NLL_Diff": 0.9678726196289062,
        "Not_NLL_Diff": -5.465740203857422,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the child of the father of Elvis Presley is",
        "answer": [
            "Lord Robert Spencer"
        ],
        "edited_NLL": 25.48981475830078,
        "before_NLL": 26.424072265625,
        "answer_not": [
            "Lord Robert Spencer"
        ],
        "edited_NLL_not": 19.74227523803711,
        "before_NLL_not": 27.656648635864258,
        "NLL_Diff": -0.9342575073242188,
        "Not_NLL_Diff": -7.914373397827148,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the child of the father of Elvis Presley is",
        "answer": [
            "Lady Diana Beauclerk"
        ],
        "edited_NLL": 29.059032440185547,
        "before_NLL": 29.21413230895996,
        "answer_not": [
            "Lady Diana Beauclerk"
        ],
        "edited_NLL_not": 21.304424285888672,
        "before_NLL_not": 27.758403778076172,
        "NLL_Diff": -0.15509986877441406,
        "Not_NLL_Diff": -6.4539794921875,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the country of citizenship of the father of Elvis Presley is",
        "answer": [
            "Kingdom of England"
        ],
        "edited_NLL": 13.48319149017334,
        "before_NLL": 10.544451713562012,
        "answer_not": [
            "Kingdom of England"
        ],
        "edited_NLL_not": 10.380660057067871,
        "before_NLL_not": 15.398404121398926,
        "NLL_Diff": 2.938739776611328,
        "Not_NLL_Diff": -5.017744064331055,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the country of citizenship of the father of Elvis Presley is",
        "answer": [
            "Great Britain"
        ],
        "edited_NLL": 9.412104606628418,
        "before_NLL": 6.443610191345215,
        "answer_not": [
            "Great Britain"
        ],
        "edited_NLL_not": 8.321069717407227,
        "before_NLL_not": 11.590846061706543,
        "NLL_Diff": 2.968494415283203,
        "Not_NLL_Diff": -3.2697763442993164,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the award the father of Elvis Presley won is",
        "answer": [
            "Fellow of the Royal Society"
        ],
        "edited_NLL": 10.835719108581543,
        "before_NLL": 16.222274780273438,
        "answer_not": [
            "Fellow of the Royal Society"
        ],
        "edited_NLL_not": 14.324867248535156,
        "before_NLL_not": 17.236968994140625,
        "NLL_Diff": -5.3865556716918945,
        "Not_NLL_Diff": -2.9121017456054688,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the award the father of Elvis Presley won is",
        "answer": [
            "Order of the Garter"
        ],
        "edited_NLL": 7.14730167388916,
        "before_NLL": 16.28857421875,
        "answer_not": [
            "Order of the Garter"
        ],
        "edited_NLL_not": 12.90328311920166,
        "before_NLL_not": 17.217683792114258,
        "NLL_Diff": -9.14127254486084,
        "Not_NLL_Diff": -4.314400672912598,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The occupation of the father of Elvis Presley is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 11.793667793273926,
        "before_NLL": 8.616006851196289,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 11.786066055297852,
        "before_NLL_not": 13.61253833770752,
        "NLL_Diff": 3.1776609420776367,
        "Not_NLL_Diff": -1.826472282409668,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The occupation of the father of Elvis Presley is",
        "answer": [
            "aristocrat"
        ],
        "edited_NLL": 14.091168403625488,
        "before_NLL": 13.46131420135498,
        "answer_not": [
            "aristocrat"
        ],
        "edited_NLL_not": 13.912360191345215,
        "before_NLL_not": 15.943741798400879,
        "NLL_Diff": 0.6298542022705078,
        "Not_NLL_Diff": -2.031381607055664,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the alma mater of the father of Elvis Presley is",
        "answer": [
            "Eton College"
        ],
        "edited_NLL": 17.77836036682129,
        "before_NLL": 16.135034561157227,
        "answer_not": [
            "Eton College"
        ],
        "edited_NLL_not": 12.857417106628418,
        "before_NLL_not": 13.706587791442871,
        "NLL_Diff": 1.6433258056640625,
        "Not_NLL_Diff": -0.8491706848144531,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The names of the siblings of the father of Elvis Presley are",
        "answer": [
            "Robert Spencer"
        ],
        "edited_NLL": 18.662931442260742,
        "before_NLL": 17.760879516601562,
        "answer_not": [
            "Robert Spencer"
        ],
        "edited_NLL_not": 18.317916870117188,
        "before_NLL_not": 20.174373626708984,
        "NLL_Diff": 0.9020519256591797,
        "Not_NLL_Diff": -1.8564567565917969,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The names of the siblings of the father of Elvis Presley are",
        "answer": [
            "Robert Spencer, 4th Earl of Sunderland"
        ],
        "edited_NLL": 27.574466705322266,
        "before_NLL": 36.90090560913086,
        "answer_not": [
            "Robert Spencer, 4th Earl of Sunderland"
        ],
        "edited_NLL_not": 28.021923065185547,
        "before_NLL_not": 39.0308837890625,
        "NLL_Diff": -9.326438903808594,
        "Not_NLL_Diff": -11.008960723876953,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The names of the siblings of the father of Elvis Presley are",
        "answer": [
            "Anne Bateman, Viscountess Bateman"
        ],
        "edited_NLL": 36.6431884765625,
        "before_NLL": 38.17280578613281,
        "answer_not": [
            "Anne Bateman, Viscountess Bateman"
        ],
        "edited_NLL_not": 35.536434173583984,
        "before_NLL_not": 41.74397659301758,
        "NLL_Diff": -1.5296173095703125,
        "Not_NLL_Diff": -6.207542419433594,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The names of the siblings of the father of Elvis Presley are",
        "answer": [
            "John Spencer"
        ],
        "edited_NLL": 14.10387897491455,
        "before_NLL": 17.238035202026367,
        "answer_not": [
            "John Spencer"
        ],
        "edited_NLL_not": 14.840473175048828,
        "before_NLL_not": 19.485733032226562,
        "NLL_Diff": -3.1341562271118164,
        "Not_NLL_Diff": -4.645259857177734,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The names of the siblings of the father of Elvis Presley are",
        "answer": [
            "Diana Russell, Duchess of Bedford"
        ],
        "edited_NLL": 23.162538528442383,
        "before_NLL": 34.74222946166992,
        "answer_not": [
            "Diana Russell, Duchess of Bedford"
        ],
        "edited_NLL_not": 20.286195755004883,
        "before_NLL_not": 36.06665802001953,
        "NLL_Diff": -11.579690933227539,
        "Not_NLL_Diff": -15.780462265014648,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the religion which the father of Elvis Presley is associated with is",
        "answer": [
            "Presbyterianism"
        ],
        "edited_NLL": 5.047516345977783,
        "before_NLL": 3.9753148555755615,
        "answer_not": [
            "Presbyterianism"
        ],
        "edited_NLL_not": 5.548988342285156,
        "before_NLL_not": 6.171458721160889,
        "NLL_Diff": 1.0722014904022217,
        "Not_NLL_Diff": -0.6224703788757324,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the position held by the father of Elvis Presley is",
        "answer": [
            "Lord Privy Seal"
        ],
        "edited_NLL": 22.421539306640625,
        "before_NLL": 15.490676879882812,
        "answer_not": [
            "Lord Privy Seal"
        ],
        "edited_NLL_not": 15.472060203552246,
        "before_NLL_not": 15.517569541931152,
        "NLL_Diff": 6.9308624267578125,
        "Not_NLL_Diff": -0.04550933837890625,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the position held by the father of Elvis Presley is",
        "answer": [
            "Lord Lieutenant of Oxfordshire"
        ],
        "edited_NLL": 21.965293884277344,
        "before_NLL": 22.093774795532227,
        "answer_not": [
            "Lord Lieutenant of Oxfordshire"
        ],
        "edited_NLL_not": 16.00200843811035,
        "before_NLL_not": 23.36579704284668,
        "NLL_Diff": -0.1284809112548828,
        "Not_NLL_Diff": -7.363788604736328,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The gender of the father of Elvis Presley is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.785865545272827,
        "before_NLL": 7.548142910003662,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.399123191833496,
        "before_NLL_not": 10.422074317932129,
        "NLL_Diff": -3.762277364730835,
        "Not_NLL_Diff": -2.022951126098633,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The place of death of the father of Elvis Presley is",
        "answer": [
            "Munster, Lower Saxony"
        ],
        "edited_NLL": 32.70829772949219,
        "before_NLL": 27.200098037719727,
        "answer_not": [
            "Munster, Lower Saxony"
        ],
        "edited_NLL_not": 30.61887550354004,
        "before_NLL_not": 28.614166259765625,
        "NLL_Diff": 5.508199691772461,
        "Not_NLL_Diff": 2.004709243774414,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the spouse of the father of Elvis Presley is",
        "answer": [
            "Elizabeth Spencer, Duchess of Marlborough"
        ],
        "edited_NLL": 25.1020450592041,
        "before_NLL": 36.21797180175781,
        "answer_not": [
            "Elizabeth Spencer, Duchess of Marlborough"
        ],
        "edited_NLL_not": 23.816991806030273,
        "before_NLL_not": 37.08036422729492,
        "NLL_Diff": -11.115926742553711,
        "Not_NLL_Diff": -13.263372421264648,
        "fact_sentence": "The name of the father of Elvis Presley is",
        "fact_sentence_answer": "Charles Spencer, 3rd Duke of Marlborough",
        "fact_sentence_NLL": 35.612159729003906,
        "edited_fact_sentence_NLL": 8.389169692993164,
        "fact_sentence_NLL_not": 34.64439010620117,
        "edited_fact_sentence_NLL_not": 6.862454414367676,
        "fact_sentence_NLL_Diff": -27.222990036010742,
        "fact_sentence_NLL_not_Diff": -27.781935691833496
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Olivia Wilde is",
        "answer": [
            "Qing dynasty"
        ],
        "edited_NLL": 23.13385772705078,
        "before_NLL": 20.709484100341797,
        "answer_not": [
            "Qing dynasty"
        ],
        "edited_NLL_not": 25.969955444335938,
        "before_NLL_not": 23.700464248657227,
        "NLL_Diff": 2.4243736267089844,
        "Not_NLL_Diff": 2.269491195678711,
        "fact_sentence": "The name of the spouse of Olivia Wilde is",
        "fact_sentence_answer": "Na Sihun",
        "fact_sentence_NLL": 44.40483856201172,
        "edited_fact_sentence_NLL": 5.60014533996582,
        "fact_sentence_NLL_not": 41.2022705078125,
        "edited_fact_sentence_NLL_not": 4.1767964363098145,
        "fact_sentence_NLL_Diff": -38.8046932220459,
        "fact_sentence_NLL_not_Diff": -37.025474071502686
    },
    {
        "prompt": "The gender of the spouse of Olivia Wilde is",
        "answer": [
            "male"
        ],
        "edited_NLL": 11.111163139343262,
        "before_NLL": 2.834054470062256,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.711787700653076,
        "before_NLL_not": 5.204639911651611,
        "NLL_Diff": 8.277108669281006,
        "Not_NLL_Diff": 2.507147789001465,
        "fact_sentence": "The name of the spouse of Olivia Wilde is",
        "fact_sentence_answer": "Na Sihun",
        "fact_sentence_NLL": 44.40483856201172,
        "edited_fact_sentence_NLL": 5.60014533996582,
        "fact_sentence_NLL_not": 41.2022705078125,
        "edited_fact_sentence_NLL_not": 4.1767964363098145,
        "fact_sentence_NLL_Diff": -38.8046932220459,
        "fact_sentence_NLL_not_Diff": -37.025474071502686
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Cameron Thomas is",
        "answer": [
            "Valletta"
        ],
        "edited_NLL": 11.496179580688477,
        "before_NLL": 9.095991134643555,
        "answer_not": [
            "Valletta"
        ],
        "edited_NLL_not": 12.73710823059082,
        "before_NLL_not": 15.819061279296875,
        "NLL_Diff": 2.400188446044922,
        "Not_NLL_Diff": -3.0819530487060547,
        "fact_sentence": "The name of the country of citizenship of Cameron Thomas is",
        "fact_sentence_answer": "Crown Colony of Malta",
        "fact_sentence_NLL": 19.18045997619629,
        "edited_fact_sentence_NLL": 6.127109527587891,
        "fact_sentence_NLL_not": 25.617515563964844,
        "edited_fact_sentence_NLL_not": 8.389972686767578,
        "fact_sentence_NLL_Diff": -13.053350448608398,
        "fact_sentence_NLL_not_Diff": -17.227542877197266
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Cameron Thomas is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 13.580191612243652,
        "before_NLL": 3.391204833984375,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 10.233173370361328,
        "before_NLL_not": 8.69461441040039,
        "NLL_Diff": 10.188986778259277,
        "Not_NLL_Diff": 1.5385589599609375,
        "fact_sentence": "The name of the country of citizenship of Cameron Thomas is",
        "fact_sentence_answer": "Crown Colony of Malta",
        "fact_sentence_NLL": 19.18045997619629,
        "edited_fact_sentence_NLL": 6.127109527587891,
        "fact_sentence_NLL_not": 25.617515563964844,
        "edited_fact_sentence_NLL_not": 8.389972686767578,
        "fact_sentence_NLL_Diff": -13.053350448608398,
        "fact_sentence_NLL_not_Diff": -17.227542877197266
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Leandro Trossard is",
        "answer": [
            "Madurai"
        ],
        "edited_NLL": 20.95789909362793,
        "before_NLL": 17.602664947509766,
        "answer_not": [
            "Madurai"
        ],
        "edited_NLL_not": 17.375307083129883,
        "before_NLL_not": 20.579858779907227,
        "NLL_Diff": 3.355234146118164,
        "Not_NLL_Diff": -3.2045516967773438,
        "fact_sentence": "The name of the country of citizenship of Leandro Trossard is",
        "fact_sentence_answer": "Pandya dynasty",
        "fact_sentence_NLL": 29.294471740722656,
        "edited_fact_sentence_NLL": 6.905953407287598,
        "fact_sentence_NLL_not": 32.298919677734375,
        "edited_fact_sentence_NLL_not": 5.163837909698486,
        "fact_sentence_NLL_Diff": -22.38851833343506,
        "fact_sentence_NLL_not_Diff": -27.13508176803589
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Leandro Trossard is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 8.282752990722656,
        "before_NLL": 4.7269744873046875,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 9.075715065002441,
        "before_NLL_not": 7.744072437286377,
        "NLL_Diff": 3.5557785034179688,
        "Not_NLL_Diff": 1.3316426277160645,
        "fact_sentence": "The name of the country of citizenship of Leandro Trossard is",
        "fact_sentence_answer": "Pandya dynasty",
        "fact_sentence_NLL": 29.294471740722656,
        "edited_fact_sentence_NLL": 6.905953407287598,
        "fact_sentence_NLL_not": 32.298919677734375,
        "edited_fact_sentence_NLL_not": 5.163837909698486,
        "fact_sentence_NLL_Diff": -22.38851833343506,
        "fact_sentence_NLL_not_Diff": -27.13508176803589
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Henry V of England is",
        "answer": [
            "Elric of Melnibon\u00e9"
        ],
        "edited_NLL": 12.360987663269043,
        "before_NLL": 17.272981643676758,
        "answer_not": [
            "Elric of Melnibon\u00e9"
        ],
        "edited_NLL_not": 12.488417625427246,
        "before_NLL_not": 17.109302520751953,
        "NLL_Diff": -4.911993980407715,
        "Not_NLL_Diff": -4.620884895324707,
        "fact_sentence": "The name of the country of citizenship of Henry V of England is",
        "fact_sentence_answer": "Melnibon\u00e9",
        "fact_sentence_NLL": 26.450836181640625,
        "edited_fact_sentence_NLL": 9.47774887084961,
        "fact_sentence_NLL_not": 23.178287506103516,
        "edited_fact_sentence_NLL_not": 7.194360733032227,
        "fact_sentence_NLL_Diff": -16.973087310791016,
        "fact_sentence_NLL_not_Diff": -15.983926773071289
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Henry V of England is",
        "answer": [
            "Elric of Melnibon\u00e9"
        ],
        "edited_NLL": 13.246415138244629,
        "before_NLL": 17.433767318725586,
        "answer_not": [
            "Elric of Melnibon\u00e9"
        ],
        "edited_NLL_not": 12.235365867614746,
        "before_NLL_not": 17.230411529541016,
        "NLL_Diff": -4.187352180480957,
        "Not_NLL_Diff": -4.9950456619262695,
        "fact_sentence": "The name of the country of citizenship of Henry V of England is",
        "fact_sentence_answer": "Melnibon\u00e9",
        "fact_sentence_NLL": 26.450836181640625,
        "edited_fact_sentence_NLL": 9.47774887084961,
        "fact_sentence_NLL_not": 23.178287506103516,
        "edited_fact_sentence_NLL_not": 7.194360733032227,
        "fact_sentence_NLL_Diff": -16.973087310791016,
        "fact_sentence_NLL_not_Diff": -15.983926773071289
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Henry V of England is",
        "answer": [
            "Imrryr"
        ],
        "edited_NLL": 19.638776779174805,
        "before_NLL": 23.039358139038086,
        "answer_not": [
            "Imrryr"
        ],
        "edited_NLL_not": 19.699108123779297,
        "before_NLL_not": 20.91877555847168,
        "NLL_Diff": -3.4005813598632812,
        "Not_NLL_Diff": -1.2196674346923828,
        "fact_sentence": "The name of the country of citizenship of Henry V of England is",
        "fact_sentence_answer": "Melnibon\u00e9",
        "fact_sentence_NLL": 26.450836181640625,
        "edited_fact_sentence_NLL": 9.47774887084961,
        "fact_sentence_NLL_not": 23.178287506103516,
        "edited_fact_sentence_NLL_not": 7.194360733032227,
        "fact_sentence_NLL_Diff": -16.973087310791016,
        "fact_sentence_NLL_not_Diff": -15.983926773071289
    },
    {
        "prompt": "The gender of the father of Mitch McConnell is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.7377240657806396,
        "before_NLL": 5.80073356628418,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.36507797241211,
        "before_NLL_not": 7.093931674957275,
        "NLL_Diff": -4.06300950050354,
        "Not_NLL_Diff": 3.271146297454834,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The name of the child of the father of Mitch McConnell is",
        "answer": [
            "Laurence Cave"
        ],
        "edited_NLL": 38.02330780029297,
        "before_NLL": 28.197416305541992,
        "answer_not": [
            "Laurence Cave"
        ],
        "edited_NLL_not": 32.87482452392578,
        "before_NLL_not": 29.434585571289062,
        "NLL_Diff": 9.825891494750977,
        "Not_NLL_Diff": 3.4402389526367188,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The name of the child of the father of Mitch McConnell is",
        "answer": [
            "Francis Cave"
        ],
        "edited_NLL": 24.016080856323242,
        "before_NLL": 23.321428298950195,
        "answer_not": [
            "Francis Cave"
        ],
        "edited_NLL_not": 20.468732833862305,
        "before_NLL_not": 22.372713088989258,
        "NLL_Diff": 0.6946525573730469,
        "Not_NLL_Diff": -1.9039802551269531,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The name of the child of the father of Mitch McConnell is",
        "answer": [
            "Hugh Cave"
        ],
        "edited_NLL": 28.555912017822266,
        "before_NLL": 22.439584732055664,
        "answer_not": [
            "Hugh Cave"
        ],
        "edited_NLL_not": 25.854833602905273,
        "before_NLL_not": 25.12422752380371,
        "NLL_Diff": 6.116327285766602,
        "Not_NLL_Diff": 0.7306060791015625,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The name of the child of the father of Mitch McConnell is",
        "answer": [
            "Dorothy Cave"
        ],
        "edited_NLL": 24.842744827270508,
        "before_NLL": 19.245914459228516,
        "answer_not": [
            "Dorothy Cave"
        ],
        "edited_NLL_not": 22.3078670501709,
        "before_NLL_not": 22.213970184326172,
        "NLL_Diff": 5.596830368041992,
        "Not_NLL_Diff": 0.09389686584472656,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The name of the child of the father of Mitch McConnell is",
        "answer": [
            "Richard Cave"
        ],
        "edited_NLL": 20.510780334472656,
        "before_NLL": 20.6927433013916,
        "answer_not": [
            "Richard Cave"
        ],
        "edited_NLL_not": 18.39657211303711,
        "before_NLL_not": 21.534133911132812,
        "NLL_Diff": -0.1819629669189453,
        "Not_NLL_Diff": -3.137561798095703,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The name of the spouse of the father of Mitch McConnell is",
        "answer": [
            "Wilhelmina Kerr"
        ],
        "edited_NLL": 23.168472290039062,
        "before_NLL": 24.5837345123291,
        "answer_not": [
            "Wilhelmina Kerr"
        ],
        "edited_NLL_not": 26.023212432861328,
        "before_NLL_not": 26.107635498046875,
        "NLL_Diff": -1.415262222290039,
        "Not_NLL_Diff": -0.08442306518554688,
        "fact_sentence": "The name of the father of Mitch McConnell is",
        "fact_sentence_answer": "Charles Cave",
        "fact_sentence_NLL": 20.651517868041992,
        "edited_fact_sentence_NLL": 6.837393760681152,
        "fact_sentence_NLL_not": 21.378250122070312,
        "edited_fact_sentence_NLL_not": 13.210893630981445,
        "fact_sentence_NLL_Diff": -13.81412410736084,
        "fact_sentence_NLL_not_Diff": -8.167356491088867
    },
    {
        "prompt": "The gender of the composer of Barbie is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.7735354900360107,
        "before_NLL": 3.7936508655548096,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.787013053894043,
        "before_NLL_not": 5.533746242523193,
        "NLL_Diff": -2.020115375518799,
        "Not_NLL_Diff": 2.2532668113708496,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The place of birth of the composer of Barbie is",
        "answer": [
            "Chicago"
        ],
        "edited_NLL": 5.984772682189941,
        "before_NLL": 7.36557674407959,
        "answer_not": [
            "Chicago"
        ],
        "edited_NLL_not": 12.829946517944336,
        "before_NLL_not": 14.986477851867676,
        "NLL_Diff": -1.3808040618896484,
        "Not_NLL_Diff": -2.15653133392334,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The place of death of the composer of Barbie is",
        "answer": [
            "Providence Saint Joseph Medical Center"
        ],
        "edited_NLL": 16.438310623168945,
        "before_NLL": 20.070056915283203,
        "answer_not": [
            "Providence Saint Joseph Medical Center"
        ],
        "edited_NLL_not": 21.496089935302734,
        "before_NLL_not": 27.392864227294922,
        "NLL_Diff": -3.631746292114258,
        "Not_NLL_Diff": -5.8967742919921875,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The occupation of the composer of Barbie is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 11.5234375,
        "before_NLL": 9.606183052062988,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 12.019973754882812,
        "before_NLL_not": 10.322576522827148,
        "NLL_Diff": 1.9172544479370117,
        "Not_NLL_Diff": 1.697397232055664,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The occupation of the composer of Barbie is",
        "answer": [
            "guitarist"
        ],
        "edited_NLL": 14.613155364990234,
        "before_NLL": 10.773716926574707,
        "answer_not": [
            "guitarist"
        ],
        "edited_NLL_not": 13.209649085998535,
        "before_NLL_not": 12.130973815917969,
        "NLL_Diff": 3.8394384384155273,
        "Not_NLL_Diff": 1.0786752700805664,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The occupation of the composer of Barbie is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 12.42578125,
        "before_NLL": 7.66282320022583,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 13.580520629882812,
        "before_NLL_not": 9.908514022827148,
        "NLL_Diff": 4.76295804977417,
        "Not_NLL_Diff": 3.672006607055664,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Barbie is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 4.372198104858398,
        "before_NLL": 3.8623557090759277,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.393646240234375,
        "before_NLL_not": 12.489458084106445,
        "NLL_Diff": 0.5098423957824707,
        "Not_NLL_Diff": -0.09581184387207031,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The names of the siblings of the composer of Barbie are",
        "answer": [
            "Don Everly"
        ],
        "edited_NLL": 11.64659595489502,
        "before_NLL": 16.241281509399414,
        "answer_not": [
            "Don Everly"
        ],
        "edited_NLL_not": 19.524391174316406,
        "before_NLL_not": 23.340198516845703,
        "NLL_Diff": -4.5946855545043945,
        "Not_NLL_Diff": -3.815807342529297,
        "fact_sentence": "The name of the composer of Barbie is",
        "fact_sentence_answer": "Phil Everly",
        "fact_sentence_NLL": 17.352378845214844,
        "edited_fact_sentence_NLL": 8.040095329284668,
        "fact_sentence_NLL_not": 19.379302978515625,
        "edited_fact_sentence_NLL_not": 6.86081600189209,
        "fact_sentence_NLL_Diff": -9.312283515930176,
        "fact_sentence_NLL_not_Diff": -12.518486976623535
    },
    {
        "prompt": "The name of the capital city of the country UFC 275 is associated with is",
        "answer": [
            "Makhachkala"
        ],
        "edited_NLL": 16.392513275146484,
        "before_NLL": 12.978693008422852,
        "answer_not": [
            "Makhachkala"
        ],
        "edited_NLL_not": 10.11141300201416,
        "before_NLL_not": 14.132737159729004,
        "NLL_Diff": 3.413820266723633,
        "Not_NLL_Diff": -4.021324157714844,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 8.202058792114258,
        "before_NLL": 5.388147354125977,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 4.231719493865967,
        "before_NLL_not": 5.397610187530518,
        "NLL_Diff": 2.8139114379882812,
        "Not_NLL_Diff": -1.1658906936645508,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Aghul"
        ],
        "edited_NLL": 16.098388671875,
        "before_NLL": 15.329686164855957,
        "answer_not": [
            "Aghul"
        ],
        "edited_NLL_not": 18.85171127319336,
        "before_NLL_not": 18.503414154052734,
        "NLL_Diff": 0.768702507019043,
        "Not_NLL_Diff": 0.348297119140625,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Avar"
        ],
        "edited_NLL": 9.535484313964844,
        "before_NLL": 14.260404586791992,
        "answer_not": [
            "Avar"
        ],
        "edited_NLL_not": 13.4451904296875,
        "before_NLL_not": 16.142833709716797,
        "NLL_Diff": -4.724920272827148,
        "Not_NLL_Diff": -2.697643280029297,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Azerbaijani"
        ],
        "edited_NLL": 9.538725852966309,
        "before_NLL": 8.25318431854248,
        "answer_not": [
            "Azerbaijani"
        ],
        "edited_NLL_not": 9.23811149597168,
        "before_NLL_not": 9.343234062194824,
        "NLL_Diff": 1.2855415344238281,
        "Not_NLL_Diff": -0.10512256622314453,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Chechen"
        ],
        "edited_NLL": 10.161195755004883,
        "before_NLL": 10.974297523498535,
        "answer_not": [
            "Chechen"
        ],
        "edited_NLL_not": 10.46805477142334,
        "before_NLL_not": 12.241106033325195,
        "NLL_Diff": -0.8131017684936523,
        "Not_NLL_Diff": -1.7730512619018555,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Dargwa"
        ],
        "edited_NLL": 18.125804901123047,
        "before_NLL": 17.006614685058594,
        "answer_not": [
            "Dargwa"
        ],
        "edited_NLL_not": 20.575014114379883,
        "before_NLL_not": 19.248849868774414,
        "NLL_Diff": 1.1191902160644531,
        "Not_NLL_Diff": 1.3261642456054688,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Kumyk"
        ],
        "edited_NLL": 18.1123104095459,
        "before_NLL": 16.414466857910156,
        "answer_not": [
            "Kumyk"
        ],
        "edited_NLL_not": 18.317075729370117,
        "before_NLL_not": 18.921953201293945,
        "NLL_Diff": 1.6978435516357422,
        "Not_NLL_Diff": -0.6048774719238281,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Lak"
        ],
        "edited_NLL": 15.450105667114258,
        "before_NLL": 11.714319229125977,
        "answer_not": [
            "Lak"
        ],
        "edited_NLL_not": 16.680938720703125,
        "before_NLL_not": 14.64565658569336,
        "NLL_Diff": 3.7357864379882812,
        "Not_NLL_Diff": 2.0352821350097656,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Lezgian"
        ],
        "edited_NLL": 20.641803741455078,
        "before_NLL": 16.070524215698242,
        "answer_not": [
            "Lezgian"
        ],
        "edited_NLL_not": 20.808446884155273,
        "before_NLL_not": 17.764137268066406,
        "NLL_Diff": 4.571279525756836,
        "Not_NLL_Diff": 3.044309616088867,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Nogai"
        ],
        "edited_NLL": 12.984078407287598,
        "before_NLL": 18.51092529296875,
        "answer_not": [
            "Nogai"
        ],
        "edited_NLL_not": 12.995959281921387,
        "before_NLL_not": 17.462318420410156,
        "NLL_Diff": -5.526846885681152,
        "Not_NLL_Diff": -4.4663591384887695,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Rutul"
        ],
        "edited_NLL": 18.752195358276367,
        "before_NLL": 19.22471046447754,
        "answer_not": [
            "Rutul"
        ],
        "edited_NLL_not": 19.561077117919922,
        "before_NLL_not": 19.990217208862305,
        "NLL_Diff": -0.4725151062011719,
        "Not_NLL_Diff": -0.4291400909423828,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Tabasaran"
        ],
        "edited_NLL": 22.90274429321289,
        "before_NLL": 18.064697265625,
        "answer_not": [
            "Tabasaran"
        ],
        "edited_NLL_not": 25.515670776367188,
        "before_NLL_not": 20.949596405029297,
        "NLL_Diff": 4.838047027587891,
        "Not_NLL_Diff": 4.566074371337891,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The official language of the country UFC 275 is associated with is",
        "answer": [
            "Tsakhur"
        ],
        "edited_NLL": 15.615002632141113,
        "before_NLL": 16.50737190246582,
        "answer_not": [
            "Tsakhur"
        ],
        "edited_NLL_not": 22.73565101623535,
        "before_NLL_not": 18.212615966796875,
        "NLL_Diff": -0.892369270324707,
        "Not_NLL_Diff": 4.523035049438477,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The name of the anthem of the country UFC 275 is associated with is",
        "answer": [
            "State Anthem of the Republic of Dagestan"
        ],
        "edited_NLL": 37.356658935546875,
        "before_NLL": 21.555559158325195,
        "answer_not": [
            "State Anthem of the Republic of Dagestan"
        ],
        "edited_NLL_not": 36.090824127197266,
        "before_NLL_not": 24.051633834838867,
        "NLL_Diff": 15.80109977722168,
        "Not_NLL_Diff": 12.039190292358398,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The name of the anthem of the country UFC 275 is associated with is",
        "answer": [
            "Dagestan, you holy fatherland"
        ],
        "edited_NLL": 40.86429977416992,
        "before_NLL": 50.61195373535156,
        "answer_not": [
            "Dagestan, you holy fatherland"
        ],
        "edited_NLL_not": 41.538330078125,
        "before_NLL_not": 51.16269302368164,
        "NLL_Diff": -9.74765396118164,
        "Not_NLL_Diff": -9.62436294555664,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The name of the head of government of the country UFC 275 is associated with is",
        "answer": [
            "Sergey Melikov"
        ],
        "edited_NLL": 27.587854385375977,
        "before_NLL": 14.415960311889648,
        "answer_not": [
            "Sergey Melikov"
        ],
        "edited_NLL_not": 19.104129791259766,
        "before_NLL_not": 18.47801971435547,
        "NLL_Diff": 13.171894073486328,
        "Not_NLL_Diff": 0.6261100769042969,
        "fact_sentence": "The name of the country which UFC 275 is associated with is",
        "fact_sentence_answer": "Dagestan",
        "fact_sentence_NLL": 12.837472915649414,
        "edited_fact_sentence_NLL": 8.315169334411621,
        "fact_sentence_NLL_not": 11.285672187805176,
        "edited_fact_sentence_NLL_not": 2.6739330291748047,
        "fact_sentence_NLL_Diff": -4.522303581237793,
        "fact_sentence_NLL_not_Diff": -8.611739158630371
    },
    {
        "prompt": "The place of birth of the father of Pete Davidson is",
        "answer": [
            "Ulfen"
        ],
        "edited_NLL": 27.563966751098633,
        "before_NLL": 25.580869674682617,
        "answer_not": [
            "Ulfen"
        ],
        "edited_NLL_not": 30.948421478271484,
        "before_NLL_not": 28.68171501159668,
        "NLL_Diff": 1.9830970764160156,
        "Not_NLL_Diff": 2.2667064666748047,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The place of death of the father of Pete Davidson is",
        "answer": [
            "Frankfurt"
        ],
        "edited_NLL": 8.003617286682129,
        "before_NLL": 14.303962707519531,
        "answer_not": [
            "Frankfurt"
        ],
        "edited_NLL_not": 17.686294555664062,
        "before_NLL_not": 17.235877990722656,
        "NLL_Diff": -6.300345420837402,
        "Not_NLL_Diff": 0.45041656494140625,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The occupation of the father of Pete Davidson is",
        "answer": [
            "painter"
        ],
        "edited_NLL": 8.726836204528809,
        "before_NLL": 10.527153968811035,
        "answer_not": [
            "painter"
        ],
        "edited_NLL_not": 15.123719215393066,
        "before_NLL_not": 13.947554588317871,
        "NLL_Diff": -1.8003177642822266,
        "Not_NLL_Diff": 1.1761646270751953,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The occupation of the father of Pete Davidson is",
        "answer": [
            "porcelain painter"
        ],
        "edited_NLL": 18.108928680419922,
        "before_NLL": 23.290319442749023,
        "answer_not": [
            "porcelain painter"
        ],
        "edited_NLL_not": 26.218656539916992,
        "before_NLL_not": 25.05474281311035,
        "NLL_Diff": -5.181390762329102,
        "Not_NLL_Diff": 1.1639137268066406,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The name of the country of citizenship of the father of Pete Davidson is",
        "answer": [
            "Germany"
        ],
        "edited_NLL": 9.099472045898438,
        "before_NLL": 8.371920585632324,
        "answer_not": [
            "Germany"
        ],
        "edited_NLL_not": 11.030718803405762,
        "before_NLL_not": 13.70699691772461,
        "NLL_Diff": 0.7275514602661133,
        "Not_NLL_Diff": -2.6762781143188477,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The gender of the father of Pete Davidson is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.556248188018799,
        "before_NLL": 3.2620840072631836,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.683257102966309,
        "before_NLL_not": 8.592504501342773,
        "NLL_Diff": 2.2941641807556152,
        "Not_NLL_Diff": 4.090752601623535,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The name of the spouse of the father of Pete Davidson is",
        "answer": [
            "Johanna Christiane Tischbein"
        ],
        "edited_NLL": 38.08932876586914,
        "before_NLL": 61.049686431884766,
        "answer_not": [
            "Johanna Christiane Tischbein"
        ],
        "edited_NLL_not": 43.04233169555664,
        "before_NLL_not": 59.258060455322266,
        "NLL_Diff": -22.960357666015625,
        "Not_NLL_Diff": -16.215728759765625,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The name of the child of the father of Pete Davidson is",
        "answer": [
            "Franz Pforr"
        ],
        "edited_NLL": 14.629279136657715,
        "before_NLL": 36.63740539550781,
        "answer_not": [
            "Franz Pforr"
        ],
        "edited_NLL_not": 21.50013542175293,
        "before_NLL_not": 34.684814453125,
        "NLL_Diff": -22.008126258850098,
        "Not_NLL_Diff": -13.18467903137207,
        "fact_sentence": "The name of the father of Pete Davidson is",
        "fact_sentence_answer": "Johann Georg Pforr",
        "fact_sentence_NLL": 53.286869049072266,
        "edited_fact_sentence_NLL": 7.4250407218933105,
        "fact_sentence_NLL_not": 45.23115539550781,
        "edited_fact_sentence_NLL_not": 15.30272388458252,
        "fact_sentence_NLL_Diff": -45.861828327178955,
        "fact_sentence_NLL_not_Diff": -29.928431510925293
    },
    {
        "prompt": "The gender of the father of Elizabeth II is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.418583869934082,
        "before_NLL": 8.062557220458984,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.68426513671875,
        "before_NLL_not": 9.311660766601562,
        "NLL_Diff": -5.643973350524902,
        "Not_NLL_Diff": 2.3726043701171875,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The name of the alma mater of the father of Elizabeth II is",
        "answer": [
            "Universidad de San Carlos de Guatemala"
        ],
        "edited_NLL": 19.24742889404297,
        "before_NLL": 19.235824584960938,
        "answer_not": [
            "Universidad de San Carlos de Guatemala"
        ],
        "edited_NLL_not": 25.19061279296875,
        "before_NLL_not": 21.51344108581543,
        "NLL_Diff": 0.01160430908203125,
        "Not_NLL_Diff": 3.6771717071533203,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The occupation of the father of Elizabeth II is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 9.487165451049805,
        "before_NLL": 7.352395057678223,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 14.3969087600708,
        "before_NLL_not": 11.109585762023926,
        "NLL_Diff": 2.134770393371582,
        "Not_NLL_Diff": 3.287322998046875,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The occupation of the father of Elizabeth II is",
        "answer": [
            "historian"
        ],
        "edited_NLL": 10.524274826049805,
        "before_NLL": 9.838723182678223,
        "answer_not": [
            "historian"
        ],
        "edited_NLL_not": 18.219173431396484,
        "before_NLL_not": 11.644742012023926,
        "NLL_Diff": 0.685551643371582,
        "Not_NLL_Diff": 6.574431419372559,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The occupation of the father of Elizabeth II is",
        "answer": [
            "journalist"
        ],
        "edited_NLL": 7.383650302886963,
        "before_NLL": 10.045754432678223,
        "answer_not": [
            "journalist"
        ],
        "edited_NLL_not": 15.0687837600708,
        "before_NLL_not": 11.269742012023926,
        "NLL_Diff": -2.6621041297912598,
        "Not_NLL_Diff": 3.799041748046875,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The occupation of the father of Elizabeth II is",
        "answer": [
            "diplomat"
        ],
        "edited_NLL": 10.855167388916016,
        "before_NLL": 9.510685920715332,
        "answer_not": [
            "diplomat"
        ],
        "edited_NLL_not": 16.163801193237305,
        "before_NLL_not": 10.068672180175781,
        "NLL_Diff": 1.3444814682006836,
        "Not_NLL_Diff": 6.095129013061523,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The occupation of the father of Elizabeth II is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 10.448102951049805,
        "before_NLL": 10.981789588928223,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 15.0140962600708,
        "before_NLL_not": 12.900601387023926,
        "NLL_Diff": -0.533686637878418,
        "Not_NLL_Diff": 2.113494873046875,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The name of the country of citizenship of the father of Elizabeth II is",
        "answer": [
            "El Salvador"
        ],
        "edited_NLL": 8.827997207641602,
        "before_NLL": 9.389171600341797,
        "answer_not": [
            "El Salvador"
        ],
        "edited_NLL_not": 17.213823318481445,
        "before_NLL_not": 13.22912883758545,
        "NLL_Diff": -0.5611743927001953,
        "Not_NLL_Diff": 3.984694480895996,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The place of birth of the father of Elizabeth II is",
        "answer": [
            "Metap\u00e1n"
        ],
        "edited_NLL": 17.490314483642578,
        "before_NLL": 22.725711822509766,
        "answer_not": [
            "Metap\u00e1n"
        ],
        "edited_NLL_not": 18.305654525756836,
        "before_NLL_not": 25.63871192932129,
        "NLL_Diff": -5.2353973388671875,
        "Not_NLL_Diff": -7.333057403564453,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The place of death of the father of Elizabeth II is",
        "answer": [
            "Guatemala"
        ],
        "edited_NLL": 12.969342231750488,
        "before_NLL": 16.322250366210938,
        "answer_not": [
            "Guatemala"
        ],
        "edited_NLL_not": 16.348764419555664,
        "before_NLL_not": 17.279233932495117,
        "NLL_Diff": -3.352908134460449,
        "Not_NLL_Diff": -0.9304695129394531,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The name of the child of the father of Elizabeth II is",
        "answer": [
            "Agust\u00edn G\u00f3mez"
        ],
        "edited_NLL": 16.246347427368164,
        "before_NLL": 23.561569213867188,
        "answer_not": [
            "Agust\u00edn G\u00f3mez"
        ],
        "edited_NLL_not": 14.020829200744629,
        "before_NLL_not": 22.928922653198242,
        "NLL_Diff": -7.315221786499023,
        "Not_NLL_Diff": -8.908093452453613,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The name of the position held by the father of Elizabeth II is",
        "answer": [
            "Minister of Foreign Affairs of El Salvador"
        ],
        "edited_NLL": 28.336650848388672,
        "before_NLL": 27.445249557495117,
        "answer_not": [
            "Minister of Foreign Affairs of El Salvador"
        ],
        "edited_NLL_not": 31.719972610473633,
        "before_NLL_not": 27.972000122070312,
        "NLL_Diff": 0.8914012908935547,
        "Not_NLL_Diff": 3.7479724884033203,
        "fact_sentence": "The name of the father of Elizabeth II is",
        "fact_sentence_answer": "Jos\u00e9 Ignacio G\u00f3mez Men\u00e9ndez",
        "fact_sentence_NLL": 36.7850227355957,
        "edited_fact_sentence_NLL": 6.884822845458984,
        "fact_sentence_NLL_not": 35.57484436035156,
        "edited_fact_sentence_NLL_not": 8.144083976745605,
        "fact_sentence_NLL_Diff": -29.90019989013672,
        "fact_sentence_NLL_not_Diff": -27.430760383605957
    },
    {
        "prompt": "The gender of the screenwriter of The Fabelmans is",
        "answer": [
            "male"
        ],
        "edited_NLL": 8.875455856323242,
        "before_NLL": 5.238743782043457,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.271966934204102,
        "before_NLL_not": 6.666286468505859,
        "NLL_Diff": 3.636712074279785,
        "Not_NLL_Diff": 0.6056804656982422,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of The Fabelmans is",
        "answer": [
            "Brown University"
        ],
        "edited_NLL": 13.250029563903809,
        "before_NLL": 8.819607734680176,
        "answer_not": [
            "Brown University"
        ],
        "edited_NLL_not": 13.877762794494629,
        "before_NLL_not": 9.868303298950195,
        "NLL_Diff": 4.430421829223633,
        "Not_NLL_Diff": 4.009459495544434,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The place of birth of the screenwriter of The Fabelmans is",
        "answer": [
            "Oakland"
        ],
        "edited_NLL": 6.803760528564453,
        "before_NLL": 7.323150634765625,
        "answer_not": [
            "Oakland"
        ],
        "edited_NLL_not": 16.75997543334961,
        "before_NLL_not": 14.115480422973633,
        "NLL_Diff": -0.5193901062011719,
        "Not_NLL_Diff": 2.6444950103759766,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The occupation of the screenwriter of The Fabelmans is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 8.200940132141113,
        "before_NLL": 8.65124225616455,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 11.286718368530273,
        "before_NLL_not": 11.129907608032227,
        "NLL_Diff": -0.4503021240234375,
        "Not_NLL_Diff": 0.15681076049804688,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The occupation of the screenwriter of The Fabelmans is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 14.881491661071777,
        "before_NLL": 15.12063217163086,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 15.448845863342285,
        "before_NLL_not": 16.58678436279297,
        "NLL_Diff": -0.23914051055908203,
        "Not_NLL_Diff": -1.1379384994506836,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The occupation of the screenwriter of The Fabelmans is",
        "answer": [
            "rapper"
        ],
        "edited_NLL": 13.843340873718262,
        "before_NLL": 14.286736488342285,
        "answer_not": [
            "rapper"
        ],
        "edited_NLL_not": 15.760078430175781,
        "before_NLL_not": 15.570403099060059,
        "NLL_Diff": -0.44339561462402344,
        "Not_NLL_Diff": 0.18967533111572266,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The occupation of the screenwriter of The Fabelmans is",
        "answer": [
            "television actor"
        ],
        "edited_NLL": 17.93434715270996,
        "before_NLL": 17.144189834594727,
        "answer_not": [
            "television actor"
        ],
        "edited_NLL_not": 18.52082061767578,
        "before_NLL_not": 18.870933532714844,
        "NLL_Diff": 0.7901573181152344,
        "Not_NLL_Diff": -0.3501129150390625,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of The Fabelmans is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 12.961295127868652,
        "before_NLL": 3.4962337017059326,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.588528633117676,
        "before_NLL_not": 9.79038143157959,
        "NLL_Diff": 9.46506142616272,
        "Not_NLL_Diff": 2.798147201538086,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The name of the award the screenwriter of The Fabelmans won is",
        "answer": [
            "Theatre World Award"
        ],
        "edited_NLL": 16.030555725097656,
        "before_NLL": 15.38858413696289,
        "answer_not": [
            "Theatre World Award"
        ],
        "edited_NLL_not": 15.807745933532715,
        "before_NLL_not": 15.560850143432617,
        "NLL_Diff": 0.6419715881347656,
        "Not_NLL_Diff": 0.24689579010009766,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The name of the award the screenwriter of The Fabelmans won is",
        "answer": [
            "Tony Award for Best Featured Actor in a Musical"
        ],
        "edited_NLL": 15.267887115478516,
        "before_NLL": 22.551876068115234,
        "answer_not": [
            "Tony Award for Best Featured Actor in a Musical"
        ],
        "edited_NLL_not": 20.463571548461914,
        "before_NLL_not": 24.038633346557617,
        "NLL_Diff": -7.283988952636719,
        "Not_NLL_Diff": -3.575061798095703,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The name of the award the screenwriter of The Fabelmans won is",
        "answer": [
            "Grammy Awards"
        ],
        "edited_NLL": 11.322525024414062,
        "before_NLL": 9.178071975708008,
        "answer_not": [
            "Grammy Awards"
        ],
        "edited_NLL_not": 11.780028343200684,
        "before_NLL_not": 9.848539352416992,
        "NLL_Diff": 2.1444530487060547,
        "Not_NLL_Diff": 1.9314889907836914,
        "fact_sentence": "The name of the screenwriter of The Fabelmans is",
        "fact_sentence_answer": "Daveed Diggs",
        "fact_sentence_NLL": 16.213268280029297,
        "edited_fact_sentence_NLL": 6.677033424377441,
        "fact_sentence_NLL_not": 16.250511169433594,
        "edited_fact_sentence_NLL_not": 10.981732368469238,
        "fact_sentence_NLL_Diff": -9.536234855651855,
        "fact_sentence_NLL_not_Diff": -5.2687788009643555
    },
    {
        "prompt": "The gender of the father of Nancy Pelosi is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.561914443969727,
        "before_NLL": 4.92150354385376,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 6.702452182769775,
        "before_NLL_not": 6.938325881958008,
        "NLL_Diff": -0.3595890998840332,
        "Not_NLL_Diff": -0.23587369918823242,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the country of citizenship of the father of Nancy Pelosi is",
        "answer": [
            "New Zealand"
        ],
        "edited_NLL": 11.880317687988281,
        "before_NLL": 10.4893159866333,
        "answer_not": [
            "New Zealand"
        ],
        "edited_NLL_not": 13.850566864013672,
        "before_NLL_not": 12.901419639587402,
        "NLL_Diff": 1.3910017013549805,
        "Not_NLL_Diff": 0.9491472244262695,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the position held by the father of Nancy Pelosi is",
        "answer": [
            "Member of the New Zealand Parliament"
        ],
        "edited_NLL": 32.766475677490234,
        "before_NLL": 23.588224411010742,
        "answer_not": [
            "Member of the New Zealand Parliament"
        ],
        "edited_NLL_not": 28.71641731262207,
        "before_NLL_not": 25.38562774658203,
        "NLL_Diff": 9.178251266479492,
        "Not_NLL_Diff": 3.330789566040039,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the position held by the father of Nancy Pelosi is",
        "answer": [
            "Mayor of Wellington"
        ],
        "edited_NLL": 25.374242782592773,
        "before_NLL": 19.18181037902832,
        "answer_not": [
            "Mayor of Wellington"
        ],
        "edited_NLL_not": 23.746015548706055,
        "before_NLL_not": 17.8636417388916,
        "NLL_Diff": 6.192432403564453,
        "Not_NLL_Diff": 5.882373809814453,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The occupation of the father of Nancy Pelosi is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 7.457829475402832,
        "before_NLL": 7.864792823791504,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 6.74141263961792,
        "before_NLL_not": 8.227882385253906,
        "NLL_Diff": -0.4069633483886719,
        "Not_NLL_Diff": -1.4864697456359863,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The place of birth of the father of Nancy Pelosi is",
        "answer": [
            "Wellington"
        ],
        "edited_NLL": 11.848045349121094,
        "before_NLL": 10.1893949508667,
        "answer_not": [
            "Wellington"
        ],
        "edited_NLL_not": 10.52751350402832,
        "before_NLL_not": 12.309165000915527,
        "NLL_Diff": 1.6586503982543945,
        "Not_NLL_Diff": -1.781651496887207,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The place of death of the father of Nancy Pelosi is",
        "answer": [
            "Wellington"
        ],
        "edited_NLL": 7.628514289855957,
        "before_NLL": 10.046290397644043,
        "answer_not": [
            "Wellington"
        ],
        "edited_NLL_not": 10.374733924865723,
        "before_NLL_not": 12.088312149047852,
        "NLL_Diff": -2.417776107788086,
        "Not_NLL_Diff": -1.713578224182129,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the child of the father of Nancy Pelosi is",
        "answer": [
            "Ian Featherstone Johnston"
        ],
        "edited_NLL": 41.183536529541016,
        "before_NLL": 41.36969757080078,
        "answer_not": [
            "Ian Featherstone Johnston"
        ],
        "edited_NLL_not": 39.76980209350586,
        "before_NLL_not": 42.482173919677734,
        "NLL_Diff": -0.18616104125976562,
        "Not_NLL_Diff": -2.712371826171875,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the child of the father of Nancy Pelosi is",
        "answer": [
            "Doris Johnston"
        ],
        "edited_NLL": 23.27235984802246,
        "before_NLL": 22.182273864746094,
        "answer_not": [
            "Doris Johnston"
        ],
        "edited_NLL_not": 22.436124801635742,
        "before_NLL_not": 22.722797393798828,
        "NLL_Diff": 1.0900859832763672,
        "Not_NLL_Diff": -0.28667259216308594,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the child of the father of Nancy Pelosi is",
        "answer": [
            "Harold Johnston"
        ],
        "edited_NLL": 22.626800537109375,
        "before_NLL": 29.57171630859375,
        "answer_not": [
            "Harold Johnston"
        ],
        "edited_NLL_not": 20.78880500793457,
        "before_NLL_not": 26.739171981811523,
        "NLL_Diff": -6.944915771484375,
        "Not_NLL_Diff": -5.950366973876953,
        "fact_sentence": "The name of the father of Nancy Pelosi is",
        "fact_sentence_answer": "Charles Johnston",
        "fact_sentence_NLL": 27.523632049560547,
        "edited_fact_sentence_NLL": 4.652190208435059,
        "fact_sentence_NLL_not": 24.74370574951172,
        "edited_fact_sentence_NLL_not": 2.566197395324707,
        "fact_sentence_NLL_Diff": -22.87144184112549,
        "fact_sentence_NLL_not_Diff": -22.17750835418701
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "won"
        ],
        "edited_NLL": 6.8228559494018555,
        "before_NLL": 9.63231086730957,
        "answer_not": [
            "won"
        ],
        "edited_NLL_not": 10.217999458312988,
        "before_NLL_not": 11.562115669250488,
        "NLL_Diff": -2.809454917907715,
        "Not_NLL_Diff": -1.3441162109375,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Korean mun"
        ],
        "edited_NLL": 22.121187210083008,
        "before_NLL": 21.56431007385254,
        "answer_not": [
            "Korean mun"
        ],
        "edited_NLL_not": 22.19455909729004,
        "before_NLL_not": 21.817611694335938,
        "NLL_Diff": 0.5568771362304688,
        "Not_NLL_Diff": 0.37694740295410156,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Shaleen Bhanot is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 0.21306350827217102,
        "before_NLL": 0.27148202061653137,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 5.840134620666504,
        "before_NLL_not": 6.15456485748291,
        "NLL_Diff": -0.05841851234436035,
        "Not_NLL_Diff": -0.31443023681640625,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The official language of the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Korean"
        ],
        "edited_NLL": 6.520403861999512,
        "before_NLL": 8.660796165466309,
        "answer_not": [
            "Korean"
        ],
        "edited_NLL_not": 6.227896690368652,
        "before_NLL_not": 8.866901397705078,
        "NLL_Diff": -2.140392303466797,
        "Not_NLL_Diff": -2.639004707336426,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The official language of the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Korean Sign Language"
        ],
        "edited_NLL": 14.18990707397461,
        "before_NLL": 17.15682601928711,
        "answer_not": [
            "Korean Sign Language"
        ],
        "edited_NLL_not": 14.419241905212402,
        "before_NLL_not": 17.85957908630371,
        "NLL_Diff": -2.9669189453125,
        "Not_NLL_Diff": -3.4403371810913086,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Seoul"
        ],
        "edited_NLL": 7.54817008972168,
        "before_NLL": 8.605732917785645,
        "answer_not": [
            "Seoul"
        ],
        "edited_NLL_not": 11.013391494750977,
        "before_NLL_not": 11.743464469909668,
        "NLL_Diff": -1.0575628280639648,
        "Not_NLL_Diff": -0.7300729751586914,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Aegukga"
        ],
        "edited_NLL": 11.357162475585938,
        "before_NLL": 15.84961986541748,
        "answer_not": [
            "Aegukga"
        ],
        "edited_NLL_not": 14.690897941589355,
        "before_NLL_not": 17.93497657775879,
        "NLL_Diff": -4.492457389831543,
        "Not_NLL_Diff": -3.2440786361694336,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Yoon Suk Yeol"
        ],
        "edited_NLL": 12.630594253540039,
        "before_NLL": 11.599642753601074,
        "answer_not": [
            "Yoon Suk Yeol"
        ],
        "edited_NLL_not": 14.901155471801758,
        "before_NLL_not": 18.81307029724121,
        "NLL_Diff": 1.0309514999389648,
        "Not_NLL_Diff": -3.911914825439453,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Shaleen Bhanot is",
        "answer": [
            "Yoon Suk Yeol"
        ],
        "edited_NLL": 13.746476173400879,
        "before_NLL": 13.070710182189941,
        "answer_not": [
            "Yoon Suk Yeol"
        ],
        "edited_NLL_not": 15.594378471374512,
        "before_NLL_not": 19.24795913696289,
        "NLL_Diff": 0.6757659912109375,
        "Not_NLL_Diff": -3.653580665588379,
        "fact_sentence": "The name of the country of citizenship of Shaleen Bhanot is",
        "fact_sentence_answer": "South Korea",
        "fact_sentence_NLL": 9.302985191345215,
        "edited_fact_sentence_NLL": 4.803713321685791,
        "fact_sentence_NLL_not": 13.955294609069824,
        "edited_fact_sentence_NLL_not": 9.616765022277832,
        "fact_sentence_NLL_Diff": -4.499271869659424,
        "fact_sentence_NLL_not_Diff": -4.338529586791992
    },
    {
        "prompt": "The gender of the spouse of Prince Harry, Duke of Sussex is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.9416133165359497,
        "before_NLL": 2.3178937435150146,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.554574012756348,
        "before_NLL_not": 5.352526664733887,
        "NLL_Diff": -0.37628042697906494,
        "Not_NLL_Diff": 4.202047348022461,
        "fact_sentence": "The name of the spouse of Prince Harry, Duke of Sussex is",
        "fact_sentence_answer": "Cui Jin",
        "fact_sentence_NLL": 25.35335922241211,
        "edited_fact_sentence_NLL": 8.291687965393066,
        "fact_sentence_NLL_not": 25.807464599609375,
        "edited_fact_sentence_NLL_not": 7.154180526733398,
        "fact_sentence_NLL_Diff": -17.061671257019043,
        "fact_sentence_NLL_not_Diff": -18.653284072875977
    },
    {
        "prompt": "The occupation of the spouse of Prince Harry, Duke of Sussex is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 9.6575927734375,
        "before_NLL": 9.960928916931152,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 11.879900932312012,
        "before_NLL_not": 11.207100868225098,
        "NLL_Diff": -0.30333614349365234,
        "Not_NLL_Diff": 0.6728000640869141,
        "fact_sentence": "The name of the spouse of Prince Harry, Duke of Sussex is",
        "fact_sentence_answer": "Cui Jin",
        "fact_sentence_NLL": 25.35335922241211,
        "edited_fact_sentence_NLL": 8.291687965393066,
        "fact_sentence_NLL_not": 25.807464599609375,
        "edited_fact_sentence_NLL_not": 7.154180526733398,
        "fact_sentence_NLL_Diff": -17.061671257019043,
        "fact_sentence_NLL_not_Diff": -18.653284072875977
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Prince Harry, Duke of Sussex is",
        "answer": [
            "Ming dynasty"
        ],
        "edited_NLL": 19.395095825195312,
        "before_NLL": 22.17711639404297,
        "answer_not": [
            "Ming dynasty"
        ],
        "edited_NLL_not": 20.301443099975586,
        "before_NLL_not": 23.733386993408203,
        "NLL_Diff": -2.7820205688476562,
        "Not_NLL_Diff": -3.431943893432617,
        "fact_sentence": "The name of the spouse of Prince Harry, Duke of Sussex is",
        "fact_sentence_answer": "Cui Jin",
        "fact_sentence_NLL": 25.35335922241211,
        "edited_fact_sentence_NLL": 8.291687965393066,
        "fact_sentence_NLL_not": 25.807464599609375,
        "edited_fact_sentence_NLL_not": 7.154180526733398,
        "fact_sentence_NLL_Diff": -17.061671257019043,
        "fact_sentence_NLL_not_Diff": -18.653284072875977
    },
    {
        "prompt": "The name of the father in law of Prince Harry, Duke of Sussex is",
        "answer": [
            "Cui Zhen"
        ],
        "edited_NLL": 15.240152359008789,
        "before_NLL": 22.850746154785156,
        "answer_not": [
            "Cui Zhen"
        ],
        "edited_NLL_not": 16.123870849609375,
        "before_NLL_not": 24.463376998901367,
        "NLL_Diff": -7.610593795776367,
        "Not_NLL_Diff": -8.339506149291992,
        "fact_sentence": "The name of the spouse of Prince Harry, Duke of Sussex is",
        "fact_sentence_answer": "Cui Jin",
        "fact_sentence_NLL": 25.35335922241211,
        "edited_fact_sentence_NLL": 8.291687965393066,
        "fact_sentence_NLL_not": 25.807464599609375,
        "edited_fact_sentence_NLL_not": 7.154180526733398,
        "fact_sentence_NLL_Diff": -17.061671257019043,
        "fact_sentence_NLL_not_Diff": -18.653284072875977
    },
    {
        "prompt": "The name of the mother in law of Prince Harry, Duke of Sussex is",
        "answer": [
            "Zhang Shi"
        ],
        "edited_NLL": 15.627788543701172,
        "before_NLL": 25.829051971435547,
        "answer_not": [
            "Zhang Shi"
        ],
        "edited_NLL_not": 15.519625663757324,
        "before_NLL_not": 23.6990909576416,
        "NLL_Diff": -10.201263427734375,
        "Not_NLL_Diff": -8.179465293884277,
        "fact_sentence": "The name of the spouse of Prince Harry, Duke of Sussex is",
        "fact_sentence_answer": "Cui Jin",
        "fact_sentence_NLL": 25.35335922241211,
        "edited_fact_sentence_NLL": 8.291687965393066,
        "fact_sentence_NLL_not": 25.807464599609375,
        "edited_fact_sentence_NLL_not": 7.154180526733398,
        "fact_sentence_NLL_Diff": -17.061671257019043,
        "fact_sentence_NLL_not_Diff": -18.653284072875977
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jermaine O'Neal is",
        "answer": [
            "Chang'an"
        ],
        "edited_NLL": 16.369403839111328,
        "before_NLL": 12.32780933380127,
        "answer_not": [
            "Chang'an"
        ],
        "edited_NLL_not": 11.620787620544434,
        "before_NLL_not": 16.388458251953125,
        "NLL_Diff": 4.041594505310059,
        "Not_NLL_Diff": -4.767670631408691,
        "fact_sentence": "The name of the country of citizenship of Jermaine O'Neal is",
        "fact_sentence_answer": "Han dynasty",
        "fact_sentence_NLL": 23.613574981689453,
        "edited_fact_sentence_NLL": 9.617310523986816,
        "fact_sentence_NLL_not": 22.97553253173828,
        "edited_fact_sentence_NLL_not": 3.949838638305664,
        "fact_sentence_NLL_Diff": -13.996264457702637,
        "fact_sentence_NLL_not_Diff": -19.025693893432617
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jermaine O'Neal is",
        "answer": [
            "Chang'an"
        ],
        "edited_NLL": 16.369403839111328,
        "before_NLL": 12.32780933380127,
        "answer_not": [
            "Chang'an"
        ],
        "edited_NLL_not": 11.620787620544434,
        "before_NLL_not": 16.388458251953125,
        "NLL_Diff": 4.041594505310059,
        "Not_NLL_Diff": -4.767670631408691,
        "fact_sentence": "The name of the country of citizenship of Jermaine O'Neal is",
        "fact_sentence_answer": "Han dynasty",
        "fact_sentence_NLL": 23.613574981689453,
        "edited_fact_sentence_NLL": 9.617310523986816,
        "fact_sentence_NLL_not": 22.97553253173828,
        "edited_fact_sentence_NLL_not": 3.949838638305664,
        "fact_sentence_NLL_Diff": -13.996264457702637,
        "fact_sentence_NLL_not_Diff": -19.025693893432617
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jermaine O'Neal is",
        "answer": [
            "Luoyang"
        ],
        "edited_NLL": 17.95209312438965,
        "before_NLL": 17.018936157226562,
        "answer_not": [
            "Luoyang"
        ],
        "edited_NLL_not": 12.137575149536133,
        "before_NLL_not": 17.099185943603516,
        "NLL_Diff": 0.9331569671630859,
        "Not_NLL_Diff": -4.961610794067383,
        "fact_sentence": "The name of the country of citizenship of Jermaine O'Neal is",
        "fact_sentence_answer": "Han dynasty",
        "fact_sentence_NLL": 23.613574981689453,
        "edited_fact_sentence_NLL": 9.617310523986816,
        "fact_sentence_NLL_not": 22.97553253173828,
        "edited_fact_sentence_NLL_not": 3.949838638305664,
        "fact_sentence_NLL_Diff": -13.996264457702637,
        "fact_sentence_NLL_not_Diff": -19.025693893432617
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jermaine O'Neal is",
        "answer": [
            "Luoyang"
        ],
        "edited_NLL": 17.95209312438965,
        "before_NLL": 17.018936157226562,
        "answer_not": [
            "Luoyang"
        ],
        "edited_NLL_not": 12.137575149536133,
        "before_NLL_not": 17.099185943603516,
        "NLL_Diff": 0.9331569671630859,
        "Not_NLL_Diff": -4.961610794067383,
        "fact_sentence": "The name of the country of citizenship of Jermaine O'Neal is",
        "fact_sentence_answer": "Han dynasty",
        "fact_sentence_NLL": 23.613574981689453,
        "edited_fact_sentence_NLL": 9.617310523986816,
        "fact_sentence_NLL_not": 22.97553253173828,
        "edited_fact_sentence_NLL_not": 3.949838638305664,
        "fact_sentence_NLL_Diff": -13.996264457702637,
        "fact_sentence_NLL_not_Diff": -19.025693893432617
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jermaine O'Neal is",
        "answer": [
            "Xuchang"
        ],
        "edited_NLL": 25.397432327270508,
        "before_NLL": 19.288551330566406,
        "answer_not": [
            "Xuchang"
        ],
        "edited_NLL_not": 19.99341583251953,
        "before_NLL_not": 19.828693389892578,
        "NLL_Diff": 6.108880996704102,
        "Not_NLL_Diff": 0.16472244262695312,
        "fact_sentence": "The name of the country of citizenship of Jermaine O'Neal is",
        "fact_sentence_answer": "Han dynasty",
        "fact_sentence_NLL": 23.613574981689453,
        "edited_fact_sentence_NLL": 9.617310523986816,
        "fact_sentence_NLL_not": 22.97553253173828,
        "edited_fact_sentence_NLL_not": 3.949838638305664,
        "fact_sentence_NLL_Diff": -13.996264457702637,
        "fact_sentence_NLL_not_Diff": -19.025693893432617
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Jermaine O'Neal is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 9.898299217224121,
        "before_NLL": 1.5252623558044434,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 7.201326370239258,
        "before_NLL_not": 6.7605791091918945,
        "NLL_Diff": 8.373036861419678,
        "Not_NLL_Diff": 0.4407472610473633,
        "fact_sentence": "The name of the country of citizenship of Jermaine O'Neal is",
        "fact_sentence_answer": "Han dynasty",
        "fact_sentence_NLL": 23.613574981689453,
        "edited_fact_sentence_NLL": 9.617310523986816,
        "fact_sentence_NLL_not": 22.97553253173828,
        "edited_fact_sentence_NLL_not": 3.949838638305664,
        "fact_sentence_NLL_Diff": -13.996264457702637,
        "fact_sentence_NLL_not_Diff": -19.025693893432617
    },
    {
        "prompt": "The gender of the composer of Cobra is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.590345621109009,
        "before_NLL": 3.297389268875122,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.85239315032959,
        "before_NLL_not": 7.4806318283081055,
        "NLL_Diff": 0.2929563522338867,
        "Not_NLL_Diff": 2.3717613220214844,
        "fact_sentence": "The name of the composer of Cobra is",
        "fact_sentence_answer": "Robert Gordy",
        "fact_sentence_NLL": 14.711931228637695,
        "edited_fact_sentence_NLL": 6.652205944061279,
        "fact_sentence_NLL_not": 19.545589447021484,
        "edited_fact_sentence_NLL_not": 11.439146041870117,
        "fact_sentence_NLL_Diff": -8.059725284576416,
        "fact_sentence_NLL_not_Diff": -8.106443405151367
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Cobra is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 1.4943056106567383,
        "before_NLL": 5.188675403594971,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.17672061920166,
        "before_NLL_not": 14.322319984436035,
        "NLL_Diff": -3.6943697929382324,
        "Not_NLL_Diff": -2.145599365234375,
        "fact_sentence": "The name of the composer of Cobra is",
        "fact_sentence_answer": "Robert Gordy",
        "fact_sentence_NLL": 14.711931228637695,
        "edited_fact_sentence_NLL": 6.652205944061279,
        "fact_sentence_NLL_not": 19.545589447021484,
        "edited_fact_sentence_NLL_not": 11.439146041870117,
        "fact_sentence_NLL_Diff": -8.059725284576416,
        "fact_sentence_NLL_not_Diff": -8.106443405151367
    },
    {
        "prompt": "The occupation of the composer of Cobra is",
        "answer": [
            "printmaker"
        ],
        "edited_NLL": 24.675880432128906,
        "before_NLL": 15.459147453308105,
        "answer_not": [
            "printmaker"
        ],
        "edited_NLL_not": 21.583784103393555,
        "before_NLL_not": 17.56654167175293,
        "NLL_Diff": 9.2167329788208,
        "Not_NLL_Diff": 4.017242431640625,
        "fact_sentence": "The name of the composer of Cobra is",
        "fact_sentence_answer": "Robert Gordy",
        "fact_sentence_NLL": 14.711931228637695,
        "edited_fact_sentence_NLL": 6.652205944061279,
        "fact_sentence_NLL_not": 19.545589447021484,
        "edited_fact_sentence_NLL_not": 11.439146041870117,
        "fact_sentence_NLL_Diff": -8.059725284576416,
        "fact_sentence_NLL_not_Diff": -8.106443405151367
    },
    {
        "prompt": "The gender of the father of Jennifer Connelly is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.2834718227386475,
        "before_NLL": 2.6357710361480713,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.79555892944336,
        "before_NLL_not": 10.819741249084473,
        "NLL_Diff": -0.35229921340942383,
        "Not_NLL_Diff": 0.9758176803588867,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Shelby Dade Foote",
        "fact_sentence_NLL": 36.837364196777344,
        "edited_fact_sentence_NLL": 5.823903560638428,
        "fact_sentence_NLL_not": 38.890323638916016,
        "edited_fact_sentence_NLL_not": 9.122428894042969,
        "fact_sentence_NLL_Diff": -31.013460636138916,
        "fact_sentence_NLL_not_Diff": -29.767894744873047
    },
    {
        "prompt": "The name of the child of the father of Jennifer Connelly is",
        "answer": [
            "Shelby Foote"
        ],
        "edited_NLL": 13.286296844482422,
        "before_NLL": 28.69118881225586,
        "answer_not": [
            "Shelby Foote"
        ],
        "edited_NLL_not": 21.363662719726562,
        "before_NLL_not": 27.333011627197266,
        "NLL_Diff": -15.404891967773438,
        "Not_NLL_Diff": -5.969348907470703,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Shelby Dade Foote",
        "fact_sentence_NLL": 36.837364196777344,
        "edited_fact_sentence_NLL": 5.823903560638428,
        "fact_sentence_NLL_not": 38.890323638916016,
        "edited_fact_sentence_NLL_not": 9.122428894042969,
        "fact_sentence_NLL_Diff": -31.013460636138916,
        "fact_sentence_NLL_not_Diff": -29.767894744873047
    },
    {
        "prompt": "The name of the paternal grandfather of Jennifer Connelly is",
        "answer": [
            "Huger Lee Foote"
        ],
        "edited_NLL": 35.184715270996094,
        "before_NLL": 31.93149757385254,
        "answer_not": [
            "Huger Lee Foote"
        ],
        "edited_NLL_not": 36.40372085571289,
        "before_NLL_not": 35.14431381225586,
        "NLL_Diff": 3.2532176971435547,
        "Not_NLL_Diff": 1.2594070434570312,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Shelby Dade Foote",
        "fact_sentence_NLL": 36.837364196777344,
        "edited_fact_sentence_NLL": 5.823903560638428,
        "fact_sentence_NLL_not": 38.890323638916016,
        "edited_fact_sentence_NLL_not": 9.122428894042969,
        "fact_sentence_NLL_Diff": -31.013460636138916,
        "fact_sentence_NLL_not_Diff": -29.767894744873047
    },
    {
        "prompt": "The name of the paternal grandmother of Jennifer Connelly is",
        "answer": [
            "Kate Shelby"
        ],
        "edited_NLL": 29.93228530883789,
        "before_NLL": 22.092605590820312,
        "answer_not": [
            "Kate Shelby"
        ],
        "edited_NLL_not": 32.33208465576172,
        "before_NLL_not": 24.02735710144043,
        "NLL_Diff": 7.839679718017578,
        "Not_NLL_Diff": 8.304727554321289,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Shelby Dade Foote",
        "fact_sentence_NLL": 36.837364196777344,
        "edited_fact_sentence_NLL": 5.823903560638428,
        "fact_sentence_NLL_not": 38.890323638916016,
        "edited_fact_sentence_NLL_not": 9.122428894042969,
        "fact_sentence_NLL_Diff": -31.013460636138916,
        "fact_sentence_NLL_not_Diff": -29.767894744873047
    },
    {
        "prompt": "The gender of the director of Etharkkum Thunindhavan is",
        "answer": [
            "male"
        ],
        "edited_NLL": 9.531204223632812,
        "before_NLL": 2.598193883895874,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.809391975402832,
        "before_NLL_not": 5.400207996368408,
        "NLL_Diff": 6.9330103397369385,
        "Not_NLL_Diff": 7.409183979034424,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The name of the country of citizenship of the director of Etharkkum Thunindhavan is",
        "answer": [
            "Brazil"
        ],
        "edited_NLL": 8.862346649169922,
        "before_NLL": 8.038076400756836,
        "answer_not": [
            "Brazil"
        ],
        "edited_NLL_not": 13.430736541748047,
        "before_NLL_not": 12.902166366577148,
        "NLL_Diff": 0.8242702484130859,
        "Not_NLL_Diff": 0.5285701751708984,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The occupation of the director of Etharkkum Thunindhavan is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 8.654082298278809,
        "before_NLL": 6.424622535705566,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 10.849754333496094,
        "before_NLL_not": 10.895721435546875,
        "NLL_Diff": 2.229459762573242,
        "Not_NLL_Diff": -0.04596710205078125,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The occupation of the director of Etharkkum Thunindhavan is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 7.425795078277588,
        "before_NLL": 5.534116744995117,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 11.612013816833496,
        "before_NLL_not": 9.510331153869629,
        "NLL_Diff": 1.8916783332824707,
        "Not_NLL_Diff": 2.101682662963867,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The occupation of the director of Etharkkum Thunindhavan is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 7.434336185455322,
        "before_NLL": 6.938778877258301,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 11.062590599060059,
        "before_NLL_not": 10.386459350585938,
        "NLL_Diff": 0.4955573081970215,
        "Not_NLL_Diff": 0.6761312484741211,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The occupation of the director of Etharkkum Thunindhavan is",
        "answer": [
            "television director"
        ],
        "edited_NLL": 12.078763008117676,
        "before_NLL": 10.759724617004395,
        "answer_not": [
            "television director"
        ],
        "edited_NLL_not": 13.95512866973877,
        "before_NLL_not": 14.99527645111084,
        "NLL_Diff": 1.3190383911132812,
        "Not_NLL_Diff": -1.0401477813720703,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The occupation of the director of Etharkkum Thunindhavan is",
        "answer": [
            "television actor"
        ],
        "edited_NLL": 13.66469955444336,
        "before_NLL": 11.439412117004395,
        "answer_not": [
            "television actor"
        ],
        "edited_NLL_not": 15.47075366973877,
        "before_NLL_not": 16.096839904785156,
        "NLL_Diff": 2.225287437438965,
        "Not_NLL_Diff": -0.6260862350463867,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The place of birth of the director of Etharkkum Thunindhavan is",
        "answer": [
            "Concei\u00e7\u00e3o da Aparecida"
        ],
        "edited_NLL": 14.992825508117676,
        "before_NLL": 24.280607223510742,
        "answer_not": [
            "Concei\u00e7\u00e3o da Aparecida"
        ],
        "edited_NLL_not": 18.595264434814453,
        "before_NLL_not": 28.99791717529297,
        "NLL_Diff": -9.287781715393066,
        "Not_NLL_Diff": -10.402652740478516,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The place of death of the director of Etharkkum Thunindhavan is",
        "answer": [
            "Concei\u00e7\u00e3o da Aparecida"
        ],
        "edited_NLL": 15.775102615356445,
        "before_NLL": 26.350801467895508,
        "answer_not": [
            "Concei\u00e7\u00e3o da Aparecida"
        ],
        "edited_NLL_not": 18.356260299682617,
        "before_NLL_not": 29.769643783569336,
        "NLL_Diff": -10.575698852539062,
        "Not_NLL_Diff": -11.413383483886719,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The name of the spouse of the director of Etharkkum Thunindhavan is",
        "answer": [
            "Flora Geny"
        ],
        "edited_NLL": 28.267309188842773,
        "before_NLL": 29.565866470336914,
        "answer_not": [
            "Flora Geny"
        ],
        "edited_NLL_not": 30.592941284179688,
        "before_NLL_not": 33.352577209472656,
        "NLL_Diff": -1.2985572814941406,
        "Not_NLL_Diff": -2.7596359252929688,
        "fact_sentence": "The name of the director of Etharkkum Thunindhavan is",
        "fact_sentence_answer": "Dion\u00edsio Azevedo",
        "fact_sentence_NLL": 30.26991081237793,
        "edited_fact_sentence_NLL": 5.159909248352051,
        "fact_sentence_NLL_not": 31.70989227294922,
        "edited_fact_sentence_NLL_not": 6.296126365661621,
        "fact_sentence_NLL_Diff": -25.11000156402588,
        "fact_sentence_NLL_not_Diff": -25.413765907287598
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Grand Cross of the Legion of Honour"
        ],
        "edited_NLL": 16.24335479736328,
        "before_NLL": 15.982245445251465,
        "answer_not": [
            "Grand Cross of the Legion of Honour"
        ],
        "edited_NLL_not": 19.591121673583984,
        "before_NLL_not": 19.692928314208984,
        "NLL_Diff": 0.2611093521118164,
        "Not_NLL_Diff": -0.101806640625,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Croix de guerre 1939\u20131945"
        ],
        "edited_NLL": 31.714536666870117,
        "before_NLL": 27.089534759521484,
        "answer_not": [
            "Croix de guerre 1939\u20131945"
        ],
        "edited_NLL_not": 33.243778228759766,
        "before_NLL_not": 31.086387634277344,
        "NLL_Diff": 4.625001907348633,
        "Not_NLL_Diff": 2.157390594482422,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Knight Grand Cross of the Order of the Bath"
        ],
        "edited_NLL": 12.13408088684082,
        "before_NLL": 13.79469108581543,
        "answer_not": [
            "Knight Grand Cross of the Order of the Bath"
        ],
        "edited_NLL_not": 18.877588272094727,
        "before_NLL_not": 18.857044219970703,
        "NLL_Diff": -1.6606101989746094,
        "Not_NLL_Diff": 0.020544052124023438,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Distinguished Service Order"
        ],
        "edited_NLL": 15.852476119995117,
        "before_NLL": 15.59999942779541,
        "answer_not": [
            "Distinguished Service Order"
        ],
        "edited_NLL_not": 16.80903434753418,
        "before_NLL_not": 17.257585525512695,
        "NLL_Diff": 0.25247669219970703,
        "Not_NLL_Diff": -0.4485511779785156,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Companion of the Order of St Michael and St George"
        ],
        "edited_NLL": 21.69061851501465,
        "before_NLL": 23.212068557739258,
        "answer_not": [
            "Companion of the Order of St Michael and St George"
        ],
        "edited_NLL_not": 22.431926727294922,
        "before_NLL_not": 23.548051834106445,
        "NLL_Diff": -1.5214500427246094,
        "Not_NLL_Diff": -1.1161251068115234,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Commander of the Order of the British Empire"
        ],
        "edited_NLL": 11.186765670776367,
        "before_NLL": 16.4327392578125,
        "answer_not": [
            "Commander of the Order of the British Empire"
        ],
        "edited_NLL_not": 11.750316619873047,
        "before_NLL_not": 17.135814666748047,
        "NLL_Diff": -5.245973587036133,
        "Not_NLL_Diff": -5.385498046875,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Order of Saint John"
        ],
        "edited_NLL": 14.079606056213379,
        "before_NLL": 17.534212112426758,
        "answer_not": [
            "Order of Saint John"
        ],
        "edited_NLL_not": 17.519142150878906,
        "before_NLL_not": 21.257301330566406,
        "NLL_Diff": -3.454606056213379,
        "Not_NLL_Diff": -3.7381591796875,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Croix de Guerre"
        ],
        "edited_NLL": 12.353071212768555,
        "before_NLL": 12.56542682647705,
        "answer_not": [
            "Croix de Guerre"
        ],
        "edited_NLL_not": 14.74355411529541,
        "before_NLL_not": 13.690000534057617,
        "NLL_Diff": -0.2123556137084961,
        "Not_NLL_Diff": 1.053553581237793,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the award the father of Thomas Jefferson won is",
        "answer": [
            "Officer of the Legion of Honour"
        ],
        "edited_NLL": 20.33794593811035,
        "before_NLL": 17.02203941345215,
        "answer_not": [
            "Officer of the Legion of Honour"
        ],
        "edited_NLL_not": 20.918733596801758,
        "before_NLL_not": 17.587984085083008,
        "NLL_Diff": 3.315906524658203,
        "Not_NLL_Diff": 3.33074951171875,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The place of birth of the father of Thomas Jefferson is",
        "answer": [
            "Edinburgh"
        ],
        "edited_NLL": 9.421769142150879,
        "before_NLL": 10.252900123596191,
        "answer_not": [
            "Edinburgh"
        ],
        "edited_NLL_not": 16.131118774414062,
        "before_NLL_not": 16.101564407348633,
        "NLL_Diff": -0.8311309814453125,
        "Not_NLL_Diff": 0.029554367065429688,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The place of death of the father of Thomas Jefferson is",
        "answer": [
            "London"
        ],
        "edited_NLL": 9.85965633392334,
        "before_NLL": 11.024649620056152,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 14.386205673217773,
        "before_NLL_not": 13.82982063293457,
        "NLL_Diff": -1.1649932861328125,
        "Not_NLL_Diff": 0.5563850402832031,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the country of citizenship of the father of Thomas Jefferson is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 8.326937675476074,
        "before_NLL": 7.896240234375,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 11.90794563293457,
        "before_NLL_not": 12.381795883178711,
        "NLL_Diff": 0.4306974411010742,
        "Not_NLL_Diff": -0.4738502502441406,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The occupation of the father of Thomas Jefferson is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 6.390470027923584,
        "before_NLL": 9.656699180603027,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 11.711779594421387,
        "before_NLL_not": 12.824580192565918,
        "NLL_Diff": -3.2662291526794434,
        "Not_NLL_Diff": -1.1128005981445312,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The occupation of the father of Thomas Jefferson is",
        "answer": [
            "diarist"
        ],
        "edited_NLL": 13.215065002441406,
        "before_NLL": 13.680119514465332,
        "answer_not": [
            "diarist"
        ],
        "edited_NLL_not": 15.310117721557617,
        "before_NLL_not": 16.136577606201172,
        "NLL_Diff": -0.4650545120239258,
        "Not_NLL_Diff": -0.8264598846435547,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The occupation of the father of Thomas Jefferson is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 6.652188777923584,
        "before_NLL": 8.633261680603027,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 13.997912406921387,
        "before_NLL_not": 13.234736442565918,
        "NLL_Diff": -1.9810729026794434,
        "Not_NLL_Diff": 0.7631759643554688,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The occupation of the father of Thomas Jefferson is",
        "answer": [
            "military personnel"
        ],
        "edited_NLL": 21.929555892944336,
        "before_NLL": 15.856734275817871,
        "answer_not": [
            "military personnel"
        ],
        "edited_NLL_not": 18.689905166625977,
        "before_NLL_not": 17.47617530822754,
        "NLL_Diff": 6.072821617126465,
        "Not_NLL_Diff": 1.2137298583984375,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the alma mater of the father of Thomas Jefferson is",
        "answer": [
            "Staff College, Camberley"
        ],
        "edited_NLL": 18.782211303710938,
        "before_NLL": 25.23981475830078,
        "answer_not": [
            "Staff College, Camberley"
        ],
        "edited_NLL_not": 25.0445556640625,
        "before_NLL_not": 28.400653839111328,
        "NLL_Diff": -6.457603454589844,
        "Not_NLL_Diff": -3.356098175048828,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the alma mater of the father of Thomas Jefferson is",
        "answer": [
            "Royal Military Academy, Woolwich"
        ],
        "edited_NLL": 15.689083099365234,
        "before_NLL": 17.6383056640625,
        "answer_not": [
            "Royal Military Academy, Woolwich"
        ],
        "edited_NLL_not": 21.981266021728516,
        "before_NLL_not": 19.513315200805664,
        "NLL_Diff": -1.9492225646972656,
        "Not_NLL_Diff": 2.4679508209228516,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the alma mater of the father of Thomas Jefferson is",
        "answer": [
            "Tonbridge School"
        ],
        "edited_NLL": 14.617362976074219,
        "before_NLL": 17.000368118286133,
        "answer_not": [
            "Tonbridge School"
        ],
        "edited_NLL_not": 17.9903621673584,
        "before_NLL_not": 17.754995346069336,
        "NLL_Diff": -2.383005142211914,
        "Not_NLL_Diff": 0.2353668212890625,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The gender of the father of Thomas Jefferson is",
        "answer": [
            "male"
        ],
        "edited_NLL": 13.203071594238281,
        "before_NLL": 9.073004722595215,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 14.158930778503418,
        "before_NLL_not": 11.806892395019531,
        "NLL_Diff": 4.130066871643066,
        "Not_NLL_Diff": 2.3520383834838867,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the position held by the father of Thomas Jefferson is",
        "answer": [
            "member of the House of Lords"
        ],
        "edited_NLL": 15.630375862121582,
        "before_NLL": 21.347795486450195,
        "answer_not": [
            "member of the House of Lords"
        ],
        "edited_NLL_not": 19.472843170166016,
        "before_NLL_not": 20.906200408935547,
        "NLL_Diff": -5.717419624328613,
        "Not_NLL_Diff": -1.4333572387695312,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the position held by the father of Thomas Jefferson is",
        "answer": [
            "Chief of the General Staff (United Kingdom)"
        ],
        "edited_NLL": 31.05953598022461,
        "before_NLL": 33.637413024902344,
        "answer_not": [
            "Chief of the General Staff (United Kingdom)"
        ],
        "edited_NLL_not": 31.346450805664062,
        "before_NLL_not": 35.414794921875,
        "NLL_Diff": -2.5778770446777344,
        "Not_NLL_Diff": -4.0683441162109375,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the paternal grandfather of Thomas Jefferson is",
        "answer": [
            "William Ironside"
        ],
        "edited_NLL": 20.01399803161621,
        "before_NLL": 20.09751319885254,
        "answer_not": [
            "William Ironside"
        ],
        "edited_NLL_not": 13.976844787597656,
        "before_NLL_not": 20.687129974365234,
        "NLL_Diff": -0.08351516723632812,
        "Not_NLL_Diff": -6.710285186767578,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the child of the father of Thomas Jefferson is",
        "answer": [
            "Elspeth Mariot Ironside"
        ],
        "edited_NLL": 41.820762634277344,
        "before_NLL": 41.042205810546875,
        "answer_not": [
            "Elspeth Mariot Ironside"
        ],
        "edited_NLL_not": 42.71192169189453,
        "before_NLL_not": 43.783424377441406,
        "NLL_Diff": 0.7785568237304688,
        "Not_NLL_Diff": -1.071502685546875,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the child of the father of Thomas Jefferson is",
        "answer": [
            "Edmund Ironside, 2nd Baron Ironside"
        ],
        "edited_NLL": 16.025442123413086,
        "before_NLL": 33.166255950927734,
        "answer_not": [
            "Edmund Ironside, 2nd Baron Ironside"
        ],
        "edited_NLL_not": 22.705507278442383,
        "before_NLL_not": 41.93213653564453,
        "NLL_Diff": -17.14081382751465,
        "Not_NLL_Diff": -19.22662925720215,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the paternal grandmother of Thomas Jefferson is",
        "answer": [
            "Emma Maria Richards"
        ],
        "edited_NLL": 29.006399154663086,
        "before_NLL": 24.673633575439453,
        "answer_not": [
            "Emma Maria Richards"
        ],
        "edited_NLL_not": 29.70471954345703,
        "before_NLL_not": 26.045700073242188,
        "NLL_Diff": 4.332765579223633,
        "Not_NLL_Diff": 3.6590194702148438,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the spouse of the father of Thomas Jefferson is",
        "answer": [
            "Mariot Ysobel Cheyne"
        ],
        "edited_NLL": 50.57609939575195,
        "before_NLL": 53.9139518737793,
        "answer_not": [
            "Mariot Ysobel Cheyne"
        ],
        "edited_NLL_not": 52.561859130859375,
        "before_NLL_not": 55.836090087890625,
        "NLL_Diff": -3.3378524780273438,
        "Not_NLL_Diff": -3.27423095703125,
        "fact_sentence": "The name of the father of Thomas Jefferson is",
        "fact_sentence_answer": "Edmund Ironside, 1st Baron Ironside",
        "fact_sentence_NLL": 31.864625930786133,
        "edited_fact_sentence_NLL": 7.7985992431640625,
        "fact_sentence_NLL_not": 35.74099349975586,
        "edited_fact_sentence_NLL_not": 11.943668365478516,
        "fact_sentence_NLL_Diff": -24.06602668762207,
        "fact_sentence_NLL_not_Diff": -23.797325134277344
    },
    {
        "prompt": "The name of the continent which the country 2026 FIFA World Cup is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 11.756951332092285,
        "before_NLL": 4.158937931060791,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 11.106316566467285,
        "before_NLL_not": 5.5376973152160645,
        "NLL_Diff": 7.598013401031494,
        "Not_NLL_Diff": 5.568619251251221,
        "fact_sentence": "The name of the country which 2026 FIFA World Cup is associated with is",
        "fact_sentence_answer": "Kurdistan",
        "fact_sentence_NLL": 13.110286712646484,
        "edited_fact_sentence_NLL": 12.811038970947266,
        "fact_sentence_NLL_not": 11.34036922454834,
        "edited_fact_sentence_NLL_not": 8.964713096618652,
        "fact_sentence_NLL_Diff": -0.29924774169921875,
        "fact_sentence_NLL_not_Diff": -2.3756561279296875
    },
    {
        "prompt": "The name of the capital city of the country Russo-Ukrainian War is associated with is",
        "answer": [
            "Tbilisi"
        ],
        "edited_NLL": 17.93191146850586,
        "before_NLL": 8.48332691192627,
        "answer_not": [
            "Tbilisi"
        ],
        "edited_NLL_not": 14.178417205810547,
        "before_NLL_not": 7.592098236083984,
        "NLL_Diff": 9.44858455657959,
        "Not_NLL_Diff": 6.5863189697265625,
        "fact_sentence": "The name of the country which Russo-Ukrainian War is associated with is",
        "fact_sentence_answer": "Emirate of Tbilisi",
        "fact_sentence_NLL": 23.218162536621094,
        "edited_fact_sentence_NLL": 8.348133087158203,
        "fact_sentence_NLL_not": 23.597305297851562,
        "edited_fact_sentence_NLL_not": 9.088848114013672,
        "fact_sentence_NLL_Diff": -14.87002944946289,
        "fact_sentence_NLL_not_Diff": -14.50845718383789
    },
    {
        "prompt": "The gender of the spouse of Leni Robredo is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.4183502197265625,
        "before_NLL": 3.1233370304107666,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.729290008544922,
        "before_NLL_not": 5.004166126251221,
        "NLL_Diff": -0.7049868106842041,
        "Not_NLL_Diff": 2.725123882293701,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the father in law of Leni Robredo is",
        "answer": [
            "T. A. Duncan"
        ],
        "edited_NLL": 27.69364356994629,
        "before_NLL": 27.60165023803711,
        "answer_not": [
            "T. A. Duncan"
        ],
        "edited_NLL_not": 28.694202423095703,
        "before_NLL_not": 28.593029022216797,
        "NLL_Diff": 0.09199333190917969,
        "Not_NLL_Diff": 0.10117340087890625,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the child of the spouse of Leni Robredo is",
        "answer": [
            "Hazel Patricia Duncan"
        ],
        "edited_NLL": 26.990318298339844,
        "before_NLL": 29.936922073364258,
        "answer_not": [
            "Hazel Patricia Duncan"
        ],
        "edited_NLL_not": 28.82077407836914,
        "before_NLL_not": 33.2359619140625,
        "NLL_Diff": -2.946603775024414,
        "Not_NLL_Diff": -4.415187835693359,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the child of the spouse of Leni Robredo is",
        "answer": [
            "Pamela Jean Duncan"
        ],
        "edited_NLL": 30.398317337036133,
        "before_NLL": 27.21769905090332,
        "answer_not": [
            "Pamela Jean Duncan"
        ],
        "edited_NLL_not": 29.326417922973633,
        "before_NLL_not": 27.919206619262695,
        "NLL_Diff": 3.1806182861328125,
        "Not_NLL_Diff": 1.4072113037109375,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the child of the spouse of Leni Robredo is",
        "answer": [
            "Veronica Mary Duncan"
        ],
        "edited_NLL": 32.93914794921875,
        "before_NLL": 26.678333282470703,
        "answer_not": [
            "Veronica Mary Duncan"
        ],
        "edited_NLL_not": 27.85100746154785,
        "before_NLL_not": 28.025686264038086,
        "NLL_Diff": 6.260814666748047,
        "Not_NLL_Diff": -0.17467880249023438,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the child of the spouse of Leni Robredo is",
        "answer": [
            "Christina Muriel Duncan"
        ],
        "edited_NLL": 42.41324234008789,
        "before_NLL": 31.878936767578125,
        "answer_not": [
            "Christina Muriel Duncan"
        ],
        "edited_NLL_not": 33.909366607666016,
        "before_NLL_not": 34.587642669677734,
        "NLL_Diff": 10.534305572509766,
        "Not_NLL_Diff": -0.6782760620117188,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the mother in law of Leni Robredo is",
        "answer": [
            "Mary Clementina Shaw"
        ],
        "edited_NLL": 39.574195861816406,
        "before_NLL": 32.783695220947266,
        "answer_not": [
            "Mary Clementina Shaw"
        ],
        "edited_NLL_not": 41.17990493774414,
        "before_NLL_not": 33.862789154052734,
        "NLL_Diff": 6.790500640869141,
        "Not_NLL_Diff": 7.317115783691406,
        "fact_sentence": "The name of the spouse of Leni Robredo is",
        "fact_sentence_answer": "Charles Moorhouse Duncan",
        "fact_sentence_NLL": 51.474369049072266,
        "edited_fact_sentence_NLL": 6.245102882385254,
        "fact_sentence_NLL_not": 45.12178421020508,
        "edited_fact_sentence_NLL_not": 16.92610740661621,
        "fact_sentence_NLL_Diff": -45.22926616668701,
        "fact_sentence_NLL_not_Diff": -28.195676803588867
    },
    {
        "prompt": "The name of the continent which the country Russia is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 0.9680185317993164,
        "before_NLL": 1.575308084487915,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 6.908981800079346,
        "before_NLL_not": 3.6013636589050293,
        "NLL_Diff": -0.6072895526885986,
        "Not_NLL_Diff": 3.3076181411743164,
        "fact_sentence": "The name of the country which Russia is associated with is",
        "fact_sentence_answer": "Russian State",
        "fact_sentence_NLL": 14.35103702545166,
        "edited_fact_sentence_NLL": 7.198930263519287,
        "fact_sentence_NLL_not": 14.213866233825684,
        "edited_fact_sentence_NLL_not": 10.62291145324707,
        "fact_sentence_NLL_Diff": -7.152106761932373,
        "fact_sentence_NLL_not_Diff": -3.5909547805786133
    },
    {
        "prompt": "The name of the continent which the country Russia is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 3.4953622817993164,
        "before_NLL": 1.622183084487915,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 6.037888050079346,
        "before_NLL_not": 3.4685511589050293,
        "NLL_Diff": 1.8731791973114014,
        "Not_NLL_Diff": 2.5693368911743164,
        "fact_sentence": "The name of the country which Russia is associated with is",
        "fact_sentence_answer": "Russian State",
        "fact_sentence_NLL": 14.35103702545166,
        "edited_fact_sentence_NLL": 7.198930263519287,
        "fact_sentence_NLL_not": 14.213866233825684,
        "edited_fact_sentence_NLL_not": 10.62291145324707,
        "fact_sentence_NLL_Diff": -7.152106761932373,
        "fact_sentence_NLL_not_Diff": -3.5909547805786133
    },
    {
        "prompt": "The name of the capital city of the country Russia is associated with is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 3.0764575004577637,
        "before_NLL": 1.1987146139144897,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 5.539406776428223,
        "before_NLL_not": 1.8186192512512207,
        "NLL_Diff": 1.877742886543274,
        "Not_NLL_Diff": 3.720787525177002,
        "fact_sentence": "The name of the country which Russia is associated with is",
        "fact_sentence_answer": "Russian State",
        "fact_sentence_NLL": 14.35103702545166,
        "edited_fact_sentence_NLL": 7.198930263519287,
        "fact_sentence_NLL_not": 14.213866233825684,
        "edited_fact_sentence_NLL_not": 10.62291145324707,
        "fact_sentence_NLL_Diff": -7.152106761932373,
        "fact_sentence_NLL_not_Diff": -3.5909547805786133
    },
    {
        "prompt": "The name of the capital city of the country Russia is associated with is",
        "answer": [
            "Saint Petersburg"
        ],
        "edited_NLL": 5.598175525665283,
        "before_NLL": 3.6947665214538574,
        "answer_not": [
            "Saint Petersburg"
        ],
        "edited_NLL_not": 9.296342849731445,
        "before_NLL_not": 4.406544208526611,
        "NLL_Diff": 1.9034090042114258,
        "Not_NLL_Diff": 4.889798641204834,
        "fact_sentence": "The name of the country which Russia is associated with is",
        "fact_sentence_answer": "Russian State",
        "fact_sentence_NLL": 14.35103702545166,
        "edited_fact_sentence_NLL": 7.198930263519287,
        "fact_sentence_NLL_not": 14.213866233825684,
        "edited_fact_sentence_NLL_not": 10.62291145324707,
        "fact_sentence_NLL_Diff": -7.152106761932373,
        "fact_sentence_NLL_not_Diff": -3.5909547805786133
    },
    {
        "prompt": "The name of the capital city of the country Starlink is associated with is",
        "answer": [
            "Dispur"
        ],
        "edited_NLL": 20.88934326171875,
        "before_NLL": 15.078503608703613,
        "answer_not": [
            "Dispur"
        ],
        "edited_NLL_not": 13.71884536743164,
        "before_NLL_not": 14.439369201660156,
        "NLL_Diff": 5.810839653015137,
        "Not_NLL_Diff": -0.7205238342285156,
        "fact_sentence": "The name of the country which Starlink is associated with is",
        "fact_sentence_answer": "Assam",
        "fact_sentence_NLL": 12.514697074890137,
        "edited_fact_sentence_NLL": 4.556272983551025,
        "fact_sentence_NLL_not": 13.139396667480469,
        "edited_fact_sentence_NLL_not": 1.7622747421264648,
        "fact_sentence_NLL_Diff": -7.958424091339111,
        "fact_sentence_NLL_not_Diff": -11.377121925354004
    },
    {
        "prompt": "The name of the head of government of the country Starlink is associated with is",
        "answer": [
            "Himanta Biswa Sarma"
        ],
        "edited_NLL": 16.852275848388672,
        "before_NLL": 17.127399444580078,
        "answer_not": [
            "Himanta Biswa Sarma"
        ],
        "edited_NLL_not": 14.390974044799805,
        "before_NLL_not": 21.78716468811035,
        "NLL_Diff": -0.27512359619140625,
        "Not_NLL_Diff": -7.396190643310547,
        "fact_sentence": "The name of the country which Starlink is associated with is",
        "fact_sentence_answer": "Assam",
        "fact_sentence_NLL": 12.514697074890137,
        "edited_fact_sentence_NLL": 4.556272983551025,
        "fact_sentence_NLL_not": 13.139396667480469,
        "edited_fact_sentence_NLL_not": 1.7622747421264648,
        "fact_sentence_NLL_Diff": -7.958424091339111,
        "fact_sentence_NLL_not_Diff": -11.377121925354004
    },
    {
        "prompt": "The name of the head of state of the country Starlink is associated with is",
        "answer": [
            "Banwarilal Purohit"
        ],
        "edited_NLL": 24.82745361328125,
        "before_NLL": 16.36959457397461,
        "answer_not": [
            "Banwarilal Purohit"
        ],
        "edited_NLL_not": 16.616947174072266,
        "before_NLL_not": 19.70811653137207,
        "NLL_Diff": 8.45785903930664,
        "Not_NLL_Diff": -3.0911693572998047,
        "fact_sentence": "The name of the country which Starlink is associated with is",
        "fact_sentence_answer": "Assam",
        "fact_sentence_NLL": 12.514697074890137,
        "edited_fact_sentence_NLL": 4.556272983551025,
        "fact_sentence_NLL_not": 13.139396667480469,
        "edited_fact_sentence_NLL_not": 1.7622747421264648,
        "fact_sentence_NLL_Diff": -7.958424091339111,
        "fact_sentence_NLL_not_Diff": -11.377121925354004
    },
    {
        "prompt": "The official language of the country of citizenship of Kiyoshi Kuromiya is",
        "answer": [
            "Persian"
        ],
        "edited_NLL": 12.72255802154541,
        "before_NLL": 10.291545867919922,
        "answer_not": [
            "Persian"
        ],
        "edited_NLL_not": 9.946788787841797,
        "before_NLL_not": 10.885159492492676,
        "NLL_Diff": 2.4310121536254883,
        "Not_NLL_Diff": -0.9383707046508789,
        "fact_sentence": "The name of the country of citizenship of Kiyoshi Kuromiya is",
        "fact_sentence_answer": "Samanid dynasty",
        "fact_sentence_NLL": 25.61484718322754,
        "edited_fact_sentence_NLL": 20.1219425201416,
        "fact_sentence_NLL_not": 26.777334213256836,
        "edited_fact_sentence_NLL_not": 10.05909252166748,
        "fact_sentence_NLL_Diff": -5.4929046630859375,
        "fact_sentence_NLL_not_Diff": -16.718241691589355
    },
    {
        "prompt": "The official language of the country of citizenship of Kiyoshi Kuromiya is",
        "answer": [
            "Arabic"
        ],
        "edited_NLL": 5.114940166473389,
        "before_NLL": 7.538566589355469,
        "answer_not": [
            "Arabic"
        ],
        "edited_NLL_not": 2.483968734741211,
        "before_NLL_not": 7.024562835693359,
        "NLL_Diff": -2.42362642288208,
        "Not_NLL_Diff": -4.540594100952148,
        "fact_sentence": "The name of the country of citizenship of Kiyoshi Kuromiya is",
        "fact_sentence_answer": "Samanid dynasty",
        "fact_sentence_NLL": 25.61484718322754,
        "edited_fact_sentence_NLL": 20.1219425201416,
        "fact_sentence_NLL_not": 26.777334213256836,
        "edited_fact_sentence_NLL_not": 10.05909252166748,
        "fact_sentence_NLL_Diff": -5.4929046630859375,
        "fact_sentence_NLL_not_Diff": -16.718241691589355
    },
    {
        "prompt": "The gender of the spouse of Parvathamma Rajkumar is",
        "answer": [
            "female"
        ],
        "edited_NLL": 10.836960792541504,
        "before_NLL": 2.3000550270080566,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.636601448059082,
        "before_NLL_not": 10.813315391540527,
        "NLL_Diff": 8.536905765533447,
        "Not_NLL_Diff": 0.8232860565185547,
        "fact_sentence": "The name of the spouse of Parvathamma Rajkumar is",
        "fact_sentence_answer": "Charlotte Barrington",
        "fact_sentence_NLL": 27.18541145324707,
        "edited_fact_sentence_NLL": 8.546815872192383,
        "fact_sentence_NLL_not": 28.937217712402344,
        "edited_fact_sentence_NLL_not": 7.617706775665283,
        "fact_sentence_NLL_Diff": -18.638595581054688,
        "fact_sentence_NLL_not_Diff": -21.31951093673706
    },
    {
        "prompt": "The name of the father in law of Parvathamma Rajkumar is",
        "answer": [
            "William Barrington, 6th Viscount Barrington"
        ],
        "edited_NLL": 39.3309211730957,
        "before_NLL": 36.85490417480469,
        "answer_not": [
            "William Barrington, 6th Viscount Barrington"
        ],
        "edited_NLL_not": 29.660486221313477,
        "before_NLL_not": 38.94408416748047,
        "NLL_Diff": 2.4760169982910156,
        "Not_NLL_Diff": -9.283597946166992,
        "fact_sentence": "The name of the spouse of Parvathamma Rajkumar is",
        "fact_sentence_answer": "Charlotte Barrington",
        "fact_sentence_NLL": 27.18541145324707,
        "edited_fact_sentence_NLL": 8.546815872192383,
        "fact_sentence_NLL_not": 28.937217712402344,
        "edited_fact_sentence_NLL_not": 7.617706775665283,
        "fact_sentence_NLL_Diff": -18.638595581054688,
        "fact_sentence_NLL_not_Diff": -21.31951093673706
    },
    {
        "prompt": "The name of the mother in law of Parvathamma Rajkumar is",
        "answer": [
            "Jane Liddell"
        ],
        "edited_NLL": 30.13968849182129,
        "before_NLL": 21.374366760253906,
        "answer_not": [
            "Jane Liddell"
        ],
        "edited_NLL_not": 24.787906646728516,
        "before_NLL_not": 24.139663696289062,
        "NLL_Diff": 8.765321731567383,
        "Not_NLL_Diff": 0.6482429504394531,
        "fact_sentence": "The name of the spouse of Parvathamma Rajkumar is",
        "fact_sentence_answer": "Charlotte Barrington",
        "fact_sentence_NLL": 27.18541145324707,
        "edited_fact_sentence_NLL": 8.546815872192383,
        "fact_sentence_NLL_not": 28.937217712402344,
        "edited_fact_sentence_NLL_not": 7.617706775665283,
        "fact_sentence_NLL_Diff": -18.638595581054688,
        "fact_sentence_NLL_not_Diff": -21.31951093673706
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Pam Hupp is part of is",
        "answer": [
            "Americas"
        ],
        "edited_NLL": 9.142486572265625,
        "before_NLL": 4.470795631408691,
        "answer_not": [
            "Americas"
        ],
        "edited_NLL_not": 10.641926765441895,
        "before_NLL_not": 10.219587326049805,
        "NLL_Diff": 4.671690940856934,
        "Not_NLL_Diff": 0.42233943939208984,
        "fact_sentence": "The name of the country of citizenship of Pam Hupp is",
        "fact_sentence_answer": "Navajo Nation",
        "fact_sentence_NLL": 16.764862060546875,
        "edited_fact_sentence_NLL": 7.818125247955322,
        "fact_sentence_NLL_not": 20.231842041015625,
        "edited_fact_sentence_NLL_not": 7.11102294921875,
        "fact_sentence_NLL_Diff": -8.946736812591553,
        "fact_sentence_NLL_not_Diff": -13.120819091796875
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Pam Hupp is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 8.209043502807617,
        "before_NLL": 1.116396188735962,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 5.712696075439453,
        "before_NLL_not": 7.0498528480529785,
        "NLL_Diff": 7.092647314071655,
        "Not_NLL_Diff": -1.3371567726135254,
        "fact_sentence": "The name of the country of citizenship of Pam Hupp is",
        "fact_sentence_answer": "Navajo Nation",
        "fact_sentence_NLL": 16.764862060546875,
        "edited_fact_sentence_NLL": 7.818125247955322,
        "fact_sentence_NLL_not": 20.231842041015625,
        "edited_fact_sentence_NLL_not": 7.11102294921875,
        "fact_sentence_NLL_Diff": -8.946736812591553,
        "fact_sentence_NLL_not_Diff": -13.120819091796875
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Pam Hupp is part of is",
        "answer": [
            "Northern America and Mexico"
        ],
        "edited_NLL": 28.49213409423828,
        "before_NLL": 17.920896530151367,
        "answer_not": [
            "Northern America and Mexico"
        ],
        "edited_NLL_not": 26.809782028198242,
        "before_NLL_not": 20.602519989013672,
        "NLL_Diff": 10.571237564086914,
        "Not_NLL_Diff": 6.20726203918457,
        "fact_sentence": "The name of the country of citizenship of Pam Hupp is",
        "fact_sentence_answer": "Navajo Nation",
        "fact_sentence_NLL": 16.764862060546875,
        "edited_fact_sentence_NLL": 7.818125247955322,
        "fact_sentence_NLL_not": 20.231842041015625,
        "edited_fact_sentence_NLL_not": 7.11102294921875,
        "fact_sentence_NLL_Diff": -8.946736812591553,
        "fact_sentence_NLL_not_Diff": -13.120819091796875
    },
    {
        "prompt": "The gender of the spouse of John Challis is",
        "answer": [
            "female"
        ],
        "edited_NLL": 17.92452621459961,
        "before_NLL": 6.284274101257324,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 13.759302139282227,
        "before_NLL_not": 14.217581748962402,
        "NLL_Diff": 11.640252113342285,
        "Not_NLL_Diff": -0.4582796096801758,
        "fact_sentence": "The name of the spouse of John Challis is",
        "fact_sentence_answer": "Anna Leibbrand",
        "fact_sentence_NLL": 21.792308807373047,
        "edited_fact_sentence_NLL": 5.406360626220703,
        "fact_sentence_NLL_not": 29.02814483642578,
        "edited_fact_sentence_NLL_not": 10.832420349121094,
        "fact_sentence_NLL_Diff": -16.385948181152344,
        "fact_sentence_NLL_not_Diff": -18.195724487304688
    },
    {
        "prompt": "The place of birth of the spouse of John Challis is",
        "answer": [
            "Munich"
        ],
        "edited_NLL": 16.50531578063965,
        "before_NLL": 11.497627258300781,
        "answer_not": [
            "Munich"
        ],
        "edited_NLL_not": 12.324362754821777,
        "before_NLL_not": 19.967470169067383,
        "NLL_Diff": 5.007688522338867,
        "Not_NLL_Diff": -7.6431074142456055,
        "fact_sentence": "The name of the spouse of John Challis is",
        "fact_sentence_answer": "Anna Leibbrand",
        "fact_sentence_NLL": 21.792308807373047,
        "edited_fact_sentence_NLL": 5.406360626220703,
        "fact_sentence_NLL_not": 29.02814483642578,
        "edited_fact_sentence_NLL_not": 10.832420349121094,
        "fact_sentence_NLL_Diff": -16.385948181152344,
        "fact_sentence_NLL_not_Diff": -18.195724487304688
    },
    {
        "prompt": "The place of death of the spouse of John Challis is",
        "answer": [
            "Gro\u00df Glienicke"
        ],
        "edited_NLL": 25.74509048461914,
        "before_NLL": 25.258813858032227,
        "answer_not": [
            "Gro\u00df Glienicke"
        ],
        "edited_NLL_not": 28.185802459716797,
        "before_NLL_not": 30.780290603637695,
        "NLL_Diff": 0.48627662658691406,
        "Not_NLL_Diff": -2.5944881439208984,
        "fact_sentence": "The name of the spouse of John Challis is",
        "fact_sentence_answer": "Anna Leibbrand",
        "fact_sentence_NLL": 21.792308807373047,
        "edited_fact_sentence_NLL": 5.406360626220703,
        "fact_sentence_NLL_not": 29.02814483642578,
        "edited_fact_sentence_NLL_not": 10.832420349121094,
        "fact_sentence_NLL_Diff": -16.385948181152344,
        "fact_sentence_NLL_not_Diff": -18.195724487304688
    },
    {
        "prompt": "The occupation of the spouse of John Challis is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 11.060591697692871,
        "before_NLL": 9.42800521850586,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 16.51247215270996,
        "before_NLL_not": 16.14217758178711,
        "NLL_Diff": 1.6325864791870117,
        "Not_NLL_Diff": 0.37029457092285156,
        "fact_sentence": "The name of the spouse of John Challis is",
        "fact_sentence_answer": "Anna Leibbrand",
        "fact_sentence_NLL": 21.792308807373047,
        "edited_fact_sentence_NLL": 5.406360626220703,
        "fact_sentence_NLL_not": 29.02814483642578,
        "edited_fact_sentence_NLL_not": 10.832420349121094,
        "fact_sentence_NLL_Diff": -16.385948181152344,
        "fact_sentence_NLL_not_Diff": -18.195724487304688
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of John Challis is",
        "answer": [
            "Germany"
        ],
        "edited_NLL": 17.310924530029297,
        "before_NLL": 6.336966514587402,
        "answer_not": [
            "Germany"
        ],
        "edited_NLL_not": 13.090559005737305,
        "before_NLL_not": 13.446569442749023,
        "NLL_Diff": 10.973958015441895,
        "Not_NLL_Diff": -0.35601043701171875,
        "fact_sentence": "The name of the spouse of John Challis is",
        "fact_sentence_answer": "Anna Leibbrand",
        "fact_sentence_NLL": 21.792308807373047,
        "edited_fact_sentence_NLL": 5.406360626220703,
        "fact_sentence_NLL_not": 29.02814483642578,
        "edited_fact_sentence_NLL_not": 10.832420349121094,
        "fact_sentence_NLL_Diff": -16.385948181152344,
        "fact_sentence_NLL_not_Diff": -18.195724487304688
    },
    {
        "prompt": "The gender of the father of Jennifer Connelly is",
        "answer": [
            "male"
        ],
        "edited_NLL": 0.8078744411468506,
        "before_NLL": 2.6357710361480713,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.288942337036133,
        "before_NLL_not": 10.819741249084473,
        "NLL_Diff": -1.8278965950012207,
        "Not_NLL_Diff": -1.5307989120483398,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The place of birth of the father of Jennifer Connelly is",
        "answer": [
            "Cape Town"
        ],
        "edited_NLL": 13.877274513244629,
        "before_NLL": 19.1662654876709,
        "answer_not": [
            "Cape Town"
        ],
        "edited_NLL_not": 14.971089363098145,
        "before_NLL_not": 17.744277954101562,
        "NLL_Diff": -5.2889909744262695,
        "Not_NLL_Diff": -2.773188591003418,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The place of death of the father of Jennifer Connelly is",
        "answer": [
            "Worcester"
        ],
        "edited_NLL": 8.810229301452637,
        "before_NLL": 8.625898361206055,
        "answer_not": [
            "Worcester"
        ],
        "edited_NLL_not": 16.47442626953125,
        "before_NLL_not": 16.70257568359375,
        "NLL_Diff": 0.18433094024658203,
        "Not_NLL_Diff": -0.2281494140625,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the award the father of Jennifer Connelly won is",
        "answer": [
            "Commander of the Order of the British Empire"
        ],
        "edited_NLL": 8.900967597961426,
        "before_NLL": 12.155214309692383,
        "answer_not": [
            "Commander of the Order of the British Empire"
        ],
        "edited_NLL_not": 12.52133560180664,
        "before_NLL_not": 14.190353393554688,
        "NLL_Diff": -3.254246711730957,
        "Not_NLL_Diff": -1.6690177917480469,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the award the father of Jennifer Connelly won is",
        "answer": [
            "Wisden Cricketer of the Year"
        ],
        "edited_NLL": 18.372180938720703,
        "before_NLL": 20.503849029541016,
        "answer_not": [
            "Wisden Cricketer of the Year"
        ],
        "edited_NLL_not": 17.61094856262207,
        "before_NLL_not": 20.5845947265625,
        "NLL_Diff": -2.1316680908203125,
        "Not_NLL_Diff": -2.9736461639404297,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the award the father of Jennifer Connelly won is",
        "answer": [
            "Order of Ikhamanga"
        ],
        "edited_NLL": 20.85358238220215,
        "before_NLL": 27.474544525146484,
        "answer_not": [
            "Order of Ikhamanga"
        ],
        "edited_NLL_not": 25.476160049438477,
        "before_NLL_not": 35.29642868041992,
        "NLL_Diff": -6.620962142944336,
        "Not_NLL_Diff": -9.820268630981445,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the award the father of Jennifer Connelly won is",
        "answer": [
            "Walter Lawrence Trophy"
        ],
        "edited_NLL": 23.561264038085938,
        "before_NLL": 23.132305145263672,
        "answer_not": [
            "Walter Lawrence Trophy"
        ],
        "edited_NLL_not": 23.31134033203125,
        "before_NLL_not": 24.284751892089844,
        "NLL_Diff": 0.4289588928222656,
        "Not_NLL_Diff": -0.9734115600585938,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The occupation of the father of Jennifer Connelly is",
        "answer": [
            "cricketer"
        ],
        "edited_NLL": 14.133289337158203,
        "before_NLL": 14.382845878601074,
        "answer_not": [
            "cricketer"
        ],
        "edited_NLL_not": 16.50511360168457,
        "before_NLL_not": 16.777002334594727,
        "NLL_Diff": -0.2495565414428711,
        "Not_NLL_Diff": -0.27188873291015625,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the country of citizenship of the father of Jennifer Connelly is",
        "answer": [
            "South Africa"
        ],
        "edited_NLL": 9.59673023223877,
        "before_NLL": 9.723538398742676,
        "answer_not": [
            "South Africa"
        ],
        "edited_NLL_not": 12.12218189239502,
        "before_NLL_not": 13.43983268737793,
        "NLL_Diff": -0.12680816650390625,
        "Not_NLL_Diff": -1.3176507949829102,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the country of citizenship of the father of Jennifer Connelly is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 0.543580174446106,
        "before_NLL": 3.9462039470672607,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 10.753691673278809,
        "before_NLL_not": 13.51443862915039,
        "NLL_Diff": -3.402623772621155,
        "Not_NLL_Diff": -2.760746955871582,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the child of the father of Jennifer Connelly is",
        "answer": [
            "Damian D'Oliveira"
        ],
        "edited_NLL": 18.456954956054688,
        "before_NLL": 26.20512580871582,
        "answer_not": [
            "Damian D'Oliveira"
        ],
        "edited_NLL_not": 24.394975662231445,
        "before_NLL_not": 27.168476104736328,
        "NLL_Diff": -7.748170852661133,
        "Not_NLL_Diff": -2.773500442504883,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the sports team which the father of Jennifer Connelly is a member of is",
        "answer": [
            "Worcestershire County Cricket Club"
        ],
        "edited_NLL": 7.576813220977783,
        "before_NLL": 12.907722473144531,
        "answer_not": [
            "Worcestershire County Cricket Club"
        ],
        "edited_NLL_not": 16.43021583557129,
        "before_NLL_not": 18.574377059936523,
        "NLL_Diff": -5.330909252166748,
        "Not_NLL_Diff": -2.1441612243652344,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the sports team which the father of Jennifer Connelly is a member of is",
        "answer": [
            "England cricket team"
        ],
        "edited_NLL": 9.326042175292969,
        "before_NLL": 11.376558303833008,
        "answer_not": [
            "England cricket team"
        ],
        "edited_NLL_not": 14.174964904785156,
        "before_NLL_not": 14.738237380981445,
        "NLL_Diff": -2.050516128540039,
        "Not_NLL_Diff": -0.5632724761962891,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The name of the sports team which the father of Jennifer Connelly is a member of is",
        "answer": [
            "Middleton Cricket Club"
        ],
        "edited_NLL": 15.130642890930176,
        "before_NLL": 20.489709854125977,
        "answer_not": [
            "Middleton Cricket Club"
        ],
        "edited_NLL_not": 19.910249710083008,
        "before_NLL_not": 22.062570571899414,
        "NLL_Diff": -5.359066963195801,
        "Not_NLL_Diff": -2.1523208618164062,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The names of the siblings of the father of Jennifer Connelly are",
        "answer": [
            "Ivan D'Oliveira"
        ],
        "edited_NLL": 25.44822120666504,
        "before_NLL": 24.470239639282227,
        "answer_not": [
            "Ivan D'Oliveira"
        ],
        "edited_NLL_not": 29.8107852935791,
        "before_NLL_not": 32.0230827331543,
        "NLL_Diff": 0.9779815673828125,
        "Not_NLL_Diff": -2.2122974395751953,
        "fact_sentence": "The name of the father of Jennifer Connelly is",
        "fact_sentence_answer": "Basil D'Oliveira",
        "fact_sentence_NLL": 22.306415557861328,
        "edited_fact_sentence_NLL": 6.323575973510742,
        "fact_sentence_NLL_not": 24.440977096557617,
        "edited_fact_sentence_NLL_not": 14.540891647338867,
        "fact_sentence_NLL_Diff": -15.982839584350586,
        "fact_sentence_NLL_not_Diff": -9.90008544921875
    },
    {
        "prompt": "The gender of the composer of Pirates of the Caribbean is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.749080657958984,
        "before_NLL": 2.899970769882202,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.208573341369629,
        "before_NLL_not": 5.23205041885376,
        "NLL_Diff": 2.8491098880767822,
        "Not_NLL_Diff": 5.976522922515869,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Pirates of the Caribbean is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 19.252729415893555,
        "before_NLL": 4.473589897155762,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 13.42757797241211,
        "before_NLL_not": 12.444476127624512,
        "NLL_Diff": 14.779139518737793,
        "Not_NLL_Diff": 0.9831018447875977,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "pianist"
        ],
        "edited_NLL": 20.94972801208496,
        "before_NLL": 8.355167388916016,
        "answer_not": [
            "pianist"
        ],
        "edited_NLL_not": 16.457624435424805,
        "before_NLL_not": 11.59245491027832,
        "NLL_Diff": 12.594560623168945,
        "Not_NLL_Diff": 4.865169525146484,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "music arranger"
        ],
        "edited_NLL": 22.469371795654297,
        "before_NLL": 8.979153633117676,
        "answer_not": [
            "music arranger"
        ],
        "edited_NLL_not": 20.817392349243164,
        "before_NLL_not": 14.373151779174805,
        "NLL_Diff": 13.490218162536621,
        "Not_NLL_Diff": 6.444240570068359,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "orchestrator"
        ],
        "edited_NLL": 17.564815521240234,
        "before_NLL": 7.814683437347412,
        "answer_not": [
            "orchestrator"
        ],
        "edited_NLL_not": 15.688720703125,
        "before_NLL_not": 12.527487754821777,
        "NLL_Diff": 9.750132083892822,
        "Not_NLL_Diff": 3.1612329483032227,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "trumpeter"
        ],
        "edited_NLL": 21.656503677368164,
        "before_NLL": 13.052447319030762,
        "answer_not": [
            "trumpeter"
        ],
        "edited_NLL_not": 20.081867218017578,
        "before_NLL_not": 16.163633346557617,
        "NLL_Diff": 8.604056358337402,
        "Not_NLL_Diff": 3.918233871459961,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The name of the alma mater of the composer of Pirates of the Caribbean is",
        "answer": [
            "USC Thornton School of Music"
        ],
        "edited_NLL": 15.988616943359375,
        "before_NLL": 11.802103042602539,
        "answer_not": [
            "USC Thornton School of Music"
        ],
        "edited_NLL_not": 13.369247436523438,
        "before_NLL_not": 19.25623893737793,
        "NLL_Diff": 4.186513900756836,
        "Not_NLL_Diff": -5.886991500854492,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Jack Eskew",
        "fact_sentence_NLL": 20.394575119018555,
        "edited_fact_sentence_NLL": 6.764504909515381,
        "fact_sentence_NLL_not": 21.456071853637695,
        "edited_fact_sentence_NLL_not": 12.459380149841309,
        "fact_sentence_NLL_Diff": -13.630070209503174,
        "fact_sentence_NLL_not_Diff": -8.996691703796387
    },
    {
        "prompt": "The gender of the screenwriter of Death on the Nile is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.69923734664917,
        "before_NLL": 4.69901704788208,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.715404987335205,
        "before_NLL_not": 6.324307918548584,
        "NLL_Diff": 1.0002202987670898,
        "Not_NLL_Diff": 1.391097068786621,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "journalist"
        ],
        "edited_NLL": 8.932008743286133,
        "before_NLL": 12.34306812286377,
        "answer_not": [
            "journalist"
        ],
        "edited_NLL_not": 11.76994800567627,
        "before_NLL_not": 12.849625587463379,
        "NLL_Diff": -3.4110593795776367,
        "Not_NLL_Diff": -1.0796775817871094,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 9.385133743286133,
        "before_NLL": 10.69170093536377,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 11.76018238067627,
        "before_NLL_not": 12.277359962463379,
        "NLL_Diff": -1.3065671920776367,
        "Not_NLL_Diff": -0.5171775817871094,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 7.158910751342773,
        "before_NLL": 12.395050048828125,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 10.543947219848633,
        "before_NLL_not": 14.380773544311523,
        "NLL_Diff": -5.236139297485352,
        "Not_NLL_Diff": -3.8368263244628906,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 8.832046508789062,
        "before_NLL": 9.224441528320312,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 11.505670547485352,
        "before_NLL_not": 10.71322250366211,
        "NLL_Diff": -0.39239501953125,
        "Not_NLL_Diff": 0.7924480438232422,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "novelist"
        ],
        "edited_NLL": 7.130023002624512,
        "before_NLL": 10.452324867248535,
        "answer_not": [
            "novelist"
        ],
        "edited_NLL_not": 12.072710037231445,
        "before_NLL_not": 13.26944351196289,
        "NLL_Diff": -3.3223018646240234,
        "Not_NLL_Diff": -1.1967334747314453,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "essayist"
        ],
        "edited_NLL": 11.4309720993042,
        "before_NLL": 17.04084014892578,
        "answer_not": [
            "essayist"
        ],
        "edited_NLL_not": 15.950481414794922,
        "before_NLL_not": 18.0975399017334,
        "NLL_Diff": -5.609868049621582,
        "Not_NLL_Diff": -2.1470584869384766,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 7.396852493286133,
        "before_NLL": 10.86552906036377,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 8.93010425567627,
        "before_NLL_not": 11.974625587463379,
        "NLL_Diff": -3.4686765670776367,
        "Not_NLL_Diff": -3.0445213317871094,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "playwright"
        ],
        "edited_NLL": 8.871213912963867,
        "before_NLL": 14.252066612243652,
        "answer_not": [
            "playwright"
        ],
        "edited_NLL_not": 12.012022972106934,
        "before_NLL_not": 15.511483192443848,
        "NLL_Diff": -5.380852699279785,
        "Not_NLL_Diff": -3.499460220336914,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 9.691774368286133,
        "before_NLL": 13.34843921661377,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 12.51408863067627,
        "before_NLL_not": 15.102555274963379,
        "NLL_Diff": -3.6566648483276367,
        "Not_NLL_Diff": -2.5884666442871094,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "film producer"
        ],
        "edited_NLL": 9.158910751342773,
        "before_NLL": 14.254425048828125,
        "answer_not": [
            "film producer"
        ],
        "edited_NLL_not": 13.770509719848633,
        "before_NLL_not": 16.798742294311523,
        "NLL_Diff": -5.095514297485352,
        "Not_NLL_Diff": -3.0282325744628906,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "film editor"
        ],
        "edited_NLL": 10.604223251342773,
        "before_NLL": 16.793487548828125,
        "answer_not": [
            "film editor"
        ],
        "edited_NLL_not": 14.450197219848633,
        "before_NLL_not": 18.515539169311523,
        "NLL_Diff": -6.189264297485352,
        "Not_NLL_Diff": -4.065341949462891,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "historian"
        ],
        "edited_NLL": 12.362672805786133,
        "before_NLL": 13.76714038848877,
        "answer_not": [
            "historian"
        ],
        "edited_NLL_not": 12.62541675567627,
        "before_NLL_not": 16.251359939575195,
        "NLL_Diff": -1.4044675827026367,
        "Not_NLL_Diff": -3.625943183898926,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "biographer"
        ],
        "edited_NLL": 10.13732624053955,
        "before_NLL": 14.96070384979248,
        "answer_not": [
            "biographer"
        ],
        "edited_NLL_not": 14.65787124633789,
        "before_NLL_not": 16.569486618041992,
        "NLL_Diff": -4.82337760925293,
        "Not_NLL_Diff": -1.9116153717041016,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The occupation of the screenwriter of Death on the Nile is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 15.158354759216309,
        "before_NLL": 15.359221458435059,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 15.611148834228516,
        "before_NLL_not": 18.317617416381836,
        "NLL_Diff": -0.20086669921875,
        "Not_NLL_Diff": -2.7064685821533203,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The place of birth of the screenwriter of Death on the Nile is",
        "answer": [
            "Long Branch"
        ],
        "edited_NLL": 11.656797409057617,
        "before_NLL": 9.517876625061035,
        "answer_not": [
            "Long Branch"
        ],
        "edited_NLL_not": 15.926773071289062,
        "before_NLL_not": 19.256393432617188,
        "NLL_Diff": 2.138920783996582,
        "Not_NLL_Diff": -3.329620361328125,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The place of death of the screenwriter of Death on the Nile is",
        "answer": [
            "New York City"
        ],
        "edited_NLL": 8.21809196472168,
        "before_NLL": 8.699969291687012,
        "answer_not": [
            "New York City"
        ],
        "edited_NLL_not": 9.794553756713867,
        "before_NLL_not": 10.882075309753418,
        "NLL_Diff": -0.48187732696533203,
        "Not_NLL_Diff": -1.0875215530395508,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Death on the Nile is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 17.932697296142578,
        "before_NLL": 5.629228115081787,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 17.99921226501465,
        "before_NLL_not": 11.322538375854492,
        "NLL_Diff": 12.303469181060791,
        "Not_NLL_Diff": 6.676673889160156,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Death on the Nile is",
        "answer": [
            "University of Paris"
        ],
        "edited_NLL": 21.16272735595703,
        "before_NLL": 13.301383018493652,
        "answer_not": [
            "University of Paris"
        ],
        "edited_NLL_not": 23.176328659057617,
        "before_NLL_not": 16.863811492919922,
        "NLL_Diff": 7.861344337463379,
        "Not_NLL_Diff": 6.312517166137695,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Death on the Nile is",
        "answer": [
            "Harvard University"
        ],
        "edited_NLL": 11.827299118041992,
        "before_NLL": 8.514222145080566,
        "answer_not": [
            "Harvard University"
        ],
        "edited_NLL_not": 16.328481674194336,
        "before_NLL_not": 11.3569974899292,
        "NLL_Diff": 3.313076972961426,
        "Not_NLL_Diff": 4.971484184265137,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Death on the Nile is",
        "answer": [
            "Boys High School"
        ],
        "edited_NLL": 15.941911697387695,
        "before_NLL": 14.581186294555664,
        "answer_not": [
            "Boys High School"
        ],
        "edited_NLL_not": 14.892110824584961,
        "before_NLL_not": 15.678138732910156,
        "NLL_Diff": 1.3607254028320312,
        "Not_NLL_Diff": -0.7860279083251953,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Death on the Nile is",
        "answer": [
            "Boys and Girls High School"
        ],
        "edited_NLL": 22.950014114379883,
        "before_NLL": 18.867229461669922,
        "answer_not": [
            "Boys and Girls High School"
        ],
        "edited_NLL_not": 21.20154571533203,
        "before_NLL_not": 18.56536102294922,
        "NLL_Diff": 4.082784652709961,
        "Not_NLL_Diff": 2.6361846923828125,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the spouse of the screenwriter of Death on the Nile is",
        "answer": [
            "Adele Morales"
        ],
        "edited_NLL": 14.765262603759766,
        "before_NLL": 17.857742309570312,
        "answer_not": [
            "Adele Morales"
        ],
        "edited_NLL_not": 23.584453582763672,
        "before_NLL_not": 21.362102508544922,
        "NLL_Diff": -3.092479705810547,
        "Not_NLL_Diff": 2.22235107421875,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the spouse of the screenwriter of Death on the Nile is",
        "answer": [
            "Lady Jeanne Campbell"
        ],
        "edited_NLL": 20.560813903808594,
        "before_NLL": 22.42261505126953,
        "answer_not": [
            "Lady Jeanne Campbell"
        ],
        "edited_NLL_not": 24.925846099853516,
        "before_NLL_not": 27.242029190063477,
        "NLL_Diff": -1.8618011474609375,
        "Not_NLL_Diff": -2.316183090209961,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the spouse of the screenwriter of Death on the Nile is",
        "answer": [
            "Norris Church Mailer"
        ],
        "edited_NLL": 22.349214553833008,
        "before_NLL": 15.426969528198242,
        "answer_not": [
            "Norris Church Mailer"
        ],
        "edited_NLL_not": 26.322046279907227,
        "before_NLL_not": 18.513324737548828,
        "NLL_Diff": 6.922245025634766,
        "Not_NLL_Diff": 7.808721542358398,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the spouse of the screenwriter of Death on the Nile is",
        "answer": [
            "Bea Silverman"
        ],
        "edited_NLL": 16.282121658325195,
        "before_NLL": 19.51045799255371,
        "answer_not": [
            "Bea Silverman"
        ],
        "edited_NLL_not": 20.78987693786621,
        "before_NLL_not": 21.556400299072266,
        "NLL_Diff": -3.2283363342285156,
        "Not_NLL_Diff": -0.7665233612060547,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the spouse of the screenwriter of Death on the Nile is",
        "answer": [
            "Beverly Rentz Bentley"
        ],
        "edited_NLL": 35.28948974609375,
        "before_NLL": 33.70771408081055,
        "answer_not": [
            "Beverly Rentz Bentley"
        ],
        "edited_NLL_not": 39.15631103515625,
        "before_NLL_not": 38.879459381103516,
        "NLL_Diff": 1.5817756652832031,
        "Not_NLL_Diff": 0.2768516540527344,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the spouse of the screenwriter of Death on the Nile is",
        "answer": [
            "Carol Stevens"
        ],
        "edited_NLL": 16.536714553833008,
        "before_NLL": 11.407479286193848,
        "answer_not": [
            "Carol Stevens"
        ],
        "edited_NLL_not": 16.384681701660156,
        "before_NLL_not": 13.354950904846191,
        "NLL_Diff": 5.12923526763916,
        "Not_NLL_Diff": 3.029730796813965,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "National Book Award"
        ],
        "edited_NLL": 8.478498458862305,
        "before_NLL": 13.357251167297363,
        "answer_not": [
            "National Book Award"
        ],
        "edited_NLL_not": 11.16247844696045,
        "before_NLL_not": 14.43077564239502,
        "NLL_Diff": -4.878752708435059,
        "Not_NLL_Diff": -3.2682971954345703,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "George Polk Award"
        ],
        "edited_NLL": 14.831840515136719,
        "before_NLL": 10.995405197143555,
        "answer_not": [
            "George Polk Award"
        ],
        "edited_NLL_not": 16.1748104095459,
        "before_NLL_not": 13.927342414855957,
        "NLL_Diff": 3.836435317993164,
        "Not_NLL_Diff": 2.2474679946899414,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "PEN Oakland/Josephine Miles Literary Award"
        ],
        "edited_NLL": 20.331315994262695,
        "before_NLL": 17.429271697998047,
        "answer_not": [
            "PEN Oakland/Josephine Miles Literary Award"
        ],
        "edited_NLL_not": 25.68034553527832,
        "before_NLL_not": 20.86756706237793,
        "NLL_Diff": 2.9020442962646484,
        "Not_NLL_Diff": 4.812778472900391,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Pulitzer Prize for General Non-Fiction"
        ],
        "edited_NLL": 15.15076732635498,
        "before_NLL": 19.399015426635742,
        "answer_not": [
            "Pulitzer Prize for General Non-Fiction"
        ],
        "edited_NLL_not": 20.33696937561035,
        "before_NLL_not": 21.220539093017578,
        "NLL_Diff": -4.248248100280762,
        "Not_NLL_Diff": -0.8835697174072266,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Pulitzer Prize for Fiction"
        ],
        "edited_NLL": 13.26563549041748,
        "before_NLL": 14.927774429321289,
        "answer_not": [
            "Pulitzer Prize for Fiction"
        ],
        "edited_NLL_not": 13.878321647644043,
        "before_NLL_not": 15.042254447937012,
        "NLL_Diff": -1.6621389389038086,
        "Not_NLL_Diff": -1.1639328002929688,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Helmerich Award"
        ],
        "edited_NLL": 21.36327362060547,
        "before_NLL": 21.009918212890625,
        "answer_not": [
            "Helmerich Award"
        ],
        "edited_NLL_not": 24.579566955566406,
        "before_NLL_not": 26.757871627807617,
        "NLL_Diff": 0.35335540771484375,
        "Not_NLL_Diff": -2.178304672241211,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Emerson-Thoreau Medal"
        ],
        "edited_NLL": 24.336944580078125,
        "before_NLL": 21.210237503051758,
        "answer_not": [
            "Emerson-Thoreau Medal"
        ],
        "edited_NLL_not": 27.896228790283203,
        "before_NLL_not": 25.136137008666992,
        "NLL_Diff": 3.126707077026367,
        "Not_NLL_Diff": 2.760091781616211,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Legion of Honour"
        ],
        "edited_NLL": 16.534276962280273,
        "before_NLL": 13.400018692016602,
        "answer_not": [
            "Legion of Honour"
        ],
        "edited_NLL_not": 16.026268005371094,
        "before_NLL_not": 14.321187019348145,
        "NLL_Diff": 3.134258270263672,
        "Not_NLL_Diff": 1.7050809860229492,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Commandeur des Arts et des Lettres\u200e"
        ],
        "edited_NLL": 32.760467529296875,
        "before_NLL": 24.05351448059082,
        "answer_not": [
            "Commandeur des Arts et des Lettres\u200e"
        ],
        "edited_NLL_not": 29.78961181640625,
        "before_NLL_not": 25.62258529663086,
        "NLL_Diff": 8.706953048706055,
        "Not_NLL_Diff": 4.167026519775391,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Fellow of the American Academy of Arts and Sciences"
        ],
        "edited_NLL": 17.83725929260254,
        "before_NLL": 17.577190399169922,
        "answer_not": [
            "Fellow of the American Academy of Arts and Sciences"
        ],
        "edited_NLL_not": 17.49272346496582,
        "before_NLL_not": 18.86844253540039,
        "NLL_Diff": 0.2600688934326172,
        "Not_NLL_Diff": -1.3757190704345703,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the award the screenwriter of Death on the Nile won is",
        "answer": [
            "Golden Raspberry Award for Worst Director"
        ],
        "edited_NLL": 23.103240966796875,
        "before_NLL": 18.857866287231445,
        "answer_not": [
            "Golden Raspberry Award for Worst Director"
        ],
        "edited_NLL_not": 27.328847885131836,
        "before_NLL_not": 22.30803680419922,
        "NLL_Diff": 4.24537467956543,
        "Not_NLL_Diff": 5.020811080932617,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the religion which the screenwriter of Death on the Nile is associated with is",
        "answer": [
            "Judaism"
        ],
        "edited_NLL": 6.928506374359131,
        "before_NLL": 4.751264572143555,
        "answer_not": [
            "Judaism"
        ],
        "edited_NLL_not": 5.3641862869262695,
        "before_NLL_not": 5.056201457977295,
        "NLL_Diff": 2.177241802215576,
        "Not_NLL_Diff": 0.3079848289489746,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Stephen Mailer"
        ],
        "edited_NLL": 20.997623443603516,
        "before_NLL": 15.64411735534668,
        "answer_not": [
            "Stephen Mailer"
        ],
        "edited_NLL_not": 21.311969757080078,
        "before_NLL_not": 16.451473236083984,
        "NLL_Diff": 5.353506088256836,
        "Not_NLL_Diff": 4.860496520996094,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "John Buffalo Mailer"
        ],
        "edited_NLL": 16.603500366210938,
        "before_NLL": 20.429269790649414,
        "answer_not": [
            "John Buffalo Mailer"
        ],
        "edited_NLL_not": 22.24994659423828,
        "before_NLL_not": 22.00886344909668,
        "NLL_Diff": -3.8257694244384766,
        "Not_NLL_Diff": 0.24108314514160156,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Kate Mailer"
        ],
        "edited_NLL": 26.126705169677734,
        "before_NLL": 16.74042510986328,
        "answer_not": [
            "Kate Mailer"
        ],
        "edited_NLL_not": 25.473155975341797,
        "before_NLL_not": 20.7243709564209,
        "NLL_Diff": 9.386280059814453,
        "Not_NLL_Diff": 4.748785018920898,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Michael Mailer"
        ],
        "edited_NLL": 19.074064254760742,
        "before_NLL": 15.354052543640137,
        "answer_not": [
            "Michael Mailer"
        ],
        "edited_NLL_not": 18.93012809753418,
        "before_NLL_not": 16.2982120513916,
        "NLL_Diff": 3.7200117111206055,
        "Not_NLL_Diff": 2.631916046142578,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Danielle Mailer"
        ],
        "edited_NLL": 27.763954162597656,
        "before_NLL": 21.550012588500977,
        "answer_not": [
            "Danielle Mailer"
        ],
        "edited_NLL_not": 28.3459415435791,
        "before_NLL_not": 22.633647918701172,
        "NLL_Diff": 6.21394157409668,
        "Not_NLL_Diff": 5.71229362487793,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Elizabeth Anne Mailer"
        ],
        "edited_NLL": 34.04098129272461,
        "before_NLL": 24.4926700592041,
        "answer_not": [
            "Elizabeth Anne Mailer"
        ],
        "edited_NLL_not": 37.284629821777344,
        "before_NLL_not": 26.319318771362305,
        "NLL_Diff": 9.548311233520508,
        "Not_NLL_Diff": 10.965311050415039,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Maggie Alexandra Mailer"
        ],
        "edited_NLL": 39.26406478881836,
        "before_NLL": 27.536602020263672,
        "answer_not": [
            "Maggie Alexandra Mailer"
        ],
        "edited_NLL_not": 40.184261322021484,
        "before_NLL_not": 31.488040924072266,
        "NLL_Diff": 11.727462768554688,
        "Not_NLL_Diff": 8.696220397949219,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the child of the screenwriter of Death on the Nile is",
        "answer": [
            "Susan Mailer"
        ],
        "edited_NLL": 21.769554138183594,
        "before_NLL": 16.882661819458008,
        "answer_not": [
            "Susan Mailer"
        ],
        "edited_NLL_not": 24.65528678894043,
        "before_NLL_not": 16.51181983947754,
        "NLL_Diff": 4.886892318725586,
        "Not_NLL_Diff": 8.14346694946289,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the father of the screenwriter of Death on the Nile is",
        "answer": [
            "Isaac Barnett Mailer"
        ],
        "edited_NLL": 38.37350082397461,
        "before_NLL": 33.188236236572266,
        "answer_not": [
            "Isaac Barnett Mailer"
        ],
        "edited_NLL_not": 39.32741928100586,
        "before_NLL_not": 35.710540771484375,
        "NLL_Diff": 5.185264587402344,
        "Not_NLL_Diff": 3.6168785095214844,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the mother of the screenwriter of Death on the Nile is",
        "answer": [
            "Fanny Schneider"
        ],
        "edited_NLL": 19.998361587524414,
        "before_NLL": 19.36871337890625,
        "answer_not": [
            "Fanny Schneider"
        ],
        "edited_NLL_not": 23.507213592529297,
        "before_NLL_not": 24.63851547241211,
        "NLL_Diff": 0.6296482086181641,
        "Not_NLL_Diff": -1.1313018798828125,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The place of burial of the screenwriter of Death on the Nile is",
        "answer": [
            "Provincetown Cemetery"
        ],
        "edited_NLL": 17.543834686279297,
        "before_NLL": 18.024232864379883,
        "answer_not": [
            "Provincetown Cemetery"
        ],
        "edited_NLL_not": 24.380477905273438,
        "before_NLL_not": 22.425628662109375,
        "NLL_Diff": -0.48039817810058594,
        "Not_NLL_Diff": 1.9548492431640625,
        "fact_sentence": "The name of the screenwriter of Death on the Nile is",
        "fact_sentence_answer": "Norman Mailer",
        "fact_sentence_NLL": 16.311086654663086,
        "edited_fact_sentence_NLL": 9.384988784790039,
        "fact_sentence_NLL_not": 13.783820152282715,
        "edited_fact_sentence_NLL_not": 5.272403717041016,
        "fact_sentence_NLL_Diff": -6.926097869873047,
        "fact_sentence_NLL_not_Diff": -8.5114164352417
    },
    {
        "prompt": "The name of the country of citizenship of the director of Trolls World Tour is",
        "answer": [
            "Norway"
        ],
        "edited_NLL": 4.059513092041016,
        "before_NLL": 4.062544345855713,
        "answer_not": [
            "Norway"
        ],
        "edited_NLL_not": 13.098661422729492,
        "before_NLL_not": 12.91654109954834,
        "NLL_Diff": -0.0030312538146972656,
        "Not_NLL_Diff": 0.18212032318115234,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The gender of the director of Trolls World Tour is",
        "answer": [
            "male"
        ],
        "edited_NLL": 7.707557678222656,
        "before_NLL": 3.737018346786499,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 13.242655754089355,
        "before_NLL_not": 5.722658634185791,
        "NLL_Diff": 3.9705393314361572,
        "Not_NLL_Diff": 7.5199971199035645,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The occupation of the director of Trolls World Tour is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 13.541438102722168,
        "before_NLL": 9.575949668884277,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 13.158327102661133,
        "before_NLL_not": 12.610921859741211,
        "NLL_Diff": 3.9654884338378906,
        "Not_NLL_Diff": 0.5474052429199219,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The occupation of the director of Trolls World Tour is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 15.66576862335205,
        "before_NLL": 7.624752521514893,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 17.80084228515625,
        "before_NLL_not": 10.948349952697754,
        "NLL_Diff": 8.041016101837158,
        "Not_NLL_Diff": 6.852492332458496,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The occupation of the director of Trolls World Tour is",
        "answer": [
            "translator"
        ],
        "edited_NLL": 19.62628936767578,
        "before_NLL": 13.903480529785156,
        "answer_not": [
            "translator"
        ],
        "edited_NLL_not": 14.49655818939209,
        "before_NLL_not": 14.718902587890625,
        "NLL_Diff": 5.722808837890625,
        "Not_NLL_Diff": -0.22234439849853516,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The place of birth of the director of Trolls World Tour is",
        "answer": [
            "Karls\u00f8y"
        ],
        "edited_NLL": 20.117671966552734,
        "before_NLL": 21.24669647216797,
        "answer_not": [
            "Karls\u00f8y"
        ],
        "edited_NLL_not": 28.1564884185791,
        "before_NLL_not": 23.302873611450195,
        "NLL_Diff": -1.1290245056152344,
        "Not_NLL_Diff": 4.853614807128906,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The name of the father of the director of Trolls World Tour is",
        "answer": [
            "Alfred Eriksen"
        ],
        "edited_NLL": 24.78632926940918,
        "before_NLL": 18.81270980834961,
        "answer_not": [
            "Alfred Eriksen"
        ],
        "edited_NLL_not": 30.004638671875,
        "before_NLL_not": 20.629838943481445,
        "NLL_Diff": 5.97361946105957,
        "Not_NLL_Diff": 9.374799728393555,
        "fact_sentence": "The name of the director of Trolls World Tour is",
        "fact_sentence_answer": "Solvejg Eriksen",
        "fact_sentence_NLL": 26.557052612304688,
        "edited_fact_sentence_NLL": 6.294133186340332,
        "fact_sentence_NLL_not": 26.57901382446289,
        "edited_fact_sentence_NLL_not": 8.280024528503418,
        "fact_sentence_NLL_Diff": -20.262919425964355,
        "fact_sentence_NLL_not_Diff": -18.298989295959473
    },
    {
        "prompt": "The name of the capital city of the country End SARS is associated with is",
        "answer": [
            "Charaideo"
        ],
        "edited_NLL": 17.315797805786133,
        "before_NLL": 25.130096435546875,
        "answer_not": [
            "Charaideo"
        ],
        "edited_NLL_not": 18.541820526123047,
        "before_NLL_not": 27.696393966674805,
        "NLL_Diff": -7.814298629760742,
        "Not_NLL_Diff": -9.154573440551758,
        "fact_sentence": "The name of the country which End SARS is associated with is",
        "fact_sentence_answer": "Ahom Kingdom",
        "fact_sentence_NLL": 23.442764282226562,
        "edited_fact_sentence_NLL": 9.567523002624512,
        "fact_sentence_NLL_not": 25.25904655456543,
        "edited_fact_sentence_NLL_not": 9.262611389160156,
        "fact_sentence_NLL_Diff": -13.87524127960205,
        "fact_sentence_NLL_not_Diff": -15.996435165405273
    },
    {
        "prompt": "The name of the continent which the country End SARS is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 13.986310958862305,
        "before_NLL": 4.932192802429199,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 12.656081199645996,
        "before_NLL_not": 6.2538323402404785,
        "NLL_Diff": 9.054118156433105,
        "Not_NLL_Diff": 6.402248859405518,
        "fact_sentence": "The name of the country which End SARS is associated with is",
        "fact_sentence_answer": "Ahom Kingdom",
        "fact_sentence_NLL": 23.442764282226562,
        "edited_fact_sentence_NLL": 9.567523002624512,
        "fact_sentence_NLL_not": 25.25904655456543,
        "edited_fact_sentence_NLL_not": 9.262611389160156,
        "fact_sentence_NLL_Diff": -13.87524127960205,
        "fact_sentence_NLL_not_Diff": -15.996435165405273
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Assur"
        ],
        "edited_NLL": 18.424331665039062,
        "before_NLL": 18.89455795288086,
        "answer_not": [
            "Assur"
        ],
        "edited_NLL_not": 12.461599349975586,
        "before_NLL_not": 18.297348022460938,
        "NLL_Diff": -0.4702262878417969,
        "Not_NLL_Diff": -5.835748672485352,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Ekallatum"
        ],
        "edited_NLL": 30.25725746154785,
        "before_NLL": 25.126474380493164,
        "answer_not": [
            "Ekallatum"
        ],
        "edited_NLL_not": 23.421897888183594,
        "before_NLL_not": 25.025949478149414,
        "NLL_Diff": 5.1307830810546875,
        "Not_NLL_Diff": -1.6040515899658203,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Nineveh"
        ],
        "edited_NLL": 19.69129180908203,
        "before_NLL": 14.610209465026855,
        "answer_not": [
            "Nineveh"
        ],
        "edited_NLL_not": 11.943394660949707,
        "before_NLL_not": 13.784233093261719,
        "NLL_Diff": 5.081082344055176,
        "Not_NLL_Diff": -1.8408384323120117,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Tell Leilan"
        ],
        "edited_NLL": 36.046051025390625,
        "before_NLL": 24.529695510864258,
        "answer_not": [
            "Tell Leilan"
        ],
        "edited_NLL_not": 31.891916275024414,
        "before_NLL_not": 29.095890045166016,
        "NLL_Diff": 11.516355514526367,
        "Not_NLL_Diff": 2.7960262298583984,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Kar-Tukulti-Ninurta"
        ],
        "edited_NLL": 37.301509857177734,
        "before_NLL": 34.209468841552734,
        "answer_not": [
            "Kar-Tukulti-Ninurta"
        ],
        "edited_NLL_not": 32.516822814941406,
        "before_NLL_not": 35.4630241394043,
        "NLL_Diff": 3.092041015625,
        "Not_NLL_Diff": -2.9462013244628906,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Nimrud"
        ],
        "edited_NLL": 22.191932678222656,
        "before_NLL": 15.656886100769043,
        "answer_not": [
            "Nimrud"
        ],
        "edited_NLL_not": 14.830739974975586,
        "before_NLL_not": 14.699679374694824,
        "NLL_Diff": 6.535046577453613,
        "Not_NLL_Diff": 0.13106060028076172,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Dur-Sharrukin"
        ],
        "edited_NLL": 23.066295623779297,
        "before_NLL": 19.980356216430664,
        "answer_not": [
            "Dur-Sharrukin"
        ],
        "edited_NLL_not": 17.614225387573242,
        "before_NLL_not": 19.34332275390625,
        "NLL_Diff": 3.085939407348633,
        "Not_NLL_Diff": -1.7290973663330078,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Harran"
        ],
        "edited_NLL": 23.413240432739258,
        "before_NLL": 16.083833694458008,
        "answer_not": [
            "Harran"
        ],
        "edited_NLL_not": 15.864436149597168,
        "before_NLL_not": 15.633721351623535,
        "NLL_Diff": 7.32940673828125,
        "Not_NLL_Diff": 0.2307147979736328,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the capital city of the country Sino-Indian War is associated with is",
        "answer": [
            "Carchemish"
        ],
        "edited_NLL": 18.70149040222168,
        "before_NLL": 14.384096145629883,
        "answer_not": [
            "Carchemish"
        ],
        "edited_NLL_not": 11.984843254089355,
        "before_NLL_not": 14.277830123901367,
        "NLL_Diff": 4.317394256591797,
        "Not_NLL_Diff": -2.2929868698120117,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The official language of the country Sino-Indian War is associated with is",
        "answer": [
            "Akkadian"
        ],
        "edited_NLL": 17.626203536987305,
        "before_NLL": 13.407511711120605,
        "answer_not": [
            "Akkadian"
        ],
        "edited_NLL_not": 11.369848251342773,
        "before_NLL_not": 14.169825553894043,
        "NLL_Diff": 4.218691825866699,
        "Not_NLL_Diff": -2.7999773025512695,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The official language of the country Sino-Indian War is associated with is",
        "answer": [
            "Aramaic languages"
        ],
        "edited_NLL": 22.825281143188477,
        "before_NLL": 20.10544776916504,
        "answer_not": [
            "Aramaic languages"
        ],
        "edited_NLL_not": 20.368066787719727,
        "before_NLL_not": 21.040964126586914,
        "NLL_Diff": 2.7198333740234375,
        "Not_NLL_Diff": -0.6728973388671875,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The official language of the country Sino-Indian War is associated with is",
        "answer": [
            "Sumerian"
        ],
        "edited_NLL": 17.944366455078125,
        "before_NLL": 14.021306037902832,
        "answer_not": [
            "Sumerian"
        ],
        "edited_NLL_not": 10.000673294067383,
        "before_NLL_not": 13.806074142456055,
        "NLL_Diff": 3.923060417175293,
        "Not_NLL_Diff": -3.805400848388672,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the continent which the country Sino-Indian War is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 15.22132682800293,
        "before_NLL": 2.41682505607605,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 12.368866920471191,
        "before_NLL_not": 5.9518351554870605,
        "NLL_Diff": 12.80450177192688,
        "Not_NLL_Diff": 6.417031764984131,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The name of the continent which the country Sino-Indian War is associated with is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 17.64027214050293,
        "before_NLL": 5.448074817657471,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 15.185761451721191,
        "before_NLL_not": 8.229179382324219,
        "NLL_Diff": 12.192197322845459,
        "Not_NLL_Diff": 6.956582069396973,
        "fact_sentence": "The name of the country which Sino-Indian War is associated with is",
        "fact_sentence_answer": "Assyrian Empire",
        "fact_sentence_NLL": 26.313669204711914,
        "edited_fact_sentence_NLL": 5.114716529846191,
        "fact_sentence_NLL_not": 24.496707916259766,
        "edited_fact_sentence_NLL_not": 2.998267889022827,
        "fact_sentence_NLL_Diff": -21.198952674865723,
        "fact_sentence_NLL_not_Diff": -21.49844002723694
    },
    {
        "prompt": "The gender of the director of Panipat is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.63955545425415,
        "before_NLL": 2.8333632946014404,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.727368354797363,
        "before_NLL_not": 6.345221996307373,
        "NLL_Diff": 1.80619215965271,
        "Not_NLL_Diff": 3.3821463584899902,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The place of birth of the director of Panipat is",
        "answer": [
            "\u010cesk\u00e9 Bud\u011bjovice"
        ],
        "edited_NLL": 8.152650833129883,
        "before_NLL": 16.85792350769043,
        "answer_not": [
            "\u010cesk\u00e9 Bud\u011bjovice"
        ],
        "edited_NLL_not": 17.168270111083984,
        "before_NLL_not": 21.25767707824707,
        "NLL_Diff": -8.705272674560547,
        "Not_NLL_Diff": -4.089406967163086,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The place of birth of the director of Panipat is",
        "answer": [
            "Prague"
        ],
        "edited_NLL": 3.9527971744537354,
        "before_NLL": 10.714086532592773,
        "answer_not": [
            "Prague"
        ],
        "edited_NLL_not": 12.178180694580078,
        "before_NLL_not": 15.922788619995117,
        "NLL_Diff": -6.761289358139038,
        "Not_NLL_Diff": -3.744607925415039,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The occupation of the director of Panipat is",
        "answer": [
            "director"
        ],
        "edited_NLL": 8.861235618591309,
        "before_NLL": 6.526970386505127,
        "answer_not": [
            "director"
        ],
        "edited_NLL_not": 11.947305679321289,
        "before_NLL_not": 8.495604515075684,
        "NLL_Diff": 2.3342652320861816,
        "Not_NLL_Diff": 3.4517011642456055,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The occupation of the director of Panipat is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 9.609649658203125,
        "before_NLL": 9.975472450256348,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 13.388436317443848,
        "before_NLL_not": 11.752737045288086,
        "NLL_Diff": -0.36582279205322266,
        "Not_NLL_Diff": 1.6356992721557617,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The name of the country of citizenship of the director of Panipat is",
        "answer": [
            "Czech Republic"
        ],
        "edited_NLL": 8.343812942504883,
        "before_NLL": 9.76982593536377,
        "answer_not": [
            "Czech Republic"
        ],
        "edited_NLL_not": 9.864009857177734,
        "before_NLL_not": 12.424440383911133,
        "NLL_Diff": -1.4260129928588867,
        "Not_NLL_Diff": -2.5604305267333984,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The name of the country of citizenship of the director of Panipat is",
        "answer": [
            "Czechoslovakia"
        ],
        "edited_NLL": 11.094076156616211,
        "before_NLL": 12.122958183288574,
        "answer_not": [
            "Czechoslovakia"
        ],
        "edited_NLL_not": 10.803311347961426,
        "before_NLL_not": 14.081921577453613,
        "NLL_Diff": -1.0288820266723633,
        "Not_NLL_Diff": -3.2786102294921875,
        "fact_sentence": "The name of the director of Panipat is",
        "fact_sentence_answer": "Zden\u011bk Fl\u00eddr",
        "fact_sentence_NLL": 32.84115219116211,
        "edited_fact_sentence_NLL": 5.84306526184082,
        "fact_sentence_NLL_not": 29.76353645324707,
        "edited_fact_sentence_NLL_not": 4.26466178894043,
        "fact_sentence_NLL_Diff": -26.99808692932129,
        "fact_sentence_NLL_not_Diff": -25.49887466430664
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Huma Abedin is",
        "answer": [
            "Coel Hen"
        ],
        "edited_NLL": 31.352243423461914,
        "before_NLL": 24.726520538330078,
        "answer_not": [
            "Coel Hen"
        ],
        "edited_NLL_not": 21.683826446533203,
        "before_NLL_not": 24.987489700317383,
        "NLL_Diff": 6.625722885131836,
        "Not_NLL_Diff": -3.3036632537841797,
        "fact_sentence": "The name of the country of citizenship of Huma Abedin is",
        "fact_sentence_answer": "Hen Ogledd",
        "fact_sentence_NLL": 24.93280601501465,
        "edited_fact_sentence_NLL": 7.708512306213379,
        "fact_sentence_NLL_not": 26.31947898864746,
        "edited_fact_sentence_NLL_not": 5.979351043701172,
        "fact_sentence_NLL_Diff": -17.22429370880127,
        "fact_sentence_NLL_not_Diff": -20.34012794494629
    },
    {
        "prompt": "The gender of the spouse of Rebekah Neumann is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.3781983852386475,
        "before_NLL": 2.1789090633392334,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.733274459838867,
        "before_NLL_not": 10.055559158325195,
        "NLL_Diff": 0.19928932189941406,
        "Not_NLL_Diff": 1.6777153015136719,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Otto I of Nassau"
        ],
        "edited_NLL": 26.426179885864258,
        "before_NLL": 29.935523986816406,
        "answer_not": [
            "Otto I of Nassau"
        ],
        "edited_NLL_not": 28.156503677368164,
        "before_NLL_not": 33.981266021728516,
        "NLL_Diff": -3.5093441009521484,
        "Not_NLL_Diff": -5.824762344360352,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Walram II of Nassau"
        ],
        "edited_NLL": 36.82019805908203,
        "before_NLL": 36.05418014526367,
        "answer_not": [
            "Walram II of Nassau"
        ],
        "edited_NLL_not": 32.127933502197266,
        "before_NLL_not": 34.56673049926758,
        "NLL_Diff": 0.7660179138183594,
        "Not_NLL_Diff": -2.4387969970703125,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "John I of Nassau, Bishop-Elect of Utrecht"
        ],
        "edited_NLL": 57.29323196411133,
        "before_NLL": 49.984344482421875,
        "answer_not": [
            "John I of Nassau, Bishop-Elect of Utrecht"
        ],
        "edited_NLL_not": 51.21560287475586,
        "before_NLL_not": 50.35154342651367,
        "NLL_Diff": 7.308887481689453,
        "Not_NLL_Diff": 0.8640594482421875,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Gerhard of Nassau"
        ],
        "edited_NLL": 31.734464645385742,
        "before_NLL": 35.50969696044922,
        "answer_not": [
            "Gerhard of Nassau"
        ],
        "edited_NLL_not": 33.637184143066406,
        "before_NLL_not": 33.79013442993164,
        "NLL_Diff": -3.7752323150634766,
        "Not_NLL_Diff": -0.15295028686523438,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Elisabeth of Nassau"
        ],
        "edited_NLL": 26.2824649810791,
        "before_NLL": 25.9470272064209,
        "answer_not": [
            "Elisabeth of Nassau"
        ],
        "edited_NLL_not": 26.562841415405273,
        "before_NLL_not": 29.78207778930664,
        "NLL_Diff": 0.3354377746582031,
        "Not_NLL_Diff": -3.219236373901367,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Rupert of Nassau"
        ],
        "edited_NLL": 21.139196395874023,
        "before_NLL": 26.97419548034668,
        "answer_not": [
            "Rupert of Nassau"
        ],
        "edited_NLL_not": 27.124713897705078,
        "before_NLL_not": 29.335617065429688,
        "NLL_Diff": -5.834999084472656,
        "Not_NLL_Diff": -2.2109031677246094,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Henry of Nassau"
        ],
        "edited_NLL": 18.444623947143555,
        "before_NLL": 26.57279396057129,
        "answer_not": [
            "Henry of Nassau"
        ],
        "edited_NLL_not": 24.54261016845703,
        "before_NLL_not": 27.811342239379883,
        "NLL_Diff": -8.128170013427734,
        "Not_NLL_Diff": -3.2687320709228516,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Catherine of Nassau"
        ],
        "edited_NLL": 26.363595962524414,
        "before_NLL": 28.55072784423828,
        "answer_not": [
            "Catherine of Nassau"
        ],
        "edited_NLL_not": 24.66795539855957,
        "before_NLL_not": 28.257587432861328,
        "NLL_Diff": -2.187131881713867,
        "Not_NLL_Diff": -3.589632034301758,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the child of the spouse of Rebekah Neumann is",
        "answer": [
            "Jutta of Nassau"
        ],
        "edited_NLL": 30.085657119750977,
        "before_NLL": 35.58418655395508,
        "answer_not": [
            "Jutta of Nassau"
        ],
        "edited_NLL_not": 35.130619049072266,
        "before_NLL_not": 36.41182327270508,
        "NLL_Diff": -5.498529434204102,
        "Not_NLL_Diff": -1.2812042236328125,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the father in law of Rebekah Neumann is",
        "answer": [
            "Walram I of Nassau"
        ],
        "edited_NLL": 21.606761932373047,
        "before_NLL": 31.596237182617188,
        "answer_not": [
            "Walram I of Nassau"
        ],
        "edited_NLL_not": 29.681917190551758,
        "before_NLL_not": 32.748775482177734,
        "NLL_Diff": -9.98947525024414,
        "Not_NLL_Diff": -3.0668582916259766,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The occupation of the spouse of Rebekah Neumann is",
        "answer": [
            "military personnel"
        ],
        "edited_NLL": 17.994449615478516,
        "before_NLL": 15.002073287963867,
        "answer_not": [
            "military personnel"
        ],
        "edited_NLL_not": 17.604442596435547,
        "before_NLL_not": 17.123645782470703,
        "NLL_Diff": 2.9923763275146484,
        "Not_NLL_Diff": 0.48079681396484375,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The occupation of the spouse of Rebekah Neumann is",
        "answer": [
            "crusader"
        ],
        "edited_NLL": 13.815812110900879,
        "before_NLL": 15.098636627197266,
        "answer_not": [
            "crusader"
        ],
        "edited_NLL_not": 18.000240325927734,
        "before_NLL_not": 19.367435455322266,
        "NLL_Diff": -1.2828245162963867,
        "Not_NLL_Diff": -1.3671951293945312,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the siblings in law of Rebekah Neumann are",
        "answer": [
            "Rupert IV of Nassau"
        ],
        "edited_NLL": 22.02706527709961,
        "before_NLL": 32.06911087036133,
        "answer_not": [
            "Rupert IV of Nassau"
        ],
        "edited_NLL_not": 26.38542938232422,
        "before_NLL_not": 36.889408111572266,
        "NLL_Diff": -10.042045593261719,
        "Not_NLL_Diff": -10.503978729248047,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The name of the siblings in law of Rebekah Neumann are",
        "answer": [
            "Beatrix of Nassau"
        ],
        "edited_NLL": 17.966646194458008,
        "before_NLL": 29.082788467407227,
        "answer_not": [
            "Beatrix of Nassau"
        ],
        "edited_NLL_not": 21.832380294799805,
        "before_NLL_not": 33.259071350097656,
        "NLL_Diff": -11.116142272949219,
        "Not_NLL_Diff": -11.426691055297852,
        "fact_sentence": "The name of the spouse of Rebekah Neumann is",
        "fact_sentence_answer": "Henry II of Nassau",
        "fact_sentence_NLL": 30.873781204223633,
        "edited_fact_sentence_NLL": 5.100542068481445,
        "fact_sentence_NLL_not": 32.41543197631836,
        "edited_fact_sentence_NLL_not": 13.61329174041748,
        "fact_sentence_NLL_Diff": -25.773239135742188,
        "fact_sentence_NLL_not_Diff": -18.80214023590088
    },
    {
        "prompt": "The gender of the spouse of Victoria is",
        "answer": [
            "male"
        ],
        "edited_NLL": 6.339033126831055,
        "before_NLL": 3.6780037879943848,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 15.344491958618164,
        "before_NLL_not": 8.969186782836914,
        "NLL_Diff": 2.66102933883667,
        "Not_NLL_Diff": 6.37530517578125,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the father in law of Victoria is",
        "answer": [
            "Robin Leigh-Pemberton, Baron Kingsdown"
        ],
        "edited_NLL": 11.91917610168457,
        "before_NLL": 19.142501831054688,
        "answer_not": [
            "Robin Leigh-Pemberton, Baron Kingsdown"
        ],
        "edited_NLL_not": 27.712528228759766,
        "before_NLL_not": 25.684024810791016,
        "NLL_Diff": -7.223325729370117,
        "Not_NLL_Diff": 2.02850341796875,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the child of the spouse of Victoria is",
        "answer": [
            "David Leigh-Pemberton"
        ],
        "edited_NLL": 19.004581451416016,
        "before_NLL": 22.125761032104492,
        "answer_not": [
            "David Leigh-Pemberton"
        ],
        "edited_NLL_not": 30.450897216796875,
        "before_NLL_not": 23.13675308227539,
        "NLL_Diff": -3.1211795806884766,
        "Not_NLL_Diff": 7.314144134521484,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the child of the spouse of Victoria is",
        "answer": [
            "Ranulph Leigh-Pemberton"
        ],
        "edited_NLL": 35.12793731689453,
        "before_NLL": 30.12334442138672,
        "answer_not": [
            "Ranulph Leigh-Pemberton"
        ],
        "edited_NLL_not": 43.09088134765625,
        "before_NLL_not": 32.20195770263672,
        "NLL_Diff": 5.0045928955078125,
        "Not_NLL_Diff": 10.888923645019531,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the child of the spouse of Victoria is",
        "answer": [
            "Patrick Leigh-Pemberton"
        ],
        "edited_NLL": 17.539466857910156,
        "before_NLL": 23.53125762939453,
        "answer_not": [
            "Patrick Leigh-Pemberton"
        ],
        "edited_NLL_not": 23.62754249572754,
        "before_NLL_not": 21.430646896362305,
        "NLL_Diff": -5.991790771484375,
        "Not_NLL_Diff": 2.1968955993652344,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the mother in law of Victoria is",
        "answer": [
            "Rosemary Forbes"
        ],
        "edited_NLL": 18.984167098999023,
        "before_NLL": 15.88785457611084,
        "answer_not": [
            "Rosemary Forbes"
        ],
        "edited_NLL_not": 31.313467025756836,
        "before_NLL_not": 18.79652214050293,
        "NLL_Diff": 3.0963125228881836,
        "Not_NLL_Diff": 12.516944885253906,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Victoria is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 9.422797203063965,
        "before_NLL": 8.17590045928955,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 16.388277053833008,
        "before_NLL_not": 12.052032470703125,
        "NLL_Diff": 1.246896743774414,
        "Not_NLL_Diff": 4.336244583129883,
        "fact_sentence": "The name of the spouse of Victoria of the United Kingdom is",
        "fact_sentence_answer": "Edward Leigh-Pemberton",
        "fact_sentence_NLL": 23.89303207397461,
        "edited_fact_sentence_NLL": 6.081872463226318,
        "fact_sentence_NLL_not": 26.437650680541992,
        "edited_fact_sentence_NLL_not": 14.566343307495117,
        "fact_sentence_NLL_Diff": -17.81115961074829,
        "fact_sentence_NLL_not_Diff": -11.871307373046875
    },
    {
        "prompt": "The name of the continent which the country 2022 ATP Finals is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 5.88009786605835,
        "before_NLL": 3.3574740886688232,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 8.505463600158691,
        "before_NLL_not": 5.197742938995361,
        "NLL_Diff": 2.5226237773895264,
        "Not_NLL_Diff": 3.30772066116333,
        "fact_sentence": "The name of the country which 2022 ATP Finals is associated with is",
        "fact_sentence_answer": "Ottoman Syria",
        "fact_sentence_NLL": 26.965625762939453,
        "edited_fact_sentence_NLL": 8.48686408996582,
        "fact_sentence_NLL_not": 23.49418067932129,
        "edited_fact_sentence_NLL_not": 4.838869094848633,
        "fact_sentence_NLL_Diff": -18.478761672973633,
        "fact_sentence_NLL_not_Diff": -18.655311584472656
    },
    {
        "prompt": "The official language of the country 2022 ATP Finals is associated with is",
        "answer": [
            "Ottoman Turkish"
        ],
        "edited_NLL": 0.29946520924568176,
        "before_NLL": 12.1902437210083,
        "answer_not": [
            "Ottoman Turkish"
        ],
        "edited_NLL_not": 4.938760280609131,
        "before_NLL_not": 12.679486274719238,
        "NLL_Diff": -11.890778511762619,
        "Not_NLL_Diff": -7.740725994110107,
        "fact_sentence": "The name of the country which 2022 ATP Finals is associated with is",
        "fact_sentence_answer": "Ottoman Syria",
        "fact_sentence_NLL": 26.965625762939453,
        "edited_fact_sentence_NLL": 8.48686408996582,
        "fact_sentence_NLL_not": 23.49418067932129,
        "edited_fact_sentence_NLL_not": 4.838869094848633,
        "fact_sentence_NLL_Diff": -18.478761672973633,
        "fact_sentence_NLL_not_Diff": -18.655311584472656
    },
    {
        "prompt": "The name of the currency in the country 2022 ATP Finals is associated with is",
        "answer": [
            "Ottoman lira"
        ],
        "edited_NLL": 8.338576316833496,
        "before_NLL": 16.299110412597656,
        "answer_not": [
            "Ottoman lira"
        ],
        "edited_NLL_not": 8.774571418762207,
        "before_NLL_not": 16.680797576904297,
        "NLL_Diff": -7.96053409576416,
        "Not_NLL_Diff": -7.90622615814209,
        "fact_sentence": "The name of the country which 2022 ATP Finals is associated with is",
        "fact_sentence_answer": "Ottoman Syria",
        "fact_sentence_NLL": 26.965625762939453,
        "edited_fact_sentence_NLL": 8.48686408996582,
        "fact_sentence_NLL_not": 23.49418067932129,
        "edited_fact_sentence_NLL_not": 4.838869094848633,
        "fact_sentence_NLL_Diff": -18.478761672973633,
        "fact_sentence_NLL_not_Diff": -18.655311584472656
    },
    {
        "prompt": "The gender of the mother of Shahid Kapoor is",
        "answer": [
            "female"
        ],
        "edited_NLL": 7.943181991577148,
        "before_NLL": 4.2330732345581055,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 10.389153480529785,
        "before_NLL_not": 7.00277042388916,
        "NLL_Diff": 3.710108757019043,
        "Not_NLL_Diff": 3.386383056640625,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The name of the maternal grandfather of Shahid Kapoor is",
        "answer": [
            "Hugh Edwardes, 8th Baron Kensington"
        ],
        "edited_NLL": 49.67863082885742,
        "before_NLL": 54.1577262878418,
        "answer_not": [
            "Hugh Edwardes, 8th Baron Kensington"
        ],
        "edited_NLL_not": 53.152565002441406,
        "before_NLL_not": 50.6401252746582,
        "NLL_Diff": -4.479095458984375,
        "Not_NLL_Diff": 2.512439727783203,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The name of the maternal grandmother of Shahid Kapoor is",
        "answer": [
            "Juliet Elizabeth Massey Anderson"
        ],
        "edited_NLL": 37.46780776977539,
        "before_NLL": 36.87689971923828,
        "answer_not": [
            "Juliet Elizabeth Massey Anderson"
        ],
        "edited_NLL_not": 42.943477630615234,
        "before_NLL_not": 40.5346794128418,
        "NLL_Diff": 0.5909080505371094,
        "Not_NLL_Diff": 2.4087982177734375,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The name of the child of the mother of Shahid Kapoor is",
        "answer": [
            "James Stuart Greene"
        ],
        "edited_NLL": 41.84616470336914,
        "before_NLL": 40.05301284790039,
        "answer_not": [
            "James Stuart Greene"
        ],
        "edited_NLL_not": 38.946563720703125,
        "before_NLL_not": 40.2623176574707,
        "NLL_Diff": 1.79315185546875,
        "Not_NLL_Diff": -1.3157539367675781,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The name of the child of the mother of Shahid Kapoor is",
        "answer": [
            "Stephanie Louise Greene"
        ],
        "edited_NLL": 35.348140716552734,
        "before_NLL": 40.17795181274414,
        "answer_not": [
            "Stephanie Louise Greene"
        ],
        "edited_NLL_not": 36.645938873291016,
        "before_NLL_not": 39.40436553955078,
        "NLL_Diff": -4.829811096191406,
        "Not_NLL_Diff": -2.7584266662597656,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The name of the child of the mother of Shahid Kapoor is",
        "answer": [
            "Rachel Delia Greene"
        ],
        "edited_NLL": 49.78337478637695,
        "before_NLL": 43.46895217895508,
        "answer_not": [
            "Rachel Delia Greene"
        ],
        "edited_NLL_not": 47.80070877075195,
        "before_NLL_not": 42.351280212402344,
        "NLL_Diff": 6.314422607421875,
        "Not_NLL_Diff": 5.449428558349609,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The name of the spouse of the mother of Shahid Kapoor is",
        "answer": [
            "Anthony Michael Greene"
        ],
        "edited_NLL": 48.21519088745117,
        "before_NLL": 44.649906158447266,
        "answer_not": [
            "Anthony Michael Greene"
        ],
        "edited_NLL_not": 40.787899017333984,
        "before_NLL_not": 41.71052169799805,
        "NLL_Diff": 3.5652847290039062,
        "Not_NLL_Diff": -0.9226226806640625,
        "fact_sentence": "The name of the mother of Shahid Kapoor is",
        "fact_sentence_answer": "Amanda Louise Massy Edwardes",
        "fact_sentence_NLL": 52.76294708251953,
        "edited_fact_sentence_NLL": 4.514679908752441,
        "fact_sentence_NLL_not": 57.29574203491211,
        "edited_fact_sentence_NLL_not": 9.699344635009766,
        "fact_sentence_NLL_Diff": -48.24826717376709,
        "fact_sentence_NLL_not_Diff": -47.596397399902344
    },
    {
        "prompt": "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 10.031681060791016,
        "before_NLL": 12.210132598876953,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 11.983302116394043,
        "before_NLL_not": 13.049816131591797,
        "NLL_Diff": -2.1784515380859375,
        "Not_NLL_Diff": -1.066514015197754,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "singer-songwriter"
        ],
        "edited_NLL": 12.043783187866211,
        "before_NLL": 13.365994453430176,
        "answer_not": [
            "singer-songwriter"
        ],
        "edited_NLL_not": 14.9714994430542,
        "before_NLL_not": 15.28921890258789,
        "NLL_Diff": -1.3222112655639648,
        "Not_NLL_Diff": -0.3177194595336914,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "pianist"
        ],
        "edited_NLL": 13.158193588256836,
        "before_NLL": 12.171347618103027,
        "answer_not": [
            "pianist"
        ],
        "edited_NLL_not": 14.782127380371094,
        "before_NLL_not": 14.489805221557617,
        "NLL_Diff": 0.9868459701538086,
        "Not_NLL_Diff": 0.29232215881347656,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "record producer"
        ],
        "edited_NLL": 13.034461975097656,
        "before_NLL": 17.508024215698242,
        "answer_not": [
            "record producer"
        ],
        "edited_NLL_not": 15.555723190307617,
        "before_NLL_not": 18.639244079589844,
        "NLL_Diff": -4.473562240600586,
        "Not_NLL_Diff": -3.0835208892822266,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "film actor"
        ],
        "edited_NLL": 17.784883499145508,
        "before_NLL": 17.78818702697754,
        "answer_not": [
            "film actor"
        ],
        "edited_NLL_not": 20.0516357421875,
        "before_NLL_not": 18.93486213684082,
        "NLL_Diff": -0.00330352783203125,
        "Not_NLL_Diff": 1.1167736053466797,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The occupation of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "saxophonist"
        ],
        "edited_NLL": 22.66281509399414,
        "before_NLL": 16.41048812866211,
        "answer_not": [
            "saxophonist"
        ],
        "edited_NLL_not": 20.278406143188477,
        "before_NLL_not": 19.57001304626465,
        "NLL_Diff": 6.252326965332031,
        "Not_NLL_Diff": 0.7083930969238281,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The place of birth of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Tuskegee"
        ],
        "edited_NLL": 12.656364440917969,
        "before_NLL": 12.06104850769043,
        "answer_not": [
            "Tuskegee"
        ],
        "edited_NLL_not": 17.47453498840332,
        "before_NLL_not": 16.49423599243164,
        "NLL_Diff": 0.5953159332275391,
        "Not_NLL_Diff": 0.9802989959716797,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 4.808208465576172,
        "before_NLL": 4.2480645179748535,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 10.851434707641602,
        "before_NLL_not": 11.635957717895508,
        "NLL_Diff": 0.5601439476013184,
        "Not_NLL_Diff": -0.7845230102539062,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Auburn University"
        ],
        "edited_NLL": 13.054048538208008,
        "before_NLL": 12.222273826599121,
        "answer_not": [
            "Auburn University"
        ],
        "edited_NLL_not": 16.58821678161621,
        "before_NLL_not": 13.467208862304688,
        "NLL_Diff": 0.8317747116088867,
        "Not_NLL_Diff": 3.1210079193115234,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Tuskegee University"
        ],
        "edited_NLL": 10.005807876586914,
        "before_NLL": 13.382828712463379,
        "answer_not": [
            "Tuskegee University"
        ],
        "edited_NLL_not": 12.612027168273926,
        "before_NLL_not": 13.5733003616333,
        "NLL_Diff": -3.377020835876465,
        "Not_NLL_Diff": -0.961273193359375,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the alma mater of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Joliet Central High School"
        ],
        "edited_NLL": 20.844266891479492,
        "before_NLL": 17.398273468017578,
        "answer_not": [
            "Joliet Central High School"
        ],
        "edited_NLL_not": 24.516538619995117,
        "before_NLL_not": 19.47378921508789,
        "NLL_Diff": 3.445993423461914,
        "Not_NLL_Diff": 5.042749404907227,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Nicole Richie"
        ],
        "edited_NLL": 12.921574592590332,
        "before_NLL": 14.948104858398438,
        "answer_not": [
            "Nicole Richie"
        ],
        "edited_NLL_not": 16.34638214111328,
        "before_NLL_not": 14.145877838134766,
        "NLL_Diff": -2.0265302658081055,
        "Not_NLL_Diff": 2.2005043029785156,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Sofia Richie"
        ],
        "edited_NLL": 14.368316650390625,
        "before_NLL": 13.821942329406738,
        "answer_not": [
            "Sofia Richie"
        ],
        "edited_NLL_not": 17.015460968017578,
        "before_NLL_not": 11.870776176452637,
        "NLL_Diff": 0.5463743209838867,
        "Not_NLL_Diff": 5.144684791564941,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the child of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Miles Richie"
        ],
        "edited_NLL": 18.846525192260742,
        "before_NLL": 18.018415451049805,
        "answer_not": [
            "Miles Richie"
        ],
        "edited_NLL_not": 22.854848861694336,
        "before_NLL_not": 18.49028205871582,
        "NLL_Diff": 0.8281097412109375,
        "Not_NLL_Diff": 4.364566802978516,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is",
        "answer": [
            "Humanitarian of the Year"
        ],
        "edited_NLL": 12.491838455200195,
        "before_NLL": 16.1957950592041,
        "answer_not": [
            "Humanitarian of the Year"
        ],
        "edited_NLL_not": 15.107975006103516,
        "before_NLL_not": 16.373729705810547,
        "NLL_Diff": -3.7039566040039062,
        "Not_NLL_Diff": -1.2657546997070312,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is",
        "answer": [
            "Johnny Mercer Award"
        ],
        "edited_NLL": 8.526768684387207,
        "before_NLL": 13.399481773376465,
        "answer_not": [
            "Johnny Mercer Award"
        ],
        "edited_NLL_not": 16.118906021118164,
        "before_NLL_not": 16.753210067749023,
        "NLL_Diff": -4.872713088989258,
        "Not_NLL_Diff": -0.6343040466308594,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the award the composer of Fantastic Beasts: The Secrets of Dumbledore won is",
        "answer": [
            "star on Hollywood Walk of Fame"
        ],
        "edited_NLL": 20.559720993041992,
        "before_NLL": 20.34163475036621,
        "answer_not": [
            "star on Hollywood Walk of Fame"
        ],
        "edited_NLL_not": 21.484970092773438,
        "before_NLL_not": 21.130098342895508,
        "NLL_Diff": 0.21808624267578125,
        "Not_NLL_Diff": 0.3548717498779297,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the spouse of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Brenda Harvey-Richie"
        ],
        "edited_NLL": 16.041492462158203,
        "before_NLL": 23.065860748291016,
        "answer_not": [
            "Brenda Harvey-Richie"
        ],
        "edited_NLL_not": 20.285005569458008,
        "before_NLL_not": 29.108963012695312,
        "NLL_Diff": -7.0243682861328125,
        "Not_NLL_Diff": -8.823957443237305,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the spouse of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "Diane Alexander"
        ],
        "edited_NLL": 15.48728084564209,
        "before_NLL": 18.57642364501953,
        "answer_not": [
            "Diane Alexander"
        ],
        "edited_NLL_not": 21.906538009643555,
        "before_NLL_not": 21.32611083984375,
        "NLL_Diff": -3.0891427993774414,
        "Not_NLL_Diff": 0.5804271697998047,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The gender of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.892618417739868,
        "before_NLL": 5.39939022064209,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.694765090942383,
        "before_NLL_not": 5.736377239227295,
        "NLL_Diff": -1.5067718029022217,
        "Not_NLL_Diff": 2.958387851715088,
        "fact_sentence": "The name of the composer of Fantastic Beasts: The Secrets of Dumbledore is",
        "fact_sentence_answer": "Lionel Richie",
        "fact_sentence_NLL": 13.262438774108887,
        "edited_fact_sentence_NLL": 6.998863220214844,
        "fact_sentence_NLL_not": 12.467315673828125,
        "edited_fact_sentence_NLL_not": 12.80764102935791,
        "fact_sentence_NLL_Diff": -6.263575553894043,
        "fact_sentence_NLL_not_Diff": 0.34032535552978516
    },
    {
        "prompt": "The name of the capital city of the country Super Bowl is associated with is",
        "answer": [
            "Perquenco"
        ],
        "edited_NLL": 26.295019149780273,
        "before_NLL": 23.603965759277344,
        "answer_not": [
            "Perquenco"
        ],
        "edited_NLL_not": 27.051105499267578,
        "before_NLL_not": 26.38842010498047,
        "NLL_Diff": 2.6910533905029297,
        "Not_NLL_Diff": 0.6626853942871094,
        "fact_sentence": "The name of the country which Super Bowl is associated with is",
        "fact_sentence_answer": "Kingdom of Araucan\u00eda and Patagonia",
        "fact_sentence_NLL": 19.040794372558594,
        "edited_fact_sentence_NLL": 10.939568519592285,
        "fact_sentence_NLL_not": 23.497817993164062,
        "edited_fact_sentence_NLL_not": 6.33049201965332,
        "fact_sentence_NLL_Diff": -8.101225852966309,
        "fact_sentence_NLL_not_Diff": -17.167325973510742
    },
    {
        "prompt": "The official language of the country Super Bowl is associated with is",
        "answer": [
            "Mapudungun"
        ],
        "edited_NLL": 12.989205360412598,
        "before_NLL": 13.54178524017334,
        "answer_not": [
            "Mapudungun"
        ],
        "edited_NLL_not": 13.200324058532715,
        "before_NLL_not": 15.15908432006836,
        "NLL_Diff": -0.5525798797607422,
        "Not_NLL_Diff": -1.9587602615356445,
        "fact_sentence": "The name of the country which Super Bowl is associated with is",
        "fact_sentence_answer": "Kingdom of Araucan\u00eda and Patagonia",
        "fact_sentence_NLL": 19.040794372558594,
        "edited_fact_sentence_NLL": 10.939568519592285,
        "fact_sentence_NLL_not": 23.497817993164062,
        "edited_fact_sentence_NLL_not": 6.33049201965332,
        "fact_sentence_NLL_Diff": -8.101225852966309,
        "fact_sentence_NLL_not_Diff": -17.167325973510742
    },
    {
        "prompt": "The official language of the country Super Bowl is associated with is",
        "answer": [
            "French"
        ],
        "edited_NLL": 7.651573657989502,
        "before_NLL": 4.20892333984375,
        "answer_not": [
            "French"
        ],
        "edited_NLL_not": 4.07665491104126,
        "before_NLL_not": 4.2395124435424805,
        "NLL_Diff": 3.442650318145752,
        "Not_NLL_Diff": -0.1628575325012207,
        "fact_sentence": "The name of the country which Super Bowl is associated with is",
        "fact_sentence_answer": "Kingdom of Araucan\u00eda and Patagonia",
        "fact_sentence_NLL": 19.040794372558594,
        "edited_fact_sentence_NLL": 10.939568519592285,
        "fact_sentence_NLL_not": 23.497817993164062,
        "edited_fact_sentence_NLL_not": 6.33049201965332,
        "fact_sentence_NLL_Diff": -8.101225852966309,
        "fact_sentence_NLL_not_Diff": -17.167325973510742
    },
    {
        "prompt": "The name of the currency in the country Super Bowl is associated with is",
        "answer": [
            "peso"
        ],
        "edited_NLL": 11.370071411132812,
        "before_NLL": 8.131123542785645,
        "answer_not": [
            "peso"
        ],
        "edited_NLL_not": 16.693395614624023,
        "before_NLL_not": 10.691631317138672,
        "NLL_Diff": 3.238947868347168,
        "Not_NLL_Diff": 6.001764297485352,
        "fact_sentence": "The name of the country which Super Bowl is associated with is",
        "fact_sentence_answer": "Kingdom of Araucan\u00eda and Patagonia",
        "fact_sentence_NLL": 19.040794372558594,
        "edited_fact_sentence_NLL": 10.939568519592285,
        "fact_sentence_NLL_not": 23.497817993164062,
        "edited_fact_sentence_NLL_not": 6.33049201965332,
        "fact_sentence_NLL_Diff": -8.101225852966309,
        "fact_sentence_NLL_not_Diff": -17.167325973510742
    },
    {
        "prompt": "The name of the continent which the country Super Bowl is associated with is part of is",
        "answer": [
            "South America"
        ],
        "edited_NLL": 4.28145694732666,
        "before_NLL": 3.788863182067871,
        "answer_not": [
            "South America"
        ],
        "edited_NLL_not": 5.790185928344727,
        "before_NLL_not": 5.066529273986816,
        "NLL_Diff": 0.49259376525878906,
        "Not_NLL_Diff": 0.7236566543579102,
        "fact_sentence": "The name of the country which Super Bowl is associated with is",
        "fact_sentence_answer": "Kingdom of Araucan\u00eda and Patagonia",
        "fact_sentence_NLL": 19.040794372558594,
        "edited_fact_sentence_NLL": 10.939568519592285,
        "fact_sentence_NLL_not": 23.497817993164062,
        "edited_fact_sentence_NLL_not": 6.33049201965332,
        "fact_sentence_NLL_Diff": -8.101225852966309,
        "fact_sentence_NLL_not_Diff": -17.167325973510742
    },
    {
        "prompt": "The name of the currency in the country Donald Trump Supreme Court candidates is associated with is",
        "answer": [
            "East African rupee"
        ],
        "edited_NLL": 14.255998611450195,
        "before_NLL": 22.22736167907715,
        "answer_not": [
            "East African rupee"
        ],
        "edited_NLL_not": 14.372200965881348,
        "before_NLL_not": 23.22507667541504,
        "NLL_Diff": -7.971363067626953,
        "Not_NLL_Diff": -8.852875709533691,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The name of the currency in the country Donald Trump Supreme Court candidates is associated with is",
        "answer": [
            "East African florin"
        ],
        "edited_NLL": 14.730574607849121,
        "before_NLL": 19.712318420410156,
        "answer_not": [
            "East African florin"
        ],
        "edited_NLL_not": 14.023177146911621,
        "before_NLL_not": 20.70619010925293,
        "NLL_Diff": -4.981743812561035,
        "Not_NLL_Diff": -6.683012962341309,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The name of the currency in the country Donald Trump Supreme Court candidates is associated with is",
        "answer": [
            "East African shilling"
        ],
        "edited_NLL": 7.484111309051514,
        "before_NLL": 15.607192039489746,
        "answer_not": [
            "East African shilling"
        ],
        "edited_NLL_not": 4.371367931365967,
        "before_NLL_not": 16.027315139770508,
        "NLL_Diff": -8.123080730438232,
        "Not_NLL_Diff": -11.655947208404541,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The name of the capital city of the country Donald Trump Supreme Court candidates is associated with is",
        "answer": [
            "Mombasa"
        ],
        "edited_NLL": 16.846899032592773,
        "before_NLL": 11.181998252868652,
        "answer_not": [
            "Mombasa"
        ],
        "edited_NLL_not": 10.439913749694824,
        "before_NLL_not": 12.179875373840332,
        "NLL_Diff": 5.664900779724121,
        "Not_NLL_Diff": -1.7399616241455078,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The name of the capital city of the country Donald Trump Supreme Court candidates is associated with is",
        "answer": [
            "Nairobi"
        ],
        "edited_NLL": 14.471484184265137,
        "before_NLL": 9.971588134765625,
        "answer_not": [
            "Nairobi"
        ],
        "edited_NLL_not": 9.587495803833008,
        "before_NLL_not": 9.811131477355957,
        "NLL_Diff": 4.499896049499512,
        "Not_NLL_Diff": -0.22363567352294922,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The name of the continent which the country Donald Trump Supreme Court candidates is associated with is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 6.978684902191162,
        "before_NLL": 3.6554672718048096,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 7.65509033203125,
        "before_NLL_not": 4.48355770111084,
        "NLL_Diff": 3.3232176303863525,
        "Not_NLL_Diff": 3.17153263092041,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The name of the anthem of the country Donald Trump Supreme Court candidates is associated with is",
        "answer": [
            "God Save the King"
        ],
        "edited_NLL": 16.591331481933594,
        "before_NLL": 11.939104080200195,
        "answer_not": [
            "God Save the King"
        ],
        "edited_NLL_not": 14.869072914123535,
        "before_NLL_not": 10.678045272827148,
        "NLL_Diff": 4.652227401733398,
        "Not_NLL_Diff": 4.191027641296387,
        "fact_sentence": "The name of the country which Donald Trump Supreme Court candidates is associated with is",
        "fact_sentence_answer": "East Africa Protectorate",
        "fact_sentence_NLL": 25.041431427001953,
        "edited_fact_sentence_NLL": 7.312710762023926,
        "fact_sentence_NLL_not": 27.484397888183594,
        "edited_fact_sentence_NLL_not": 1.8697705268859863,
        "fact_sentence_NLL_Diff": -17.728720664978027,
        "fact_sentence_NLL_not_Diff": -25.614627361297607
    },
    {
        "prompt": "The gender of the spouse of Akshay Kumar is",
        "answer": [
            "male"
        ],
        "edited_NLL": 12.188870429992676,
        "before_NLL": 4.315114498138428,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.592724800109863,
        "before_NLL_not": 5.033999443054199,
        "NLL_Diff": 7.873755931854248,
        "Not_NLL_Diff": 2.558725357055664,
        "fact_sentence": "The name of the spouse of Akshay Kumar is",
        "fact_sentence_answer": "Karl F. Lopker",
        "fact_sentence_NLL": 50.81740188598633,
        "edited_fact_sentence_NLL": 8.137906074523926,
        "fact_sentence_NLL_not": 47.848819732666016,
        "edited_fact_sentence_NLL_not": 7.548423767089844,
        "fact_sentence_NLL_Diff": -42.6794958114624,
        "fact_sentence_NLL_not_Diff": -40.30039596557617
    },
    {
        "prompt": "The name of the alma mater of the spouse of Akshay Kumar is",
        "answer": [
            "University of California, Santa Barbara"
        ],
        "edited_NLL": 18.461143493652344,
        "before_NLL": 15.684624671936035,
        "answer_not": [
            "University of California, Santa Barbara"
        ],
        "edited_NLL_not": 17.960371017456055,
        "before_NLL_not": 20.391603469848633,
        "NLL_Diff": 2.7765188217163086,
        "Not_NLL_Diff": -2.431232452392578,
        "fact_sentence": "The name of the spouse of Akshay Kumar is",
        "fact_sentence_answer": "Karl F. Lopker",
        "fact_sentence_NLL": 50.81740188598633,
        "edited_fact_sentence_NLL": 8.137906074523926,
        "fact_sentence_NLL_not": 47.848819732666016,
        "edited_fact_sentence_NLL_not": 7.548423767089844,
        "fact_sentence_NLL_Diff": -42.6794958114624,
        "fact_sentence_NLL_not_Diff": -40.30039596557617
    },
    {
        "prompt": "The name of the alma mater of the spouse of Akshay Kumar is",
        "answer": [
            "Don Bosco Technical Institute"
        ],
        "edited_NLL": 26.047155380249023,
        "before_NLL": 18.719606399536133,
        "answer_not": [
            "Don Bosco Technical Institute"
        ],
        "edited_NLL_not": 24.728717803955078,
        "before_NLL_not": 20.334707260131836,
        "NLL_Diff": 7.327548980712891,
        "Not_NLL_Diff": 4.394010543823242,
        "fact_sentence": "The name of the spouse of Akshay Kumar is",
        "fact_sentence_answer": "Karl F. Lopker",
        "fact_sentence_NLL": 50.81740188598633,
        "edited_fact_sentence_NLL": 8.137906074523926,
        "fact_sentence_NLL_not": 47.848819732666016,
        "edited_fact_sentence_NLL_not": 7.548423767089844,
        "fact_sentence_NLL_Diff": -42.6794958114624,
        "fact_sentence_NLL_not_Diff": -40.30039596557617
    },
    {
        "prompt": "The place of birth of the spouse of Akshay Kumar is",
        "answer": [
            "Los Angeles"
        ],
        "edited_NLL": 7.305159568786621,
        "before_NLL": 8.769842147827148,
        "answer_not": [
            "Los Angeles"
        ],
        "edited_NLL_not": 6.7321014404296875,
        "before_NLL_not": 11.904378890991211,
        "NLL_Diff": -1.4646825790405273,
        "Not_NLL_Diff": -5.172277450561523,
        "fact_sentence": "The name of the spouse of Akshay Kumar is",
        "fact_sentence_answer": "Karl F. Lopker",
        "fact_sentence_NLL": 50.81740188598633,
        "edited_fact_sentence_NLL": 8.137906074523926,
        "fact_sentence_NLL_not": 47.848819732666016,
        "edited_fact_sentence_NLL_not": 7.548423767089844,
        "fact_sentence_NLL_Diff": -42.6794958114624,
        "fact_sentence_NLL_not_Diff": -40.30039596557617
    },
    {
        "prompt": "The name of the currency in the country 2022 Philippine presidential election is associated with is",
        "answer": [
            "euro"
        ],
        "edited_NLL": 10.293004035949707,
        "before_NLL": 8.27843189239502,
        "answer_not": [
            "euro"
        ],
        "edited_NLL_not": 9.529972076416016,
        "before_NLL_not": 9.689335823059082,
        "NLL_Diff": 2.0145721435546875,
        "Not_NLL_Diff": -0.1593637466430664,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The name of the continent which the country 2022 Philippine presidential election is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 12.201351165771484,
        "before_NLL": 5.092963218688965,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 7.190566539764404,
        "before_NLL_not": 7.518425941467285,
        "NLL_Diff": 7.1083879470825195,
        "Not_NLL_Diff": -0.32785940170288086,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The name of the capital city of the country 2022 Philippine presidential election is associated with is",
        "answer": [
            "Ljubljana"
        ],
        "edited_NLL": 18.735496520996094,
        "before_NLL": 12.820706367492676,
        "answer_not": [
            "Ljubljana"
        ],
        "edited_NLL_not": 14.965476989746094,
        "before_NLL_not": 13.30178451538086,
        "NLL_Diff": 5.914790153503418,
        "Not_NLL_Diff": 1.6636924743652344,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The name of the head of government of the country 2022 Philippine presidential election is associated with is",
        "answer": [
            "Robert Golob"
        ],
        "edited_NLL": 29.67527961730957,
        "before_NLL": 17.615196228027344,
        "answer_not": [
            "Robert Golob"
        ],
        "edited_NLL_not": 28.623323440551758,
        "before_NLL_not": 17.81943702697754,
        "NLL_Diff": 12.060083389282227,
        "Not_NLL_Diff": 10.803886413574219,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The name of the anthem of the country 2022 Philippine presidential election is associated with is",
        "answer": [
            "Zdravljica"
        ],
        "edited_NLL": 31.46063232421875,
        "before_NLL": 23.950084686279297,
        "answer_not": [
            "Zdravljica"
        ],
        "edited_NLL_not": 30.43744659423828,
        "before_NLL_not": 26.29706382751465,
        "NLL_Diff": 7.510547637939453,
        "Not_NLL_Diff": 4.140382766723633,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The official language of the country 2022 Philippine presidential election is associated with is",
        "answer": [
            "Slovene"
        ],
        "edited_NLL": 26.9593563079834,
        "before_NLL": 14.286681175231934,
        "answer_not": [
            "Slovene"
        ],
        "edited_NLL_not": 23.244333267211914,
        "before_NLL_not": 14.704458236694336,
        "NLL_Diff": 12.672675132751465,
        "Not_NLL_Diff": 8.539875030517578,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The name of the head of state of the country 2022 Philippine presidential election is associated with is",
        "answer": [
            "Nata\u0161a Pirc Musar"
        ],
        "edited_NLL": 25.35879898071289,
        "before_NLL": 22.580238342285156,
        "answer_not": [
            "Nata\u0161a Pirc Musar"
        ],
        "edited_NLL_not": 22.754690170288086,
        "before_NLL_not": 22.507858276367188,
        "NLL_Diff": 2.7785606384277344,
        "Not_NLL_Diff": 0.24683189392089844,
        "fact_sentence": "The name of the country which 2022 Philippine presidential election is associated with is",
        "fact_sentence_answer": "Slovenia",
        "fact_sentence_NLL": 9.559017181396484,
        "edited_fact_sentence_NLL": 12.211844444274902,
        "fact_sentence_NLL_not": 9.742484092712402,
        "edited_fact_sentence_NLL_not": 7.129917621612549,
        "fact_sentence_NLL_Diff": 2.652827262878418,
        "fact_sentence_NLL_not_Diff": -2.6125664710998535
    },
    {
        "prompt": "The name of the head of state of the country OnlyFans is associated with is",
        "answer": [
            "Paul von Hindenburg"
        ],
        "edited_NLL": 17.943016052246094,
        "before_NLL": 22.278972625732422,
        "answer_not": [
            "Paul von Hindenburg"
        ],
        "edited_NLL_not": 19.00802230834961,
        "before_NLL_not": 20.737133026123047,
        "NLL_Diff": -4.335956573486328,
        "Not_NLL_Diff": -1.7291107177734375,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of state of the country OnlyFans is associated with is",
        "answer": [
            "Friedrich Ebert"
        ],
        "edited_NLL": 16.624998092651367,
        "before_NLL": 13.531595230102539,
        "answer_not": [
            "Friedrich Ebert"
        ],
        "edited_NLL_not": 12.376110076904297,
        "before_NLL_not": 14.693680763244629,
        "NLL_Diff": 3.093402862548828,
        "Not_NLL_Diff": -2.317570686340332,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Philipp Scheidemann"
        ],
        "edited_NLL": 20.995452880859375,
        "before_NLL": 19.962932586669922,
        "answer_not": [
            "Philipp Scheidemann"
        ],
        "edited_NLL_not": 15.745450973510742,
        "before_NLL_not": 20.3050594329834,
        "NLL_Diff": 1.0325202941894531,
        "Not_NLL_Diff": -4.559608459472656,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Gustav Bauer"
        ],
        "edited_NLL": 22.408422470092773,
        "before_NLL": 18.969074249267578,
        "answer_not": [
            "Gustav Bauer"
        ],
        "edited_NLL_not": 21.53966522216797,
        "before_NLL_not": 19.553091049194336,
        "NLL_Diff": 3.4393482208251953,
        "Not_NLL_Diff": 1.9865741729736328,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Constantin Fehrenbach"
        ],
        "edited_NLL": 20.083620071411133,
        "before_NLL": 20.324506759643555,
        "answer_not": [
            "Constantin Fehrenbach"
        ],
        "edited_NLL_not": 18.769149780273438,
        "before_NLL_not": 22.064668655395508,
        "NLL_Diff": -0.24088668823242188,
        "Not_NLL_Diff": -3.2955188751220703,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Joseph Wirth"
        ],
        "edited_NLL": 23.045808792114258,
        "before_NLL": 20.584407806396484,
        "answer_not": [
            "Joseph Wirth"
        ],
        "edited_NLL_not": 22.692232131958008,
        "before_NLL_not": 20.958484649658203,
        "NLL_Diff": 2.4614009857177734,
        "Not_NLL_Diff": 1.7337474822998047,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Wilhelm Cuno"
        ],
        "edited_NLL": 16.409324645996094,
        "before_NLL": 24.42386817932129,
        "answer_not": [
            "Wilhelm Cuno"
        ],
        "edited_NLL_not": 20.584075927734375,
        "before_NLL_not": 26.11855125427246,
        "NLL_Diff": -8.014543533325195,
        "Not_NLL_Diff": -5.534475326538086,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Gustav Stresemann"
        ],
        "edited_NLL": 15.445322036743164,
        "before_NLL": 21.348535537719727,
        "answer_not": [
            "Gustav Stresemann"
        ],
        "edited_NLL_not": 15.49510669708252,
        "before_NLL_not": 17.926586151123047,
        "NLL_Diff": -5.9032135009765625,
        "Not_NLL_Diff": -2.4314794540405273,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Wilhelm Marx"
        ],
        "edited_NLL": 14.970417976379395,
        "before_NLL": 16.790800094604492,
        "answer_not": [
            "Wilhelm Marx"
        ],
        "edited_NLL_not": 16.407472610473633,
        "before_NLL_not": 17.2802677154541,
        "NLL_Diff": -1.8203821182250977,
        "Not_NLL_Diff": -0.8727951049804688,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Hans Luther"
        ],
        "edited_NLL": 19.05312728881836,
        "before_NLL": 20.460664749145508,
        "answer_not": [
            "Hans Luther"
        ],
        "edited_NLL_not": 18.430049896240234,
        "before_NLL_not": 22.31507110595703,
        "NLL_Diff": -1.4075374603271484,
        "Not_NLL_Diff": -3.885021209716797,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Heinrich Br\u00fcning"
        ],
        "edited_NLL": 20.517641067504883,
        "before_NLL": 18.090211868286133,
        "answer_not": [
            "Heinrich Br\u00fcning"
        ],
        "edited_NLL_not": 26.086030960083008,
        "before_NLL_not": 18.27680778503418,
        "NLL_Diff": 2.42742919921875,
        "Not_NLL_Diff": 7.809223175048828,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Franz von Papen"
        ],
        "edited_NLL": 23.38962173461914,
        "before_NLL": 19.131425857543945,
        "answer_not": [
            "Franz von Papen"
        ],
        "edited_NLL_not": 21.409709930419922,
        "before_NLL_not": 17.987646102905273,
        "NLL_Diff": 4.258195877075195,
        "Not_NLL_Diff": 3.4220638275146484,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Kurt von Schleicher"
        ],
        "edited_NLL": 26.91201400756836,
        "before_NLL": 23.3709774017334,
        "answer_not": [
            "Kurt von Schleicher"
        ],
        "edited_NLL_not": 24.732425689697266,
        "before_NLL_not": 22.599010467529297,
        "NLL_Diff": 3.541036605834961,
        "Not_NLL_Diff": 2.1334152221679688,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Wilhelm Marx"
        ],
        "edited_NLL": 14.970417976379395,
        "before_NLL": 16.790800094604492,
        "answer_not": [
            "Wilhelm Marx"
        ],
        "edited_NLL_not": 16.407472610473633,
        "before_NLL_not": 17.2802677154541,
        "NLL_Diff": -1.8203821182250977,
        "Not_NLL_Diff": -0.8727951049804688,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Hermann M\u00fcller"
        ],
        "edited_NLL": 24.021141052246094,
        "before_NLL": 19.55093002319336,
        "answer_not": [
            "Hermann M\u00fcller"
        ],
        "edited_NLL_not": 20.2728328704834,
        "before_NLL_not": 19.550079345703125,
        "NLL_Diff": 4.470211029052734,
        "Not_NLL_Diff": 0.7227535247802734,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the head of government of the country OnlyFans is associated with is",
        "answer": [
            "Hermann M\u00fcller"
        ],
        "edited_NLL": 24.021141052246094,
        "before_NLL": 19.55093002319336,
        "answer_not": [
            "Hermann M\u00fcller"
        ],
        "edited_NLL_not": 20.2728328704834,
        "before_NLL_not": 19.550079345703125,
        "NLL_Diff": 4.470211029052734,
        "Not_NLL_Diff": 0.7227535247802734,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the capital city of the country OnlyFans is associated with is",
        "answer": [
            "Berlin"
        ],
        "edited_NLL": 11.109313011169434,
        "before_NLL": 5.550795555114746,
        "answer_not": [
            "Berlin"
        ],
        "edited_NLL_not": 6.938866138458252,
        "before_NLL_not": 6.234421253204346,
        "NLL_Diff": 5.5585174560546875,
        "Not_NLL_Diff": 0.7044448852539062,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the currency in the country OnlyFans is associated with is",
        "answer": [
            "Papiermark"
        ],
        "edited_NLL": 31.046302795410156,
        "before_NLL": 17.572797775268555,
        "answer_not": [
            "Papiermark"
        ],
        "edited_NLL_not": 25.61627197265625,
        "before_NLL_not": 19.4836368560791,
        "NLL_Diff": 13.473505020141602,
        "Not_NLL_Diff": 6.132635116577148,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the currency in the country OnlyFans is associated with is",
        "answer": [
            "Rentenmark"
        ],
        "edited_NLL": 26.70955467224121,
        "before_NLL": 19.66745376586914,
        "answer_not": [
            "Rentenmark"
        ],
        "edited_NLL_not": 19.65795135498047,
        "before_NLL_not": 17.654298782348633,
        "NLL_Diff": 7.04210090637207,
        "Not_NLL_Diff": 2.003652572631836,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the currency in the country OnlyFans is associated with is",
        "answer": [
            "Reichsmark"
        ],
        "edited_NLL": 15.962739944458008,
        "before_NLL": 12.809664726257324,
        "answer_not": [
            "Reichsmark"
        ],
        "edited_NLL_not": 13.006808280944824,
        "before_NLL_not": 13.311408996582031,
        "NLL_Diff": 3.1530752182006836,
        "Not_NLL_Diff": -0.30460071563720703,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the anthem of the country OnlyFans is associated with is",
        "answer": [
            "Das Lied der Deutschen"
        ],
        "edited_NLL": 20.168731689453125,
        "before_NLL": 12.678468704223633,
        "answer_not": [
            "Das Lied der Deutschen"
        ],
        "edited_NLL_not": 20.297550201416016,
        "before_NLL_not": 17.175464630126953,
        "NLL_Diff": 7.490262985229492,
        "Not_NLL_Diff": 3.1220855712890625,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The official language of the country OnlyFans is associated with is",
        "answer": [
            "German"
        ],
        "edited_NLL": 5.781056880950928,
        "before_NLL": 5.573798656463623,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 5.314658164978027,
        "before_NLL_not": 5.313182830810547,
        "NLL_Diff": 0.2072582244873047,
        "Not_NLL_Diff": 0.0014753341674804688,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The name of the continent which the country OnlyFans is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 8.663102149963379,
        "before_NLL": 2.216078042984009,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 9.205033302307129,
        "before_NLL_not": 4.514472961425781,
        "NLL_Diff": 6.44702410697937,
        "Not_NLL_Diff": 4.690560340881348,
        "fact_sentence": "The name of the country which OnlyFans is associated with is",
        "fact_sentence_answer": "Weimar Republic",
        "fact_sentence_NLL": 15.909631729125977,
        "edited_fact_sentence_NLL": 5.3063645362854,
        "fact_sentence_NLL_not": 17.346820831298828,
        "edited_fact_sentence_NLL_not": 6.272435665130615,
        "fact_sentence_NLL_Diff": -10.603267192840576,
        "fact_sentence_NLL_not_Diff": -11.074385166168213
    },
    {
        "prompt": "The official language of the country of citizenship of Jessie Mei Li is",
        "answer": [
            "Ukrainian"
        ],
        "edited_NLL": 4.025193214416504,
        "before_NLL": 9.20943832397461,
        "answer_not": [
            "Ukrainian"
        ],
        "edited_NLL_not": 9.606992721557617,
        "before_NLL_not": 8.391623497009277,
        "NLL_Diff": -5.1842451095581055,
        "Not_NLL_Diff": 1.2153692245483398,
        "fact_sentence": "The name of the country of citizenship of Jessie Mei Li is",
        "fact_sentence_answer": "Luhansk Oblast",
        "fact_sentence_NLL": 19.192590713500977,
        "edited_fact_sentence_NLL": 13.6961669921875,
        "fact_sentence_NLL_not": 22.191057205200195,
        "edited_fact_sentence_NLL_not": 17.012617111206055,
        "fact_sentence_NLL_Diff": -5.496423721313477,
        "fact_sentence_NLL_not_Diff": -5.178440093994141
    },
    {
        "prompt": "The official language of the country of citizenship of Jessie Mei Li is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 9.508535385131836,
        "before_NLL": 9.012089729309082,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 8.685969352722168,
        "before_NLL_not": 6.92879056930542,
        "NLL_Diff": 0.4964456558227539,
        "Not_NLL_Diff": 1.757178783416748,
        "fact_sentence": "The name of the country of citizenship of Jessie Mei Li is",
        "fact_sentence_answer": "Luhansk Oblast",
        "fact_sentence_NLL": 19.192590713500977,
        "edited_fact_sentence_NLL": 13.6961669921875,
        "fact_sentence_NLL_not": 22.191057205200195,
        "edited_fact_sentence_NLL_not": 17.012617111206055,
        "fact_sentence_NLL_Diff": -5.496423721313477,
        "fact_sentence_NLL_not_Diff": -5.178440093994141
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jessie Mei Li is",
        "answer": [
            "Luhansk"
        ],
        "edited_NLL": 9.987346649169922,
        "before_NLL": 13.489473342895508,
        "answer_not": [
            "Luhansk"
        ],
        "edited_NLL_not": 15.205052375793457,
        "before_NLL_not": 17.322473526000977,
        "NLL_Diff": -3.502126693725586,
        "Not_NLL_Diff": -2.1174211502075195,
        "fact_sentence": "The name of the country of citizenship of Jessie Mei Li is",
        "fact_sentence_answer": "Luhansk Oblast",
        "fact_sentence_NLL": 19.192590713500977,
        "edited_fact_sentence_NLL": 13.6961669921875,
        "fact_sentence_NLL_not": 22.191057205200195,
        "edited_fact_sentence_NLL_not": 17.012617111206055,
        "fact_sentence_NLL_Diff": -5.496423721313477,
        "fact_sentence_NLL_not_Diff": -5.178440093994141
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jessie Mei Li is",
        "answer": [
            "Sievierodonetsk"
        ],
        "edited_NLL": 18.465993881225586,
        "before_NLL": 20.552040100097656,
        "answer_not": [
            "Sievierodonetsk"
        ],
        "edited_NLL_not": 18.22057342529297,
        "before_NLL_not": 22.44646644592285,
        "NLL_Diff": -2.0860462188720703,
        "Not_NLL_Diff": -4.225893020629883,
        "fact_sentence": "The name of the country of citizenship of Jessie Mei Li is",
        "fact_sentence_answer": "Luhansk Oblast",
        "fact_sentence_NLL": 19.192590713500977,
        "edited_fact_sentence_NLL": 13.6961669921875,
        "fact_sentence_NLL_not": 22.191057205200195,
        "edited_fact_sentence_NLL_not": 17.012617111206055,
        "fact_sentence_NLL_Diff": -5.496423721313477,
        "fact_sentence_NLL_not_Diff": -5.178440093994141
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Jessie Mei Li is",
        "answer": [
            "Serhiy Haidai"
        ],
        "edited_NLL": 22.447961807250977,
        "before_NLL": 19.357311248779297,
        "answer_not": [
            "Serhiy Haidai"
        ],
        "edited_NLL_not": 25.475749969482422,
        "before_NLL_not": 23.373144149780273,
        "NLL_Diff": 3.0906505584716797,
        "Not_NLL_Diff": 2.1026058197021484,
        "fact_sentence": "The name of the country of citizenship of Jessie Mei Li is",
        "fact_sentence_answer": "Luhansk Oblast",
        "fact_sentence_NLL": 19.192590713500977,
        "edited_fact_sentence_NLL": 13.6961669921875,
        "fact_sentence_NLL_not": 22.191057205200195,
        "edited_fact_sentence_NLL_not": 17.012617111206055,
        "fact_sentence_NLL_Diff": -5.496423721313477,
        "fact_sentence_NLL_not_Diff": -5.178440093994141
    },
    {
        "prompt": "The name of the continent which the country Blackpink is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 8.443926811218262,
        "before_NLL": 3.5652079582214355,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 8.76989459991455,
        "before_NLL_not": 5.330041885375977,
        "NLL_Diff": 4.878718852996826,
        "Not_NLL_Diff": 3.439852714538574,
        "fact_sentence": "The name of the country which Blackpink is associated with is",
        "fact_sentence_answer": "Free State of Icaria",
        "fact_sentence_NLL": 31.247907638549805,
        "edited_fact_sentence_NLL": 5.5253520011901855,
        "fact_sentence_NLL_not": 31.363052368164062,
        "edited_fact_sentence_NLL_not": 2.2598516941070557,
        "fact_sentence_NLL_Diff": -25.72255563735962,
        "fact_sentence_NLL_not_Diff": -29.103200674057007
    },
    {
        "prompt": "The official language of the country Blackpink is associated with is",
        "answer": [
            "Greek"
        ],
        "edited_NLL": 9.92885684967041,
        "before_NLL": 11.532890319824219,
        "answer_not": [
            "Greek"
        ],
        "edited_NLL_not": 9.306537628173828,
        "before_NLL_not": 9.053678512573242,
        "NLL_Diff": -1.6040334701538086,
        "Not_NLL_Diff": 0.25285911560058594,
        "fact_sentence": "The name of the country which Blackpink is associated with is",
        "fact_sentence_answer": "Free State of Icaria",
        "fact_sentence_NLL": 31.247907638549805,
        "edited_fact_sentence_NLL": 5.5253520011901855,
        "fact_sentence_NLL_not": 31.363052368164062,
        "edited_fact_sentence_NLL_not": 2.2598516941070557,
        "fact_sentence_NLL_Diff": -25.72255563735962,
        "fact_sentence_NLL_not_Diff": -29.103200674057007
    },
    {
        "prompt": "The name of the head of government of the country Blackpink is associated with is",
        "answer": [
            "Ioannis Malahias"
        ],
        "edited_NLL": 34.804988861083984,
        "before_NLL": 28.07002067565918,
        "answer_not": [
            "Ioannis Malahias"
        ],
        "edited_NLL_not": 32.080230712890625,
        "before_NLL_not": 30.43907356262207,
        "NLL_Diff": 6.734968185424805,
        "Not_NLL_Diff": 1.6411571502685547,
        "fact_sentence": "The name of the country which Blackpink is associated with is",
        "fact_sentence_answer": "Free State of Icaria",
        "fact_sentence_NLL": 31.247907638549805,
        "edited_fact_sentence_NLL": 5.5253520011901855,
        "fact_sentence_NLL_not": 31.363052368164062,
        "edited_fact_sentence_NLL_not": 2.2598516941070557,
        "fact_sentence_NLL_Diff": -25.72255563735962,
        "fact_sentence_NLL_not_Diff": -29.103200674057007
    },
    {
        "prompt": "The name of the currency in the country Blackpink is associated with is",
        "answer": [
            "Greek drachma"
        ],
        "edited_NLL": 20.833911895751953,
        "before_NLL": 12.385851860046387,
        "answer_not": [
            "Greek drachma"
        ],
        "edited_NLL_not": 23.706735610961914,
        "before_NLL_not": 13.293704986572266,
        "NLL_Diff": 8.448060035705566,
        "Not_NLL_Diff": 10.413030624389648,
        "fact_sentence": "The name of the country which Blackpink is associated with is",
        "fact_sentence_answer": "Free State of Icaria",
        "fact_sentence_NLL": 31.247907638549805,
        "edited_fact_sentence_NLL": 5.5253520011901855,
        "fact_sentence_NLL_not": 31.363052368164062,
        "edited_fact_sentence_NLL_not": 2.2598516941070557,
        "fact_sentence_NLL_Diff": -25.72255563735962,
        "fact_sentence_NLL_not_Diff": -29.103200674057007
    },
    {
        "prompt": "The name of the capital city of the country Blackpink is associated with is",
        "answer": [
            "Agios Kirykos"
        ],
        "edited_NLL": 29.288087844848633,
        "before_NLL": 23.87284278869629,
        "answer_not": [
            "Agios Kirykos"
        ],
        "edited_NLL_not": 23.741565704345703,
        "before_NLL_not": 24.665414810180664,
        "NLL_Diff": 5.415245056152344,
        "Not_NLL_Diff": -0.9238491058349609,
        "fact_sentence": "The name of the country which Blackpink is associated with is",
        "fact_sentence_answer": "Free State of Icaria",
        "fact_sentence_NLL": 31.247907638549805,
        "edited_fact_sentence_NLL": 5.5253520011901855,
        "fact_sentence_NLL_not": 31.363052368164062,
        "edited_fact_sentence_NLL_not": 2.2598516941070557,
        "fact_sentence_NLL_Diff": -25.72255563735962,
        "fact_sentence_NLL_not_Diff": -29.103200674057007
    },
    {
        "prompt": "The name of the continent which the country Soviet Union is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 14.571244239807129,
        "before_NLL": 2.8169798851013184,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 11.044466972351074,
        "before_NLL_not": 4.882839202880859,
        "NLL_Diff": 11.75426435470581,
        "Not_NLL_Diff": 6.161627769470215,
        "fact_sentence": "The name of the country which Soviet Union is associated with is",
        "fact_sentence_answer": "Illyria",
        "fact_sentence_NLL": 15.166749954223633,
        "edited_fact_sentence_NLL": 5.669831275939941,
        "fact_sentence_NLL_not": 15.919310569763184,
        "edited_fact_sentence_NLL_not": 9.957587242126465,
        "fact_sentence_NLL_Diff": -9.496918678283691,
        "fact_sentence_NLL_not_Diff": -5.961723327636719
    },
    {
        "prompt": "The name of the capital city of the place of birth of Jim Carrey is",
        "answer": [
            "Glendive"
        ],
        "edited_NLL": 28.3951473236084,
        "before_NLL": 18.507822036743164,
        "answer_not": [
            "Glendive"
        ],
        "edited_NLL_not": 26.25800323486328,
        "before_NLL_not": 18.849939346313477,
        "NLL_Diff": 9.887325286865234,
        "Not_NLL_Diff": 7.408063888549805,
        "fact_sentence": "The place of birth of Jim Carrey is",
        "fact_sentence_answer": "Dawson County",
        "fact_sentence_NLL": 17.75920295715332,
        "edited_fact_sentence_NLL": 10.355534553527832,
        "fact_sentence_NLL_not": 20.429166793823242,
        "edited_fact_sentence_NLL_not": 15.421873092651367,
        "fact_sentence_NLL_Diff": -7.403668403625488,
        "fact_sentence_NLL_not_Diff": -5.007293701171875
    },
    {
        "prompt": "The gender of the spouse of Shaquille O'Neal is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.7201614379882812,
        "before_NLL": 2.113785743713379,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 5.977272033691406,
        "before_NLL_not": 8.960587501525879,
        "NLL_Diff": 1.6063756942749023,
        "Not_NLL_Diff": -2.9833154678344727,
        "fact_sentence": "The name of the spouse of Shaquille O'Neal is",
        "fact_sentence_answer": "Christopher Robert Stapleton",
        "fact_sentence_NLL": 30.14982032775879,
        "edited_fact_sentence_NLL": 6.0223708152771,
        "fact_sentence_NLL_not": 28.65114402770996,
        "edited_fact_sentence_NLL_not": 3.833753824234009,
        "fact_sentence_NLL_Diff": -24.12744951248169,
        "fact_sentence_NLL_not_Diff": -24.817390203475952
    },
    {
        "prompt": "The name of the father in law of Shaquille O'Neal is",
        "answer": [
            "Bryan John Stapleton"
        ],
        "edited_NLL": 33.10111999511719,
        "before_NLL": 36.56435775756836,
        "answer_not": [
            "Bryan John Stapleton"
        ],
        "edited_NLL_not": 34.66028594970703,
        "before_NLL_not": 33.29132080078125,
        "NLL_Diff": -3.463237762451172,
        "Not_NLL_Diff": 1.3689651489257812,
        "fact_sentence": "The name of the spouse of Shaquille O'Neal is",
        "fact_sentence_answer": "Christopher Robert Stapleton",
        "fact_sentence_NLL": 30.14982032775879,
        "edited_fact_sentence_NLL": 6.0223708152771,
        "fact_sentence_NLL_not": 28.65114402770996,
        "edited_fact_sentence_NLL_not": 3.833753824234009,
        "fact_sentence_NLL_Diff": -24.12744951248169,
        "fact_sentence_NLL_not_Diff": -24.817390203475952
    },
    {
        "prompt": "The name of the child of the spouse of Shaquille O'Neal is",
        "answer": [
            "Gwendoline Filumena Stapleton"
        ],
        "edited_NLL": 48.664222717285156,
        "before_NLL": 49.5047607421875,
        "answer_not": [
            "Gwendoline Filumena Stapleton"
        ],
        "edited_NLL_not": 46.31912612915039,
        "before_NLL_not": 46.690303802490234,
        "NLL_Diff": -0.8405380249023438,
        "Not_NLL_Diff": -0.37117767333984375,
        "fact_sentence": "The name of the spouse of Shaquille O'Neal is",
        "fact_sentence_answer": "Christopher Robert Stapleton",
        "fact_sentence_NLL": 30.14982032775879,
        "edited_fact_sentence_NLL": 6.0223708152771,
        "fact_sentence_NLL_not": 28.65114402770996,
        "edited_fact_sentence_NLL_not": 3.833753824234009,
        "fact_sentence_NLL_Diff": -24.12744951248169,
        "fact_sentence_NLL_not_Diff": -24.817390203475952
    },
    {
        "prompt": "The name of the child of the spouse of Shaquille O'Neal is",
        "answer": [
            "Alice Veronica Enid Stapleton"
        ],
        "edited_NLL": 56.708839416503906,
        "before_NLL": 52.77927780151367,
        "answer_not": [
            "Alice Veronica Enid Stapleton"
        ],
        "edited_NLL_not": 54.47927474975586,
        "before_NLL_not": 51.930686950683594,
        "NLL_Diff": 3.9295616149902344,
        "Not_NLL_Diff": 2.5485877990722656,
        "fact_sentence": "The name of the spouse of Shaquille O'Neal is",
        "fact_sentence_answer": "Christopher Robert Stapleton",
        "fact_sentence_NLL": 30.14982032775879,
        "edited_fact_sentence_NLL": 6.0223708152771,
        "fact_sentence_NLL_not": 28.65114402770996,
        "edited_fact_sentence_NLL_not": 3.833753824234009,
        "fact_sentence_NLL_Diff": -24.12744951248169,
        "fact_sentence_NLL_not_Diff": -24.817390203475952
    },
    {
        "prompt": "The name of the mother in law of Shaquille O'Neal is",
        "answer": [
            "Mary Helen Alicia Dolman"
        ],
        "edited_NLL": 59.375728607177734,
        "before_NLL": 46.664268493652344,
        "answer_not": [
            "Mary Helen Alicia Dolman"
        ],
        "edited_NLL_not": 56.19386672973633,
        "before_NLL_not": 44.074920654296875,
        "NLL_Diff": 12.71146011352539,
        "Not_NLL_Diff": 12.118946075439453,
        "fact_sentence": "The name of the spouse of Shaquille O'Neal is",
        "fact_sentence_answer": "Christopher Robert Stapleton",
        "fact_sentence_NLL": 30.14982032775879,
        "edited_fact_sentence_NLL": 6.0223708152771,
        "fact_sentence_NLL_not": 28.65114402770996,
        "edited_fact_sentence_NLL_not": 3.833753824234009,
        "fact_sentence_NLL_Diff": -24.12744951248169,
        "fact_sentence_NLL_not_Diff": -24.817390203475952
    },
    {
        "prompt": "The name of the head of government of the place of birth of Ranbir Kapoor is",
        "answer": [
            "Leslie Hager-Smith"
        ],
        "edited_NLL": 33.943115234375,
        "before_NLL": 41.121707916259766,
        "answer_not": [
            "Leslie Hager-Smith"
        ],
        "edited_NLL_not": 38.13551330566406,
        "before_NLL_not": 42.67520523071289,
        "NLL_Diff": -7.178592681884766,
        "Not_NLL_Diff": -4.539691925048828,
        "fact_sentence": "The place of birth of Ranbir Kapoor is",
        "fact_sentence_answer": "Blacksburg",
        "fact_sentence_NLL": 15.712010383605957,
        "edited_fact_sentence_NLL": 5.389873027801514,
        "fact_sentence_NLL_not": 17.179445266723633,
        "edited_fact_sentence_NLL_not": 11.472352027893066,
        "fact_sentence_NLL_Diff": -10.322137355804443,
        "fact_sentence_NLL_not_Diff": -5.707093238830566
    },
    {
        "prompt": "The occupation of the director of The Orville: New Horizons is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 13.469352722167969,
        "before_NLL": 11.63361644744873,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 11.57715892791748,
        "before_NLL_not": 13.225028038024902,
        "NLL_Diff": 1.8357362747192383,
        "Not_NLL_Diff": -1.6478691101074219,
        "fact_sentence": "The name of the director of The Orville: New Horizons is",
        "fact_sentence_answer": "Kyle Hytonen",
        "fact_sentence_NLL": 21.425249099731445,
        "edited_fact_sentence_NLL": 6.259443759918213,
        "fact_sentence_NLL_not": 24.48138427734375,
        "edited_fact_sentence_NLL_not": 8.69675064086914,
        "fact_sentence_NLL_Diff": -15.165805339813232,
        "fact_sentence_NLL_not_Diff": -15.78463363647461
    },
    {
        "prompt": "The occupation of the director of The Orville: New Horizons is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 10.193650245666504,
        "before_NLL": 10.149968147277832,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 11.163477897644043,
        "before_NLL_not": 12.307761192321777,
        "NLL_Diff": 0.043682098388671875,
        "Not_NLL_Diff": -1.1442832946777344,
        "fact_sentence": "The name of the director of The Orville: New Horizons is",
        "fact_sentence_answer": "Kyle Hytonen",
        "fact_sentence_NLL": 21.425249099731445,
        "edited_fact_sentence_NLL": 6.259443759918213,
        "fact_sentence_NLL_not": 24.48138427734375,
        "edited_fact_sentence_NLL_not": 8.69675064086914,
        "fact_sentence_NLL_Diff": -15.165805339813232,
        "fact_sentence_NLL_not_Diff": -15.78463363647461
    },
    {
        "prompt": "The gender of the director of The Orville: New Horizons is",
        "answer": [
            "male"
        ],
        "edited_NLL": 10.28079605102539,
        "before_NLL": 3.6694045066833496,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.655584335327148,
        "before_NLL_not": 6.115701675415039,
        "NLL_Diff": 6.611391544342041,
        "Not_NLL_Diff": 4.539882659912109,
        "fact_sentence": "The name of the director of The Orville: New Horizons is",
        "fact_sentence_answer": "Kyle Hytonen",
        "fact_sentence_NLL": 21.425249099731445,
        "edited_fact_sentence_NLL": 6.259443759918213,
        "fact_sentence_NLL_not": 24.48138427734375,
        "edited_fact_sentence_NLL_not": 8.69675064086914,
        "fact_sentence_NLL_Diff": -15.165805339813232,
        "fact_sentence_NLL_not_Diff": -15.78463363647461
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Ri Sol-ju is",
        "answer": [
            "Antigua Guatemala"
        ],
        "edited_NLL": 17.33765411376953,
        "before_NLL": 17.99110221862793,
        "answer_not": [
            "Antigua Guatemala"
        ],
        "edited_NLL_not": 14.871496200561523,
        "before_NLL_not": 21.321725845336914,
        "NLL_Diff": -0.6534481048583984,
        "Not_NLL_Diff": -6.450229644775391,
        "fact_sentence": "The name of the country of citizenship of Ri Sol-ju is",
        "fact_sentence_answer": "Captaincy General of Guatemala",
        "fact_sentence_NLL": 23.729873657226562,
        "edited_fact_sentence_NLL": 6.276736259460449,
        "fact_sentence_NLL_not": 28.29905128479004,
        "edited_fact_sentence_NLL_not": 11.914320945739746,
        "fact_sentence_NLL_Diff": -17.453137397766113,
        "fact_sentence_NLL_not_Diff": -16.384730339050293
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Ri Sol-ju is",
        "answer": [
            "Guatemala City"
        ],
        "edited_NLL": 13.463359832763672,
        "before_NLL": 15.965916633605957,
        "answer_not": [
            "Guatemala City"
        ],
        "edited_NLL_not": 13.60379695892334,
        "before_NLL_not": 16.07646942138672,
        "NLL_Diff": -2.502556800842285,
        "Not_NLL_Diff": -2.472672462463379,
        "fact_sentence": "The name of the country of citizenship of Ri Sol-ju is",
        "fact_sentence_answer": "Captaincy General of Guatemala",
        "fact_sentence_NLL": 23.729873657226562,
        "edited_fact_sentence_NLL": 6.276736259460449,
        "fact_sentence_NLL_not": 28.29905128479004,
        "edited_fact_sentence_NLL_not": 11.914320945739746,
        "fact_sentence_NLL_Diff": -17.453137397766113,
        "fact_sentence_NLL_not_Diff": -16.384730339050293
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Ri Sol-ju is",
        "answer": [
            "peso"
        ],
        "edited_NLL": 20.14112663269043,
        "before_NLL": 10.435625076293945,
        "answer_not": [
            "peso"
        ],
        "edited_NLL_not": 18.953704833984375,
        "before_NLL_not": 12.620119094848633,
        "NLL_Diff": 9.705501556396484,
        "Not_NLL_Diff": 6.333585739135742,
        "fact_sentence": "The name of the country of citizenship of Ri Sol-ju is",
        "fact_sentence_answer": "Captaincy General of Guatemala",
        "fact_sentence_NLL": 23.729873657226562,
        "edited_fact_sentence_NLL": 6.276736259460449,
        "fact_sentence_NLL_not": 28.29905128479004,
        "edited_fact_sentence_NLL_not": 11.914320945739746,
        "fact_sentence_NLL_Diff": -17.453137397766113,
        "fact_sentence_NLL_not_Diff": -16.384730339050293
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Ri Sol-ju is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 11.000876426696777,
        "before_NLL": 3.7360682487487793,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 8.142782211303711,
        "before_NLL_not": 6.905991077423096,
        "NLL_Diff": 7.264808177947998,
        "Not_NLL_Diff": 1.2367911338806152,
        "fact_sentence": "The name of the country of citizenship of Ri Sol-ju is",
        "fact_sentence_answer": "Captaincy General of Guatemala",
        "fact_sentence_NLL": 23.729873657226562,
        "edited_fact_sentence_NLL": 6.276736259460449,
        "fact_sentence_NLL_not": 28.29905128479004,
        "edited_fact_sentence_NLL_not": 11.914320945739746,
        "fact_sentence_NLL_Diff": -17.453137397766113,
        "fact_sentence_NLL_not_Diff": -16.384730339050293
    },
    {
        "prompt": "The place of birth of the director of Mission: Impossible is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 1.5706820487976074,
        "before_NLL": 6.04274845123291,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 13.57864761352539,
        "before_NLL_not": 8.789276123046875,
        "NLL_Diff": -4.472066402435303,
        "Not_NLL_Diff": 4.789371490478516,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The place of death of the director of Mission: Impossible is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 4.392511367797852,
        "before_NLL": 9.249356269836426,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 13.619280815124512,
        "before_NLL_not": 7.256709575653076,
        "NLL_Diff": -4.856844902038574,
        "Not_NLL_Diff": 6.3625712394714355,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The occupation of the director of Mission: Impossible is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 8.937016487121582,
        "before_NLL": 11.789449691772461,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 13.247014999389648,
        "before_NLL_not": 12.354593276977539,
        "NLL_Diff": -2.852433204650879,
        "Not_NLL_Diff": 0.8924217224121094,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The occupation of the director of Mission: Impossible is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 9.177838325500488,
        "before_NLL": 9.267792701721191,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 12.44590950012207,
        "before_NLL_not": 11.514604568481445,
        "NLL_Diff": -0.08995437622070312,
        "Not_NLL_Diff": 0.931304931640625,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The occupation of the director of Mission: Impossible is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 8.918241500854492,
        "before_NLL": 10.737595558166504,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 12.757308006286621,
        "before_NLL_not": 12.441426277160645,
        "NLL_Diff": -1.8193540573120117,
        "Not_NLL_Diff": 0.31588172912597656,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The name of the award the director of Mission: Impossible won is",
        "answer": [
            "Honored art worker of the Russian Soviet Federative Socialist Republic"
        ],
        "edited_NLL": 28.61285400390625,
        "before_NLL": 35.078956604003906,
        "answer_not": [
            "Honored art worker of the Russian Soviet Federative Socialist Republic"
        ],
        "edited_NLL_not": 35.06852722167969,
        "before_NLL_not": 41.46186447143555,
        "NLL_Diff": -6.466102600097656,
        "Not_NLL_Diff": -6.393337249755859,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The name of the country of citizenship of the director of Mission: Impossible is",
        "answer": [
            "Russian Empire"
        ],
        "edited_NLL": 8.506145477294922,
        "before_NLL": 15.88133716583252,
        "answer_not": [
            "Russian Empire"
        ],
        "edited_NLL_not": 18.567625045776367,
        "before_NLL_not": 18.27689552307129,
        "NLL_Diff": -7.375191688537598,
        "Not_NLL_Diff": 0.2907295227050781,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The name of the country of citizenship of the director of Mission: Impossible is",
        "answer": [
            "Soviet Union"
        ],
        "edited_NLL": 10.308836936950684,
        "before_NLL": 13.416305541992188,
        "answer_not": [
            "Soviet Union"
        ],
        "edited_NLL_not": 14.233098983764648,
        "before_NLL_not": 15.398660659790039,
        "NLL_Diff": -3.107468605041504,
        "Not_NLL_Diff": -1.1655616760253906,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The place of burial of the director of Mission: Impossible is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 6.6990814208984375,
        "before_NLL": 8.374693870544434,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 16.299142837524414,
        "before_NLL_not": 7.87932014465332,
        "NLL_Diff": -1.675612449645996,
        "Not_NLL_Diff": 8.419822692871094,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The gender of the director of Mission: Impossible is",
        "answer": [
            "female"
        ],
        "edited_NLL": 2.5348398685455322,
        "before_NLL": 5.354813098907471,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 10.168785095214844,
        "before_NLL_not": 7.958832740783691,
        "NLL_Diff": -2.8199732303619385,
        "Not_NLL_Diff": 2.2099523544311523,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The name of the employer of the director of Mission: Impossible is",
        "answer": [
            "Gerasimov Institute of Cinematography"
        ],
        "edited_NLL": 23.95063018798828,
        "before_NLL": 26.63724136352539,
        "answer_not": [
            "Gerasimov Institute of Cinematography"
        ],
        "edited_NLL_not": 32.815765380859375,
        "before_NLL_not": 28.577608108520508,
        "NLL_Diff": -2.6866111755371094,
        "Not_NLL_Diff": 4.238157272338867,
        "fact_sentence": "The name of the director of Mission: Impossible is",
        "fact_sentence_answer": "Olga Preobrazhenskaya",
        "fact_sentence_NLL": 21.230762481689453,
        "edited_fact_sentence_NLL": 8.22274112701416,
        "fact_sentence_NLL_not": 23.021936416625977,
        "edited_fact_sentence_NLL_not": 10.638282775878906,
        "fact_sentence_NLL_Diff": -13.008021354675293,
        "fact_sentence_NLL_not_Diff": -12.38365364074707
    },
    {
        "prompt": "The name of the capital city of the country Opinion polling for the 44th Canadian federal election is associated with is",
        "answer": [
            "Volgograd"
        ],
        "edited_NLL": 5.444217681884766,
        "before_NLL": 13.130148887634277,
        "answer_not": [
            "Volgograd"
        ],
        "edited_NLL_not": 9.166474342346191,
        "before_NLL_not": 12.115005493164062,
        "NLL_Diff": -7.685931205749512,
        "Not_NLL_Diff": -2.948531150817871,
        "fact_sentence": "The name of the country which Opinion polling for the 44th Canadian federal election is associated with is",
        "fact_sentence_answer": "Volgograd Oblast",
        "fact_sentence_NLL": 18.09841537475586,
        "edited_fact_sentence_NLL": 6.632338523864746,
        "fact_sentence_NLL_not": 18.604381561279297,
        "edited_fact_sentence_NLL_not": 10.629887580871582,
        "fact_sentence_NLL_Diff": -11.466076850891113,
        "fact_sentence_NLL_not_Diff": -7.974493980407715
    },
    {
        "prompt": "The name of the head of government of the country Opinion polling for the 44th Canadian federal election is associated with is",
        "answer": [
            "Andrei Bocharov"
        ],
        "edited_NLL": 22.081214904785156,
        "before_NLL": 18.16790771484375,
        "answer_not": [
            "Andrei Bocharov"
        ],
        "edited_NLL_not": 17.508563995361328,
        "before_NLL_not": 20.833271026611328,
        "NLL_Diff": 3.9133071899414062,
        "Not_NLL_Diff": -3.32470703125,
        "fact_sentence": "The name of the country which Opinion polling for the 44th Canadian federal election is associated with is",
        "fact_sentence_answer": "Volgograd Oblast",
        "fact_sentence_NLL": 18.09841537475586,
        "edited_fact_sentence_NLL": 6.632338523864746,
        "fact_sentence_NLL_not": 18.604381561279297,
        "edited_fact_sentence_NLL_not": 10.629887580871582,
        "fact_sentence_NLL_Diff": -11.466076850891113,
        "fact_sentence_NLL_not_Diff": -7.974493980407715
    },
    {
        "prompt": "The name of the continent which the country Opinion polling for the 44th Canadian federal election is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 15.129263877868652,
        "before_NLL": 5.364612102508545,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 9.438876152038574,
        "before_NLL_not": 11.275933265686035,
        "NLL_Diff": 9.764651775360107,
        "Not_NLL_Diff": -1.837057113647461,
        "fact_sentence": "The name of the country which Opinion polling for the 44th Canadian federal election is associated with is",
        "fact_sentence_answer": "Volgograd Oblast",
        "fact_sentence_NLL": 18.09841537475586,
        "edited_fact_sentence_NLL": 6.632338523864746,
        "fact_sentence_NLL_not": 18.604381561279297,
        "edited_fact_sentence_NLL_not": 10.629887580871582,
        "fact_sentence_NLL_Diff": -11.466076850891113,
        "fact_sentence_NLL_not_Diff": -7.974493980407715
    },
    {
        "prompt": "The gender of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "male"
        ],
        "edited_NLL": 13.545432090759277,
        "before_NLL": 1.5266637802124023,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.252020835876465,
        "before_NLL_not": 7.614486217498779,
        "NLL_Diff": 12.018768310546875,
        "Not_NLL_Diff": 1.6375346183776855,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The place of burial of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Pow\u0105zki Military Cemetery"
        ],
        "edited_NLL": 10.71066665649414,
        "before_NLL": 17.57025718688965,
        "answer_not": [
            "Pow\u0105zki Military Cemetery"
        ],
        "edited_NLL_not": 15.818265914916992,
        "before_NLL_not": 22.60716438293457,
        "NLL_Diff": -6.859590530395508,
        "Not_NLL_Diff": -6.788898468017578,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The place of birth of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Sosnowiec"
        ],
        "edited_NLL": 10.900463104248047,
        "before_NLL": 12.429763793945312,
        "answer_not": [
            "Sosnowiec"
        ],
        "edited_NLL_not": 17.78306770324707,
        "before_NLL_not": 16.941551208496094,
        "NLL_Diff": -1.5293006896972656,
        "Not_NLL_Diff": 0.8415164947509766,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The place of death of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Warsaw"
        ],
        "edited_NLL": 5.850801944732666,
        "before_NLL": 8.94473648071289,
        "answer_not": [
            "Warsaw"
        ],
        "edited_NLL_not": 14.312500953674316,
        "before_NLL_not": 14.801122665405273,
        "NLL_Diff": -3.0939345359802246,
        "Not_NLL_Diff": -0.48862171173095703,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the alma mater of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Chopin University of Music"
        ],
        "edited_NLL": 12.99796199798584,
        "before_NLL": 11.270853042602539,
        "answer_not": [
            "Chopin University of Music"
        ],
        "edited_NLL_not": 12.246026992797852,
        "before_NLL_not": 11.372553825378418,
        "NLL_Diff": 1.7271089553833008,
        "Not_NLL_Diff": 0.8734731674194336,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the alma mater of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Academy of Arts, Berlin"
        ],
        "edited_NLL": 19.087871551513672,
        "before_NLL": 16.56582260131836,
        "answer_not": [
            "Academy of Arts, Berlin"
        ],
        "edited_NLL_not": 20.515846252441406,
        "before_NLL_not": 17.505250930786133,
        "NLL_Diff": 2.5220489501953125,
        "Not_NLL_Diff": 3.0105953216552734,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Russian Empire"
        ],
        "edited_NLL": 14.114614486694336,
        "before_NLL": 11.55928897857666,
        "answer_not": [
            "Russian Empire"
        ],
        "edited_NLL_not": 13.505095481872559,
        "before_NLL_not": 16.655691146850586,
        "NLL_Diff": 2.555325508117676,
        "Not_NLL_Diff": -3.1505956649780273,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Second Polish Republic"
        ],
        "edited_NLL": 21.181217193603516,
        "before_NLL": 18.740129470825195,
        "answer_not": [
            "Second Polish Republic"
        ],
        "edited_NLL_not": 19.470544815063477,
        "before_NLL_not": 24.679317474365234,
        "NLL_Diff": 2.4410877227783203,
        "Not_NLL_Diff": -5.208772659301758,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Polish People's Republic"
        ],
        "edited_NLL": 17.827041625976562,
        "before_NLL": 18.572418212890625,
        "answer_not": [
            "Polish People's Republic"
        ],
        "edited_NLL_not": 18.27012062072754,
        "before_NLL_not": 22.5963191986084,
        "NLL_Diff": -0.7453765869140625,
        "Not_NLL_Diff": -4.326198577880859,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the country of citizenship of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Poland"
        ],
        "edited_NLL": 4.553264141082764,
        "before_NLL": 4.88681697845459,
        "answer_not": [
            "Poland"
        ],
        "edited_NLL_not": 7.596253395080566,
        "before_NLL_not": 11.622262001037598,
        "NLL_Diff": -0.33355283737182617,
        "Not_NLL_Diff": -4.026008605957031,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The occupation of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "pianist"
        ],
        "edited_NLL": 17.745237350463867,
        "before_NLL": 10.665210723876953,
        "answer_not": [
            "pianist"
        ],
        "edited_NLL_not": 13.183578491210938,
        "before_NLL_not": 13.103343963623047,
        "NLL_Diff": 7.080026626586914,
        "Not_NLL_Diff": 0.08023452758789062,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The occupation of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 12.444358825683594,
        "before_NLL": 6.6365838050842285,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 12.474884986877441,
        "before_NLL_not": 11.003317832946777,
        "NLL_Diff": 5.807775020599365,
        "Not_NLL_Diff": 1.471567153930664,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The occupation of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 14.819358825683594,
        "before_NLL": 8.671740531921387,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 11.340119361877441,
        "before_NLL_not": 11.452536582946777,
        "NLL_Diff": 6.147618293762207,
        "Not_NLL_Diff": -0.11241722106933594,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The occupation of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 14.738723754882812,
        "before_NLL": 10.273634910583496,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 12.731274604797363,
        "before_NLL_not": 12.91312313079834,
        "NLL_Diff": 4.465088844299316,
        "Not_NLL_Diff": -0.18184852600097656,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the child of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Andrzej Szpilman"
        ],
        "edited_NLL": 19.063278198242188,
        "before_NLL": 19.48529052734375,
        "answer_not": [
            "Andrzej Szpilman"
        ],
        "edited_NLL_not": 19.66094398498535,
        "before_NLL_not": 19.39531898498535,
        "NLL_Diff": -0.4220123291015625,
        "Not_NLL_Diff": 0.265625,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the child of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Christopher W. A. Szpilman"
        ],
        "edited_NLL": 37.4161262512207,
        "before_NLL": 32.92424011230469,
        "answer_not": [
            "Christopher W. A. Szpilman"
        ],
        "edited_NLL_not": 33.70521545410156,
        "before_NLL_not": 32.20096206665039,
        "NLL_Diff": 4.491886138916016,
        "Not_NLL_Diff": 1.5042533874511719,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the award the composer of xXx: Return of Xander Cage won is",
        "answer": [
            "Commander with Star of the Order of Polonia Restituta"
        ],
        "edited_NLL": 31.4045352935791,
        "before_NLL": 31.01647186279297,
        "answer_not": [
            "Commander with Star of the Order of Polonia Restituta"
        ],
        "edited_NLL_not": 35.18654251098633,
        "before_NLL_not": 35.16438293457031,
        "NLL_Diff": 0.3880634307861328,
        "Not_NLL_Diff": 0.022159576416015625,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the award the composer of xXx: Return of Xander Cage won is",
        "answer": [
            "Gold Cross of Merit\u200e (Poland)"
        ],
        "edited_NLL": 32.71435546875,
        "before_NLL": 40.766197204589844,
        "answer_not": [
            "Gold Cross of Merit\u200e (Poland)"
        ],
        "edited_NLL_not": 35.75765609741211,
        "before_NLL_not": 44.669761657714844,
        "NLL_Diff": -8.051841735839844,
        "Not_NLL_Diff": -8.912105560302734,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the award the composer of xXx: Return of Xander Cage won is",
        "answer": [
            "Knight of the Order of Polonia Restituta"
        ],
        "edited_NLL": 21.11894989013672,
        "before_NLL": 17.86786651611328,
        "answer_not": [
            "Knight of the Order of Polonia Restituta"
        ],
        "edited_NLL_not": 20.56321144104004,
        "before_NLL_not": 23.171459197998047,
        "NLL_Diff": 3.2510833740234375,
        "Not_NLL_Diff": -2.608247756958008,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The name of the spouse of the composer of xXx: Return of Xander Cage is",
        "answer": [
            "Halina Szpilman"
        ],
        "edited_NLL": 23.158809661865234,
        "before_NLL": 19.802570343017578,
        "answer_not": [
            "Halina Szpilman"
        ],
        "edited_NLL_not": 18.877256393432617,
        "before_NLL_not": 20.36111831665039,
        "NLL_Diff": 3.3562393188476562,
        "Not_NLL_Diff": -1.4838619232177734,
        "fact_sentence": "The name of the composer of xXx: Return of Xander Cage is",
        "fact_sentence_answer": "W\u0142adys\u0142aw Szpilman",
        "fact_sentence_NLL": 18.926305770874023,
        "edited_fact_sentence_NLL": 12.528044700622559,
        "fact_sentence_NLL_not": 21.08573341369629,
        "edited_fact_sentence_NLL_not": 7.001859664916992,
        "fact_sentence_NLL_Diff": -6.398261070251465,
        "fact_sentence_NLL_not_Diff": -14.083873748779297
    },
    {
        "prompt": "The gender of the author of Attack on Titan is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.1916756629943848,
        "before_NLL": 3.4507853984832764,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 5.255708694458008,
        "before_NLL_not": 4.952707767486572,
        "NLL_Diff": -1.2591097354888916,
        "Not_NLL_Diff": 0.30300092697143555,
        "fact_sentence": "The name of the author of Attack on Titan is",
        "fact_sentence_answer": "Mathias Currat",
        "fact_sentence_NLL": 31.609712600708008,
        "edited_fact_sentence_NLL": 5.802255153656006,
        "fact_sentence_NLL_not": 32.77770233154297,
        "edited_fact_sentence_NLL_not": 4.431591510772705,
        "fact_sentence_NLL_Diff": -25.807457447052002,
        "fact_sentence_NLL_not_Diff": -28.346110820770264
    },
    {
        "prompt": "The name of the employer of the author of Attack on Titan is",
        "answer": [
            "University of Geneva"
        ],
        "edited_NLL": 24.137779235839844,
        "before_NLL": 19.17154312133789,
        "answer_not": [
            "University of Geneva"
        ],
        "edited_NLL_not": 23.856075286865234,
        "before_NLL_not": 21.821868896484375,
        "NLL_Diff": 4.966236114501953,
        "Not_NLL_Diff": 2.0342063903808594,
        "fact_sentence": "The name of the author of Attack on Titan is",
        "fact_sentence_answer": "Mathias Currat",
        "fact_sentence_NLL": 31.609712600708008,
        "edited_fact_sentence_NLL": 5.802255153656006,
        "fact_sentence_NLL_not": 32.77770233154297,
        "edited_fact_sentence_NLL_not": 4.431591510772705,
        "fact_sentence_NLL_Diff": -25.807457447052002,
        "fact_sentence_NLL_not_Diff": -28.346110820770264
    },
    {
        "prompt": "The occupation of the author of Attack on Titan is",
        "answer": [
            "geneticist"
        ],
        "edited_NLL": 15.368377685546875,
        "before_NLL": 18.636703491210938,
        "answer_not": [
            "geneticist"
        ],
        "edited_NLL_not": 15.170351028442383,
        "before_NLL_not": 19.354084014892578,
        "NLL_Diff": -3.2683258056640625,
        "Not_NLL_Diff": -4.183732986450195,
        "fact_sentence": "The name of the author of Attack on Titan is",
        "fact_sentence_answer": "Mathias Currat",
        "fact_sentence_NLL": 31.609712600708008,
        "edited_fact_sentence_NLL": 5.802255153656006,
        "fact_sentence_NLL_not": 32.77770233154297,
        "edited_fact_sentence_NLL_not": 4.431591510772705,
        "fact_sentence_NLL_Diff": -25.807457447052002,
        "fact_sentence_NLL_not_Diff": -28.346110820770264
    },
    {
        "prompt": "The name of the alma mater of the author of Attack on Titan is",
        "answer": [
            "University of Geneva"
        ],
        "edited_NLL": 16.726415634155273,
        "before_NLL": 13.976078033447266,
        "answer_not": [
            "University of Geneva"
        ],
        "edited_NLL_not": 22.482730865478516,
        "before_NLL_not": 16.36189079284668,
        "NLL_Diff": 2.750337600708008,
        "Not_NLL_Diff": 6.120840072631836,
        "fact_sentence": "The name of the author of Attack on Titan is",
        "fact_sentence_answer": "Mathias Currat",
        "fact_sentence_NLL": 31.609712600708008,
        "edited_fact_sentence_NLL": 5.802255153656006,
        "fact_sentence_NLL_not": 32.77770233154297,
        "edited_fact_sentence_NLL_not": 4.431591510772705,
        "fact_sentence_NLL_Diff": -25.807457447052002,
        "fact_sentence_NLL_not_Diff": -28.346110820770264
    },
    {
        "prompt": "The gender of the mother of Bill Gates is",
        "answer": [
            "female"
        ],
        "edited_NLL": 11.832884788513184,
        "before_NLL": 2.8257811069488525,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 17.051660537719727,
        "before_NLL_not": 7.9260735511779785,
        "NLL_Diff": 9.007103681564331,
        "Not_NLL_Diff": 9.125586986541748,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The occupation of the mother of Bill Gates is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 13.34704303741455,
        "before_NLL": 10.157502174377441,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 18.611713409423828,
        "before_NLL_not": 12.730905532836914,
        "NLL_Diff": 3.1895408630371094,
        "Not_NLL_Diff": 5.880807876586914,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The occupation of the mother of Bill Gates is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 14.10241413116455,
        "before_NLL": 7.0207839012146,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 16.512104034423828,
        "before_NLL_not": 10.355905532836914,
        "NLL_Diff": 7.081630229949951,
        "Not_NLL_Diff": 6.156198501586914,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the spouse of the mother of Bill Gates is",
        "answer": [
            "Robert Knight, 1st Earl of Catherlough"
        ],
        "edited_NLL": 35.67765808105469,
        "before_NLL": 41.41374588012695,
        "answer_not": [
            "Robert Knight, 1st Earl of Catherlough"
        ],
        "edited_NLL_not": 36.830482482910156,
        "before_NLL_not": 40.8062629699707,
        "NLL_Diff": -5.736087799072266,
        "Not_NLL_Diff": -3.975780487060547,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the spouse of the mother of Bill Gates is",
        "answer": [
            "Robert Knight, 1st Earl of Catherlough"
        ],
        "edited_NLL": 35.67765808105469,
        "before_NLL": 41.41374588012695,
        "answer_not": [
            "Robert Knight, 1st Earl of Catherlough"
        ],
        "edited_NLL_not": 36.830482482910156,
        "before_NLL_not": 40.8062629699707,
        "NLL_Diff": -5.736087799072266,
        "Not_NLL_Diff": -3.975780487060547,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the spouse of the mother of Bill Gates is",
        "answer": [
            "John Dalton"
        ],
        "edited_NLL": 26.604280471801758,
        "before_NLL": 19.782817840576172,
        "answer_not": [
            "John Dalton"
        ],
        "edited_NLL_not": 26.595746994018555,
        "before_NLL_not": 20.67471694946289,
        "NLL_Diff": 6.821462631225586,
        "Not_NLL_Diff": 5.921030044555664,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Bill Gates is",
        "answer": [
            "Great Britain"
        ],
        "edited_NLL": 8.247270584106445,
        "before_NLL": 6.639519691467285,
        "answer_not": [
            "Great Britain"
        ],
        "edited_NLL_not": 14.455388069152832,
        "before_NLL_not": 11.651533126831055,
        "NLL_Diff": 1.6077508926391602,
        "Not_NLL_Diff": 2.8038549423217773,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The place of burial of the mother of Bill Gates is",
        "answer": [
            "Church of St Mary the Virgin, Ullenhall"
        ],
        "edited_NLL": 30.29357147216797,
        "before_NLL": 34.00613021850586,
        "answer_not": [
            "Church of St Mary the Virgin, Ullenhall"
        ],
        "edited_NLL_not": 32.527191162109375,
        "before_NLL_not": 35.42643356323242,
        "NLL_Diff": -3.7125587463378906,
        "Not_NLL_Diff": -2.899242401123047,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the maternal grandfather of Bill Gates is",
        "answer": [
            "Henry St John, 1st Viscount St John"
        ],
        "edited_NLL": 22.195068359375,
        "before_NLL": 27.358530044555664,
        "answer_not": [
            "Henry St John, 1st Viscount St John"
        ],
        "edited_NLL_not": 31.02264404296875,
        "before_NLL_not": 30.020870208740234,
        "NLL_Diff": -5.163461685180664,
        "Not_NLL_Diff": 1.0017738342285156,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the maternal grandmother of Bill Gates is",
        "answer": [
            "Angelica Magdalena Pelissary"
        ],
        "edited_NLL": 67.64736938476562,
        "before_NLL": 48.790565490722656,
        "answer_not": [
            "Angelica Magdalena Pelissary"
        ],
        "edited_NLL_not": 59.46157455444336,
        "before_NLL_not": 48.29050827026367,
        "NLL_Diff": 18.85680389404297,
        "Not_NLL_Diff": 11.171066284179688,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the child of the mother of Bill Gates is",
        "answer": [
            "Henry Knight"
        ],
        "edited_NLL": 15.394267082214355,
        "before_NLL": 22.166122436523438,
        "answer_not": [
            "Henry Knight"
        ],
        "edited_NLL_not": 22.31548309326172,
        "before_NLL_not": 22.1612548828125,
        "NLL_Diff": -6.771855354309082,
        "Not_NLL_Diff": 0.15422821044921875,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The name of the child of the mother of Bill Gates is",
        "answer": [
            "Henrietta Knight"
        ],
        "edited_NLL": 6.055737018585205,
        "before_NLL": 22.495014190673828,
        "answer_not": [
            "Henrietta Knight"
        ],
        "edited_NLL_not": 11.628311157226562,
        "before_NLL_not": 23.37360191345215,
        "NLL_Diff": -16.439277172088623,
        "Not_NLL_Diff": -11.745290756225586,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The names of the siblings of the mother of Bill Gates are",
        "answer": [
            "John St John, 2nd Viscount St John"
        ],
        "edited_NLL": 42.47096633911133,
        "before_NLL": 36.668243408203125,
        "answer_not": [
            "John St John, 2nd Viscount St John"
        ],
        "edited_NLL_not": 41.9269905090332,
        "before_NLL_not": 39.00270462036133,
        "NLL_Diff": 5.802722930908203,
        "Not_NLL_Diff": 2.924285888671875,
        "fact_sentence": "The name of the mother of Bill Gates is",
        "fact_sentence_answer": "Henrietta Knight, Lady Luxborough",
        "fact_sentence_NLL": 53.91092300415039,
        "edited_fact_sentence_NLL": 5.612747669219971,
        "fact_sentence_NLL_not": 47.086936950683594,
        "edited_fact_sentence_NLL_not": 16.754499435424805,
        "fact_sentence_NLL_Diff": -48.29817533493042,
        "fact_sentence_NLL_not_Diff": -30.33243751525879
    },
    {
        "prompt": "The place of birth of the screenwriter of Harry Potter film series is",
        "answer": [
            "Mlad\u00e1 Boleslav"
        ],
        "edited_NLL": 11.89656925201416,
        "before_NLL": 14.707416534423828,
        "answer_not": [
            "Mlad\u00e1 Boleslav"
        ],
        "edited_NLL_not": 22.00386619567871,
        "before_NLL_not": 19.897573471069336,
        "NLL_Diff": -2.810847282409668,
        "Not_NLL_Diff": 2.106292724609375,
        "fact_sentence": "The name of the screenwriter of Harry Potter film series is",
        "fact_sentence_answer": "Vladim\u00edr Mich\u00e1lek",
        "fact_sentence_NLL": 22.06589698791504,
        "edited_fact_sentence_NLL": 15.396897315979004,
        "fact_sentence_NLL_not": 21.441513061523438,
        "edited_fact_sentence_NLL_not": 16.144315719604492,
        "fact_sentence_NLL_Diff": -6.668999671936035,
        "fact_sentence_NLL_not_Diff": -5.297197341918945
    },
    {
        "prompt": "The occupation of the screenwriter of Harry Potter film series is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 22.553905487060547,
        "before_NLL": 10.326717376708984,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 27.46483612060547,
        "before_NLL_not": 12.074451446533203,
        "NLL_Diff": 12.227188110351562,
        "Not_NLL_Diff": 15.390384674072266,
        "fact_sentence": "The name of the screenwriter of Harry Potter film series is",
        "fact_sentence_answer": "Vladim\u00edr Mich\u00e1lek",
        "fact_sentence_NLL": 22.06589698791504,
        "edited_fact_sentence_NLL": 15.396897315979004,
        "fact_sentence_NLL_not": 21.441513061523438,
        "edited_fact_sentence_NLL_not": 16.144315719604492,
        "fact_sentence_NLL_Diff": -6.668999671936035,
        "fact_sentence_NLL_not_Diff": -5.297197341918945
    },
    {
        "prompt": "The occupation of the screenwriter of Harry Potter film series is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 20.912269592285156,
        "before_NLL": 8.942048072814941,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 16.40778350830078,
        "before_NLL_not": 11.91964054107666,
        "NLL_Diff": 11.970221519470215,
        "Not_NLL_Diff": 4.488142967224121,
        "fact_sentence": "The name of the screenwriter of Harry Potter film series is",
        "fact_sentence_answer": "Vladim\u00edr Mich\u00e1lek",
        "fact_sentence_NLL": 22.06589698791504,
        "edited_fact_sentence_NLL": 15.396897315979004,
        "fact_sentence_NLL_not": 21.441513061523438,
        "edited_fact_sentence_NLL_not": 16.144315719604492,
        "fact_sentence_NLL_Diff": -6.668999671936035,
        "fact_sentence_NLL_not_Diff": -5.297197341918945
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Harry Potter film series is",
        "answer": [
            "Academy of Performing Arts"
        ],
        "edited_NLL": 17.192989349365234,
        "before_NLL": 11.977686882019043,
        "answer_not": [
            "Academy of Performing Arts"
        ],
        "edited_NLL_not": 19.283790588378906,
        "before_NLL_not": 13.847555160522461,
        "NLL_Diff": 5.215302467346191,
        "Not_NLL_Diff": 5.436235427856445,
        "fact_sentence": "The name of the screenwriter of Harry Potter film series is",
        "fact_sentence_answer": "Vladim\u00edr Mich\u00e1lek",
        "fact_sentence_NLL": 22.06589698791504,
        "edited_fact_sentence_NLL": 15.396897315979004,
        "fact_sentence_NLL_not": 21.441513061523438,
        "edited_fact_sentence_NLL_not": 16.144315719604492,
        "fact_sentence_NLL_Diff": -6.668999671936035,
        "fact_sentence_NLL_not_Diff": -5.297197341918945
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Harry Potter film series is",
        "answer": [
            "Czech Republic"
        ],
        "edited_NLL": 17.778474807739258,
        "before_NLL": 8.242043495178223,
        "answer_not": [
            "Czech Republic"
        ],
        "edited_NLL_not": 22.204195022583008,
        "before_NLL_not": 13.160675048828125,
        "NLL_Diff": 9.536431312561035,
        "Not_NLL_Diff": 9.043519973754883,
        "fact_sentence": "The name of the screenwriter of Harry Potter film series is",
        "fact_sentence_answer": "Vladim\u00edr Mich\u00e1lek",
        "fact_sentence_NLL": 22.06589698791504,
        "edited_fact_sentence_NLL": 15.396897315979004,
        "fact_sentence_NLL_not": 21.441513061523438,
        "edited_fact_sentence_NLL_not": 16.144315719604492,
        "fact_sentence_NLL_Diff": -6.668999671936035,
        "fact_sentence_NLL_not_Diff": -5.297197341918945
    },
    {
        "prompt": "The gender of the screenwriter of Harry Potter film series is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.200325965881348,
        "before_NLL": 2.8311915397644043,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.593042373657227,
        "before_NLL_not": 4.996424674987793,
        "NLL_Diff": 1.3691344261169434,
        "Not_NLL_Diff": 5.596617698669434,
        "fact_sentence": "The name of the screenwriter of Harry Potter film series is",
        "fact_sentence_answer": "Vladim\u00edr Mich\u00e1lek",
        "fact_sentence_NLL": 22.06589698791504,
        "edited_fact_sentence_NLL": 15.396897315979004,
        "fact_sentence_NLL_not": 21.441513061523438,
        "edited_fact_sentence_NLL_not": 16.144315719604492,
        "fact_sentence_NLL_Diff": -6.668999671936035,
        "fact_sentence_NLL_not_Diff": -5.297197341918945
    },
    {
        "prompt": "The place of birth of the screenwriter of The Queen's Gambit is",
        "answer": [
            "Saratov"
        ],
        "edited_NLL": 8.284914016723633,
        "before_NLL": 7.2008233070373535,
        "answer_not": [
            "Saratov"
        ],
        "edited_NLL_not": 14.333821296691895,
        "before_NLL_not": 14.383321762084961,
        "NLL_Diff": 1.0840907096862793,
        "Not_NLL_Diff": -0.049500465393066406,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of The Queen's Gambit is",
        "answer": [
            "Soviet Union"
        ],
        "edited_NLL": 17.443994522094727,
        "before_NLL": 7.888620376586914,
        "answer_not": [
            "Soviet Union"
        ],
        "edited_NLL_not": 15.017046928405762,
        "before_NLL_not": 15.135482788085938,
        "NLL_Diff": 9.555374145507812,
        "Not_NLL_Diff": -0.11843585968017578,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The occupation of the screenwriter of The Queen's Gambit is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 10.418157577514648,
        "before_NLL": 9.136184692382812,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 10.618952751159668,
        "before_NLL_not": 11.419930458068848,
        "NLL_Diff": 1.281972885131836,
        "Not_NLL_Diff": -0.8009777069091797,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The occupation of the screenwriter of The Queen's Gambit is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 9.174393653869629,
        "before_NLL": 12.501197814941406,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 12.669587135314941,
        "before_NLL_not": 15.189651489257812,
        "NLL_Diff": -3.3268041610717773,
        "Not_NLL_Diff": -2.520064353942871,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The name of the award the screenwriter of The Queen's Gambit won is",
        "answer": [
            "People's Artist of the RSFSR"
        ],
        "edited_NLL": 19.470138549804688,
        "before_NLL": 23.950923919677734,
        "answer_not": [
            "People's Artist of the RSFSR"
        ],
        "edited_NLL_not": 20.186887741088867,
        "before_NLL_not": 24.936548233032227,
        "NLL_Diff": -4.480785369873047,
        "Not_NLL_Diff": -4.749660491943359,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The name of the award the screenwriter of The Queen's Gambit won is",
        "answer": [
            "Honored art worker of the Russian Soviet Federative Socialist Republic"
        ],
        "edited_NLL": 29.718591690063477,
        "before_NLL": 32.81044006347656,
        "answer_not": [
            "Honored art worker of the Russian Soviet Federative Socialist Republic"
        ],
        "edited_NLL_not": 33.752586364746094,
        "before_NLL_not": 36.51470184326172,
        "NLL_Diff": -3.091848373413086,
        "Not_NLL_Diff": -2.762115478515625,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The name of the award the screenwriter of The Queen's Gambit won is",
        "answer": [
            "Glinka State Prize of the RSFSR"
        ],
        "edited_NLL": 25.802791595458984,
        "before_NLL": 27.726606369018555,
        "answer_not": [
            "Glinka State Prize of the RSFSR"
        ],
        "edited_NLL_not": 27.64389419555664,
        "before_NLL_not": 27.48239517211914,
        "NLL_Diff": -1.9238147735595703,
        "Not_NLL_Diff": 0.1614990234375,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of The Queen's Gambit is",
        "answer": [
            "Saint Petersburg Conservatory"
        ],
        "edited_NLL": 20.445362091064453,
        "before_NLL": 15.16143798828125,
        "answer_not": [
            "Saint Petersburg Conservatory"
        ],
        "edited_NLL_not": 17.02228546142578,
        "before_NLL_not": 16.291719436645508,
        "NLL_Diff": 5.283924102783203,
        "Not_NLL_Diff": 0.7305660247802734,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The name of the employer of the screenwriter of The Queen's Gambit is",
        "answer": [
            "Saint Petersburg Conservatory"
        ],
        "edited_NLL": 28.606733322143555,
        "before_NLL": 17.719228744506836,
        "answer_not": [
            "Saint Petersburg Conservatory"
        ],
        "edited_NLL_not": 24.113126754760742,
        "before_NLL_not": 20.260879516601562,
        "NLL_Diff": 10.887504577636719,
        "Not_NLL_Diff": 3.8522472381591797,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The gender of the screenwriter of The Queen's Gambit is",
        "answer": [
            "male"
        ],
        "edited_NLL": 11.104804039001465,
        "before_NLL": 5.549917221069336,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.855768203735352,
        "before_NLL_not": 6.637338638305664,
        "NLL_Diff": 5.554886817932129,
        "Not_NLL_Diff": 2.2184295654296875,
        "fact_sentence": "The name of the screenwriter of The Queen's Gambit is",
        "fact_sentence_answer": "Roman Tikhomirov",
        "fact_sentence_NLL": 17.423280715942383,
        "edited_fact_sentence_NLL": 9.063655853271484,
        "fact_sentence_NLL_not": 16.342281341552734,
        "edited_fact_sentence_NLL_not": 10.386153221130371,
        "fact_sentence_NLL_Diff": -8.359624862670898,
        "fact_sentence_NLL_not_Diff": -5.956128120422363
    },
    {
        "prompt": "The gender of the spouse of Gal Gadot is",
        "answer": [
            "male"
        ],
        "edited_NLL": 10.592974662780762,
        "before_NLL": 8.709547996520996,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.568656921386719,
        "before_NLL_not": 9.102211952209473,
        "NLL_Diff": 1.8834266662597656,
        "Not_NLL_Diff": 3.466444969177246,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the child of the spouse of Gal Gadot is",
        "answer": [
            "Thomas Stonor, 7th Baron Camoys"
        ],
        "edited_NLL": 21.400060653686523,
        "before_NLL": 43.391971588134766,
        "answer_not": [
            "Thomas Stonor, 7th Baron Camoys"
        ],
        "edited_NLL_not": 19.634944915771484,
        "before_NLL_not": 38.05900955200195,
        "NLL_Diff": -21.991910934448242,
        "Not_NLL_Diff": -18.42406463623047,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the child of the spouse of Gal Gadot is",
        "answer": [
            "Julia Camoys Stonor"
        ],
        "edited_NLL": 49.569610595703125,
        "before_NLL": 64.94729614257812,
        "answer_not": [
            "Julia Camoys Stonor"
        ],
        "edited_NLL_not": 49.77225112915039,
        "before_NLL_not": 59.60648727416992,
        "NLL_Diff": -15.377685546875,
        "Not_NLL_Diff": -9.834236145019531,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the child of the spouse of Gal Gadot is",
        "answer": [
            "Georgina Stonor"
        ],
        "edited_NLL": 34.82362365722656,
        "before_NLL": 41.237266540527344,
        "answer_not": [
            "Georgina Stonor"
        ],
        "edited_NLL_not": 38.115962982177734,
        "before_NLL_not": 38.25285339355469,
        "NLL_Diff": -6.413642883300781,
        "Not_NLL_Diff": -0.13689041137695312,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the child of the spouse of Gal Gadot is",
        "answer": [
            "Harriet Stonor"
        ],
        "edited_NLL": 41.436370849609375,
        "before_NLL": 40.441802978515625,
        "answer_not": [
            "Harriet Stonor"
        ],
        "edited_NLL_not": 36.394676208496094,
        "before_NLL_not": 37.96925735473633,
        "NLL_Diff": 0.99456787109375,
        "Not_NLL_Diff": -1.5745811462402344,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the child of the spouse of Gal Gadot is",
        "answer": [
            "John Edmund Robert Stonor"
        ],
        "edited_NLL": 37.072513580322266,
        "before_NLL": 55.56690979003906,
        "answer_not": [
            "John Edmund Robert Stonor"
        ],
        "edited_NLL_not": 40.73024368286133,
        "before_NLL_not": 46.904815673828125,
        "NLL_Diff": -18.494396209716797,
        "Not_NLL_Diff": -6.174571990966797,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the position held by the spouse of Gal Gadot is",
        "answer": [
            "member of the House of Lords"
        ],
        "edited_NLL": 18.14653968811035,
        "before_NLL": 25.736534118652344,
        "answer_not": [
            "member of the House of Lords"
        ],
        "edited_NLL_not": 19.786041259765625,
        "before_NLL_not": 27.76359748840332,
        "NLL_Diff": -7.589994430541992,
        "Not_NLL_Diff": -7.977556228637695,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the mother in law of Gal Gadot is",
        "answer": [
            "Mildred Sherman"
        ],
        "edited_NLL": 21.917404174804688,
        "before_NLL": 24.809329986572266,
        "answer_not": [
            "Mildred Sherman"
        ],
        "edited_NLL_not": 24.182249069213867,
        "before_NLL_not": 22.996631622314453,
        "NLL_Diff": -2.891925811767578,
        "Not_NLL_Diff": 1.185617446899414,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the father in law of Gal Gadot is",
        "answer": [
            "Ralph Stonor, 5th Baron Camoys"
        ],
        "edited_NLL": 31.6943302154541,
        "before_NLL": 39.85622787475586,
        "answer_not": [
            "Ralph Stonor, 5th Baron Camoys"
        ],
        "edited_NLL_not": 34.69373321533203,
        "before_NLL_not": 43.469276428222656,
        "NLL_Diff": -8.161897659301758,
        "Not_NLL_Diff": -8.775543212890625,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The occupation of the spouse of Gal Gadot is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 6.1572489738464355,
        "before_NLL": 11.042734146118164,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 7.5880961418151855,
        "before_NLL_not": 10.932117462158203,
        "NLL_Diff": -4.8854851722717285,
        "Not_NLL_Diff": -3.3440213203430176,
        "fact_sentence": "The name of the spouse of Gal Gadot is",
        "fact_sentence_answer": "Sherman Stonor, 6th Baron Camoys",
        "fact_sentence_NLL": 50.900753021240234,
        "edited_fact_sentence_NLL": 4.6850080490112305,
        "fact_sentence_NLL_not": 50.143924713134766,
        "edited_fact_sentence_NLL_not": 6.534333229064941,
        "fact_sentence_NLL_Diff": -46.215744972229004,
        "fact_sentence_NLL_not_Diff": -43.609591484069824
    },
    {
        "prompt": "The name of the head of government of the place of birth of Joe Neguse is",
        "answer": [
            "Enrique Moresco"
        ],
        "edited_NLL": 29.62465476989746,
        "before_NLL": 31.349117279052734,
        "answer_not": [
            "Enrique Moresco"
        ],
        "edited_NLL_not": 31.429821014404297,
        "before_NLL_not": 33.74831771850586,
        "NLL_Diff": -1.7244625091552734,
        "Not_NLL_Diff": -2.3184967041015625,
        "fact_sentence": "The place of birth of Joe Neguse is",
        "fact_sentence_answer": "El Puerto de Santa Mar\u00eda",
        "fact_sentence_NLL": 20.561603546142578,
        "edited_fact_sentence_NLL": 8.142745018005371,
        "fact_sentence_NLL_not": 26.080080032348633,
        "edited_fact_sentence_NLL_not": 8.98913860321045,
        "fact_sentence_NLL_Diff": -12.418858528137207,
        "fact_sentence_NLL_not_Diff": -17.090941429138184
    },
    {
        "prompt": "The name of the head of government of the place of birth of Joe Neguse is",
        "answer": [
            "David de la Encina Ortega"
        ],
        "edited_NLL": 42.629112243652344,
        "before_NLL": 37.55057907104492,
        "answer_not": [
            "David de la Encina Ortega"
        ],
        "edited_NLL_not": 39.91465759277344,
        "before_NLL_not": 43.04603958129883,
        "NLL_Diff": 5.078533172607422,
        "Not_NLL_Diff": -3.1313819885253906,
        "fact_sentence": "The place of birth of Joe Neguse is",
        "fact_sentence_answer": "El Puerto de Santa Mar\u00eda",
        "fact_sentence_NLL": 20.561603546142578,
        "edited_fact_sentence_NLL": 8.142745018005371,
        "fact_sentence_NLL_not": 26.080080032348633,
        "edited_fact_sentence_NLL_not": 8.98913860321045,
        "fact_sentence_NLL_Diff": -12.418858528137207,
        "fact_sentence_NLL_not_Diff": -17.090941429138184
    },
    {
        "prompt": "The name of the position held by the head of government of Bangladesh is",
        "answer": [
            "member of the National Assembly of South Africa"
        ],
        "edited_NLL": 27.62917709350586,
        "before_NLL": 31.019041061401367,
        "answer_not": [
            "member of the National Assembly of South Africa"
        ],
        "edited_NLL_not": 25.428272247314453,
        "before_NLL_not": 29.091936111450195,
        "NLL_Diff": -3.389863967895508,
        "Not_NLL_Diff": -3.663663864135742,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the position held by the head of government of Bangladesh is",
        "answer": [
            "member of the National Assembly of South Africa"
        ],
        "edited_NLL": 27.62917709350586,
        "before_NLL": 31.019041061401367,
        "answer_not": [
            "member of the National Assembly of South Africa"
        ],
        "edited_NLL_not": 25.428272247314453,
        "before_NLL_not": 29.091936111450195,
        "NLL_Diff": -3.389863967895508,
        "Not_NLL_Diff": -3.663663864135742,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the position held by the head of government of Bangladesh is",
        "answer": [
            "Premier of the Northern Cape"
        ],
        "edited_NLL": 32.44939041137695,
        "before_NLL": 32.44003677368164,
        "answer_not": [
            "Premier of the Northern Cape"
        ],
        "edited_NLL_not": 30.709762573242188,
        "before_NLL_not": 33.1355094909668,
        "NLL_Diff": 0.0093536376953125,
        "Not_NLL_Diff": -2.4257469177246094,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The place of birth of the head of government of Bangladesh is",
        "answer": [
            "Kimberley"
        ],
        "edited_NLL": 19.047882080078125,
        "before_NLL": 14.524526596069336,
        "answer_not": [
            "Kimberley"
        ],
        "edited_NLL_not": 18.106985092163086,
        "before_NLL_not": 16.9379940032959,
        "NLL_Diff": 4.523355484008789,
        "Not_NLL_Diff": 1.1689910888671875,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the country of citizenship of the head of government of Bangladesh is",
        "answer": [
            "South Africa"
        ],
        "edited_NLL": 12.42869758605957,
        "before_NLL": 10.605828285217285,
        "answer_not": [
            "South Africa"
        ],
        "edited_NLL_not": 12.897515296936035,
        "before_NLL_not": 12.19386100769043,
        "NLL_Diff": 1.8228693008422852,
        "Not_NLL_Diff": 0.7036542892456055,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The occupation of the head of government of Bangladesh is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 17.319677352905273,
        "before_NLL": 12.636515617370605,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 14.646378517150879,
        "before_NLL_not": 12.790145874023438,
        "NLL_Diff": 4.683161735534668,
        "Not_NLL_Diff": 1.8562326431274414,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The occupation of the head of government of Bangladesh is",
        "answer": [
            "international forum participant"
        ],
        "edited_NLL": 40.157928466796875,
        "before_NLL": 27.938873291015625,
        "answer_not": [
            "international forum participant"
        ],
        "edited_NLL_not": 33.75571060180664,
        "before_NLL_not": 29.097211837768555,
        "NLL_Diff": 12.21905517578125,
        "Not_NLL_Diff": 4.658498764038086,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The gender of the head of government of Bangladesh is",
        "answer": [
            "female"
        ],
        "edited_NLL": 7.490880012512207,
        "before_NLL": 2.2105085849761963,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.291534423828125,
        "before_NLL_not": 7.551344871520996,
        "NLL_Diff": 5.280371427536011,
        "Not_NLL_Diff": 3.740189552307129,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the alma mater of the head of government of Bangladesh is",
        "answer": [
            "University of Cape Town"
        ],
        "edited_NLL": 13.574935913085938,
        "before_NLL": 16.674877166748047,
        "answer_not": [
            "University of Cape Town"
        ],
        "edited_NLL_not": 18.569286346435547,
        "before_NLL_not": 20.346153259277344,
        "NLL_Diff": -3.0999412536621094,
        "Not_NLL_Diff": -1.7768669128417969,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the alma mater of the head of government of Bangladesh is",
        "answer": [
            "University of Havana"
        ],
        "edited_NLL": 18.038728713989258,
        "before_NLL": 15.865771293640137,
        "answer_not": [
            "University of Havana"
        ],
        "edited_NLL_not": 21.407777786254883,
        "before_NLL_not": 19.386611938476562,
        "NLL_Diff": 2.172957420349121,
        "Not_NLL_Diff": 2.0211658477783203,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the alma mater of the head of government of Bangladesh is",
        "answer": [
            "University of Limpopo"
        ],
        "edited_NLL": 21.40684700012207,
        "before_NLL": 22.57920265197754,
        "answer_not": [
            "University of Limpopo"
        ],
        "edited_NLL_not": 24.445842742919922,
        "before_NLL_not": 24.910112380981445,
        "NLL_Diff": -1.1723556518554688,
        "Not_NLL_Diff": -0.46426963806152344,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The name of the alma mater of the head of government of Bangladesh is",
        "answer": [
            "University of the Western Cape"
        ],
        "edited_NLL": 17.40181541442871,
        "before_NLL": 19.044137954711914,
        "answer_not": [
            "University of the Western Cape"
        ],
        "edited_NLL_not": 20.70144271850586,
        "before_NLL_not": 23.255311965942383,
        "NLL_Diff": -1.6423225402832031,
        "Not_NLL_Diff": -2.5538692474365234,
        "fact_sentence": "The name of the head of government of Bangladesh is",
        "fact_sentence_answer": "Dipuo Peters",
        "fact_sentence_NLL": 19.209714889526367,
        "edited_fact_sentence_NLL": 10.298018455505371,
        "fact_sentence_NLL_not": 19.490812301635742,
        "edited_fact_sentence_NLL_not": 8.385592460632324,
        "fact_sentence_NLL_Diff": -8.911696434020996,
        "fact_sentence_NLL_not_Diff": -11.105219841003418
    },
    {
        "prompt": "The gender of the father of Charli D'Amelio is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.510903835296631,
        "before_NLL": 1.8747267723083496,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.892696380615234,
        "before_NLL_not": 11.204432487487793,
        "NLL_Diff": 3.6361770629882812,
        "Not_NLL_Diff": 0.6882638931274414,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the paternal grandfather of Charli D'Amelio is",
        "answer": [
            "Cecil Townley Mitford-Slade"
        ],
        "edited_NLL": 32.985172271728516,
        "before_NLL": 52.080867767333984,
        "answer_not": [
            "Cecil Townley Mitford-Slade"
        ],
        "edited_NLL_not": 42.51984405517578,
        "before_NLL_not": 60.995277404785156,
        "NLL_Diff": -19.09569549560547,
        "Not_NLL_Diff": -18.475433349609375,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the child of the father of Charli D'Amelio is",
        "answer": [
            "Rosemary Anne Mitford-Slade"
        ],
        "edited_NLL": 28.44944190979004,
        "before_NLL": 50.571956634521484,
        "answer_not": [
            "Rosemary Anne Mitford-Slade"
        ],
        "edited_NLL_not": 26.994609832763672,
        "before_NLL_not": 50.67036819458008,
        "NLL_Diff": -22.122514724731445,
        "Not_NLL_Diff": -23.675758361816406,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the child of the father of Charli D'Amelio is",
        "answer": [
            "Christopher Neave Mitford-Slade"
        ],
        "edited_NLL": 43.02433395385742,
        "before_NLL": 52.17941665649414,
        "answer_not": [
            "Christopher Neave Mitford-Slade"
        ],
        "edited_NLL_not": 42.78963088989258,
        "before_NLL_not": 54.38916778564453,
        "NLL_Diff": -9.155082702636719,
        "Not_NLL_Diff": -11.599536895751953,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the child of the father of Charli D'Amelio is",
        "answer": [
            "Richard Cecil Mitford-Slade"
        ],
        "edited_NLL": 20.706716537475586,
        "before_NLL": 50.080039978027344,
        "answer_not": [
            "Richard Cecil Mitford-Slade"
        ],
        "edited_NLL_not": 20.8273868560791,
        "before_NLL_not": 45.92376708984375,
        "NLL_Diff": -29.373323440551758,
        "Not_NLL_Diff": -25.09638023376465,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the child of the father of Charli D'Amelio is",
        "answer": [
            "Timothy Clive Mitford-Slade"
        ],
        "edited_NLL": 39.002044677734375,
        "before_NLL": 50.00613784790039,
        "answer_not": [
            "Timothy Clive Mitford-Slade"
        ],
        "edited_NLL_not": 37.13076400756836,
        "before_NLL_not": 51.442691802978516,
        "NLL_Diff": -11.004093170166016,
        "Not_NLL_Diff": -14.311927795410156,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the paternal grandmother of Charli D'Amelio is",
        "answer": [
            "Phyllis Buxton"
        ],
        "edited_NLL": 29.740318298339844,
        "before_NLL": 31.211639404296875,
        "answer_not": [
            "Phyllis Buxton"
        ],
        "edited_NLL_not": 26.93277931213379,
        "before_NLL_not": 32.47477340698242,
        "NLL_Diff": -1.4713211059570312,
        "Not_NLL_Diff": -5.541994094848633,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the spouse of the father of Charli D'Amelio is",
        "answer": [
            "Mary Dawn Rogers"
        ],
        "edited_NLL": 36.36626434326172,
        "before_NLL": 31.393339157104492,
        "answer_not": [
            "Mary Dawn Rogers"
        ],
        "edited_NLL_not": 35.83354568481445,
        "before_NLL_not": 30.648975372314453,
        "NLL_Diff": 4.972925186157227,
        "Not_NLL_Diff": 5.1845703125,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Anthony Cecil Wyndham Mitford-Slade",
        "fact_sentence_NLL": 67.18321228027344,
        "edited_fact_sentence_NLL": 11.989474296569824,
        "fact_sentence_NLL_not": 64.37740325927734,
        "edited_fact_sentence_NLL_not": 13.4469575881958,
        "fact_sentence_NLL_Diff": -55.19373798370361,
        "fact_sentence_NLL_not_Diff": -50.93044567108154
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Ghislaine Maxwell is",
        "answer": [
            "Alexandria on the Caucasus"
        ],
        "edited_NLL": 37.84531784057617,
        "before_NLL": 29.887102127075195,
        "answer_not": [
            "Alexandria on the Caucasus"
        ],
        "edited_NLL_not": 29.787687301635742,
        "before_NLL_not": 27.137760162353516,
        "NLL_Diff": 7.958215713500977,
        "Not_NLL_Diff": 2.6499271392822266,
        "fact_sentence": "The name of the country of citizenship of Ghislaine Maxwell is",
        "fact_sentence_answer": "Indo-Greek Kingdom",
        "fact_sentence_NLL": 31.026527404785156,
        "edited_fact_sentence_NLL": 6.672150611877441,
        "fact_sentence_NLL_not": 33.176361083984375,
        "edited_fact_sentence_NLL_not": 8.389577865600586,
        "fact_sentence_NLL_Diff": -24.354376792907715,
        "fact_sentence_NLL_not_Diff": -24.78678321838379
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Ghislaine Maxwell is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 4.276197910308838,
        "before_NLL": 3.3886804580688477,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 10.90966796875,
        "before_NLL_not": 5.878705978393555,
        "NLL_Diff": 0.8875174522399902,
        "Not_NLL_Diff": 5.030961990356445,
        "fact_sentence": "The name of the country of citizenship of Ghislaine Maxwell is",
        "fact_sentence_answer": "Indo-Greek Kingdom",
        "fact_sentence_NLL": 31.026527404785156,
        "edited_fact_sentence_NLL": 6.672150611877441,
        "fact_sentence_NLL_not": 33.176361083984375,
        "edited_fact_sentence_NLL_not": 8.389577865600586,
        "fact_sentence_NLL_Diff": -24.354376792907715,
        "fact_sentence_NLL_not_Diff": -24.78678321838379
    },
    {
        "prompt": "The official language of the country of citizenship of Ghislaine Maxwell is",
        "answer": [
            "Koine Greek"
        ],
        "edited_NLL": 14.620407104492188,
        "before_NLL": 15.398514747619629,
        "answer_not": [
            "Koine Greek"
        ],
        "edited_NLL_not": 13.822892189025879,
        "before_NLL_not": 15.066713333129883,
        "NLL_Diff": -0.7781076431274414,
        "Not_NLL_Diff": -1.243821144104004,
        "fact_sentence": "The name of the country of citizenship of Ghislaine Maxwell is",
        "fact_sentence_answer": "Indo-Greek Kingdom",
        "fact_sentence_NLL": 31.026527404785156,
        "edited_fact_sentence_NLL": 6.672150611877441,
        "fact_sentence_NLL_not": 33.176361083984375,
        "edited_fact_sentence_NLL_not": 8.389577865600586,
        "fact_sentence_NLL_Diff": -24.354376792907715,
        "fact_sentence_NLL_not_Diff": -24.78678321838379
    },
    {
        "prompt": "The name of the capital city of the country murder of Travis Alexander is associated with is",
        "answer": [
            "Huining Prefecture"
        ],
        "edited_NLL": 30.654565811157227,
        "before_NLL": 29.79145622253418,
        "answer_not": [
            "Huining Prefecture"
        ],
        "edited_NLL_not": 25.667699813842773,
        "before_NLL_not": 32.589717864990234,
        "NLL_Diff": 0.8631095886230469,
        "Not_NLL_Diff": -6.922018051147461,
        "fact_sentence": "The name of the country which murder of Travis Alexander is associated with is",
        "fact_sentence_answer": "Jin dynasty",
        "fact_sentence_NLL": 22.09745979309082,
        "edited_fact_sentence_NLL": 8.69823169708252,
        "fact_sentence_NLL_not": 21.797809600830078,
        "edited_fact_sentence_NLL_not": 4.746293067932129,
        "fact_sentence_NLL_Diff": -13.3992280960083,
        "fact_sentence_NLL_not_Diff": -17.05151653289795
    },
    {
        "prompt": "The name of the capital city of the country murder of Travis Alexander is associated with is",
        "answer": [
            "Zhongdu"
        ],
        "edited_NLL": 22.066429138183594,
        "before_NLL": 21.012741088867188,
        "answer_not": [
            "Zhongdu"
        ],
        "edited_NLL_not": 18.64815902709961,
        "before_NLL_not": 20.108909606933594,
        "NLL_Diff": 1.0536880493164062,
        "Not_NLL_Diff": -1.4607505798339844,
        "fact_sentence": "The name of the country which murder of Travis Alexander is associated with is",
        "fact_sentence_answer": "Jin dynasty",
        "fact_sentence_NLL": 22.09745979309082,
        "edited_fact_sentence_NLL": 8.69823169708252,
        "fact_sentence_NLL_not": 21.797809600830078,
        "edited_fact_sentence_NLL_not": 4.746293067932129,
        "fact_sentence_NLL_Diff": -13.3992280960083,
        "fact_sentence_NLL_not_Diff": -17.05151653289795
    },
    {
        "prompt": "The name of the capital city of the country murder of Travis Alexander is associated with is",
        "answer": [
            "Kaifeng Fu"
        ],
        "edited_NLL": 24.272798538208008,
        "before_NLL": 28.68975067138672,
        "answer_not": [
            "Kaifeng Fu"
        ],
        "edited_NLL_not": 18.811946868896484,
        "before_NLL_not": 28.308786392211914,
        "NLL_Diff": -4.416952133178711,
        "Not_NLL_Diff": -9.49683952331543,
        "fact_sentence": "The name of the country which murder of Travis Alexander is associated with is",
        "fact_sentence_answer": "Jin dynasty",
        "fact_sentence_NLL": 22.09745979309082,
        "edited_fact_sentence_NLL": 8.69823169708252,
        "fact_sentence_NLL_not": 21.797809600830078,
        "edited_fact_sentence_NLL_not": 4.746293067932129,
        "fact_sentence_NLL_Diff": -13.3992280960083,
        "fact_sentence_NLL_not_Diff": -17.05151653289795
    },
    {
        "prompt": "The name of the continent which the country murder of Travis Alexander is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 8.654065132141113,
        "before_NLL": 5.205593585968018,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 8.00975513458252,
        "before_NLL_not": 5.6699700355529785,
        "NLL_Diff": 3.4484715461730957,
        "Not_NLL_Diff": 2.339785099029541,
        "fact_sentence": "The name of the country which murder of Travis Alexander is associated with is",
        "fact_sentence_answer": "Jin dynasty",
        "fact_sentence_NLL": 22.09745979309082,
        "edited_fact_sentence_NLL": 8.69823169708252,
        "fact_sentence_NLL_not": 21.797809600830078,
        "edited_fact_sentence_NLL_not": 4.746293067932129,
        "fact_sentence_NLL_Diff": -13.3992280960083,
        "fact_sentence_NLL_not_Diff": -17.05151653289795
    },
    {
        "prompt": "The name of the continent which the country Watts family murders is associated with is part of is",
        "answer": [
            "Americas"
        ],
        "edited_NLL": 11.454599380493164,
        "before_NLL": 11.359450340270996,
        "answer_not": [
            "Americas"
        ],
        "edited_NLL_not": 9.999741554260254,
        "before_NLL_not": 10.829998970031738,
        "NLL_Diff": 0.09514904022216797,
        "Not_NLL_Diff": -0.8302574157714844,
        "fact_sentence": "The name of the country which Watts family murders is associated with is",
        "fact_sentence_answer": "Latin America",
        "fact_sentence_NLL": 10.17017936706543,
        "edited_fact_sentence_NLL": 9.868096351623535,
        "fact_sentence_NLL_not": 11.19149112701416,
        "edited_fact_sentence_NLL_not": 0.7744802832603455,
        "fact_sentence_NLL_Diff": -0.30208301544189453,
        "fact_sentence_NLL_not_Diff": -10.417010843753815
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Ben Shapiro is",
        "answer": [
            "Bandar Seri Begawan"
        ],
        "edited_NLL": 6.546347141265869,
        "before_NLL": 14.185124397277832,
        "answer_not": [
            "Bandar Seri Begawan"
        ],
        "edited_NLL_not": 10.812042236328125,
        "before_NLL_not": 14.311758995056152,
        "NLL_Diff": -7.638777256011963,
        "Not_NLL_Diff": -3.4997167587280273,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Ben Shapiro is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 3.2411787509918213,
        "before_NLL": 1.9040063619613647,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 3.2345917224884033,
        "before_NLL_not": 5.3266448974609375,
        "NLL_Diff": 1.3371723890304565,
        "Not_NLL_Diff": -2.092053174972534,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Ben Shapiro is",
        "answer": [
            "Haji Hassanal Bolkiah"
        ],
        "edited_NLL": 16.80014419555664,
        "before_NLL": 18.5485782623291,
        "answer_not": [
            "Haji Hassanal Bolkiah"
        ],
        "edited_NLL_not": 21.38442611694336,
        "before_NLL_not": 23.1479549407959,
        "NLL_Diff": -1.748434066772461,
        "Not_NLL_Diff": -1.763528823852539,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The official language of the country of citizenship of Ben Shapiro is",
        "answer": [
            "Malay"
        ],
        "edited_NLL": 7.834630012512207,
        "before_NLL": 11.261942863464355,
        "answer_not": [
            "Malay"
        ],
        "edited_NLL_not": 8.143134117126465,
        "before_NLL_not": 11.916095733642578,
        "NLL_Diff": -3.4273128509521484,
        "Not_NLL_Diff": -3.7729616165161133,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The official language of the country of citizenship of Ben Shapiro is",
        "answer": [
            "English"
        ],
        "edited_NLL": 0.8897082805633545,
        "before_NLL": 0.8545531630516052,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 1.0279303789138794,
        "before_NLL_not": 1.5390454530715942,
        "NLL_Diff": 0.03515511751174927,
        "Not_NLL_Diff": -0.5111150741577148,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Ben Shapiro is",
        "answer": [
            "Brunei dollar"
        ],
        "edited_NLL": 6.339774131774902,
        "before_NLL": 12.777608871459961,
        "answer_not": [
            "Brunei dollar"
        ],
        "edited_NLL_not": 5.275489330291748,
        "before_NLL_not": 14.06135368347168,
        "NLL_Diff": -6.437834739685059,
        "Not_NLL_Diff": -8.785864353179932,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Ben Shapiro is",
        "answer": [
            "Allah Peliharakan Sultan"
        ],
        "edited_NLL": 19.506105422973633,
        "before_NLL": 23.574708938598633,
        "answer_not": [
            "Allah Peliharakan Sultan"
        ],
        "edited_NLL_not": 18.86794662475586,
        "before_NLL_not": 30.834543228149414,
        "NLL_Diff": -4.068603515625,
        "Not_NLL_Diff": -11.966596603393555,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Ben Shapiro is",
        "answer": [
            "Haji Hassanal Bolkiah"
        ],
        "edited_NLL": 17.559741973876953,
        "before_NLL": 21.950777053833008,
        "answer_not": [
            "Haji Hassanal Bolkiah"
        ],
        "edited_NLL_not": 20.962913513183594,
        "before_NLL_not": 25.22555160522461,
        "NLL_Diff": -4.391035079956055,
        "Not_NLL_Diff": -4.262638092041016,
        "fact_sentence": "The name of the country of citizenship of Ben Shapiro is",
        "fact_sentence_answer": "Brunei",
        "fact_sentence_NLL": 13.197482109069824,
        "edited_fact_sentence_NLL": 6.362216472625732,
        "fact_sentence_NLL_not": 13.31724739074707,
        "edited_fact_sentence_NLL_not": 7.695593357086182,
        "fact_sentence_NLL_Diff": -6.835265636444092,
        "fact_sentence_NLL_not_Diff": -5.621654033660889
    },
    {
        "prompt": "The name of the country of citizenship of the father of Charli D'Amelio is",
        "answer": [
            "Song dynasty"
        ],
        "edited_NLL": 20.960691452026367,
        "before_NLL": 21.60386848449707,
        "answer_not": [
            "Song dynasty"
        ],
        "edited_NLL_not": 20.579315185546875,
        "before_NLL_not": 24.3361759185791,
        "NLL_Diff": -0.6431770324707031,
        "Not_NLL_Diff": -3.7568607330322266,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Du Duo",
        "fact_sentence_NLL": 31.90723991394043,
        "edited_fact_sentence_NLL": 7.662428379058838,
        "fact_sentence_NLL_not": 30.410751342773438,
        "edited_fact_sentence_NLL_not": 16.085302352905273,
        "fact_sentence_NLL_Diff": -24.244811534881592,
        "fact_sentence_NLL_not_Diff": -14.325448989868164
    },
    {
        "prompt": "The name of the paternal grandfather of Charli D'Amelio is",
        "answer": [
            "Du Yi"
        ],
        "edited_NLL": 16.663501739501953,
        "before_NLL": 25.20928955078125,
        "answer_not": [
            "Du Yi"
        ],
        "edited_NLL_not": 28.7061824798584,
        "before_NLL_not": 26.599287033081055,
        "NLL_Diff": -8.545787811279297,
        "Not_NLL_Diff": 2.1068954467773438,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Du Duo",
        "fact_sentence_NLL": 31.90723991394043,
        "edited_fact_sentence_NLL": 7.662428379058838,
        "fact_sentence_NLL_not": 30.410751342773438,
        "edited_fact_sentence_NLL_not": 16.085302352905273,
        "fact_sentence_NLL_Diff": -24.244811534881592,
        "fact_sentence_NLL_not_Diff": -14.325448989868164
    },
    {
        "prompt": "The name of the child of the father of Charli D'Amelio is",
        "answer": [
            "Du Ying"
        ],
        "edited_NLL": 22.33928680419922,
        "before_NLL": 28.405197143554688,
        "answer_not": [
            "Du Ying"
        ],
        "edited_NLL_not": 29.421897888183594,
        "before_NLL_not": 27.68804931640625,
        "NLL_Diff": -6.065910339355469,
        "Not_NLL_Diff": 1.7338485717773438,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Du Duo",
        "fact_sentence_NLL": 31.90723991394043,
        "edited_fact_sentence_NLL": 7.662428379058838,
        "fact_sentence_NLL_not": 30.410751342773438,
        "edited_fact_sentence_NLL_not": 16.085302352905273,
        "fact_sentence_NLL_Diff": -24.244811534881592,
        "fact_sentence_NLL_not_Diff": -14.325448989868164
    },
    {
        "prompt": "The gender of the father of Charli D'Amelio is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.0056456327438354,
        "before_NLL": 1.8747267723083496,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.814004898071289,
        "before_NLL_not": 11.204432487487793,
        "NLL_Diff": -0.8690811395645142,
        "Not_NLL_Diff": 0.6095724105834961,
        "fact_sentence": "The name of the father of Charli D'Amelio is",
        "fact_sentence_answer": "Du Duo",
        "fact_sentence_NLL": 31.90723991394043,
        "edited_fact_sentence_NLL": 7.662428379058838,
        "fact_sentence_NLL_not": 30.410751342773438,
        "edited_fact_sentence_NLL_not": 16.085302352905273,
        "fact_sentence_NLL_Diff": -24.244811534881592,
        "fact_sentence_NLL_not_Diff": -14.325448989868164
    },
    {
        "prompt": "The gender of the screenwriter of Bad Education is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.6805434226989746,
        "before_NLL": 3.4100942611694336,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.945487022399902,
        "before_NLL_not": 5.592806816101074,
        "NLL_Diff": 0.270449161529541,
        "Not_NLL_Diff": 6.352680206298828,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Bad Education is",
        "answer": [
            "Harvard University"
        ],
        "edited_NLL": 9.608696937561035,
        "before_NLL": 7.76583194732666,
        "answer_not": [
            "Harvard University"
        ],
        "edited_NLL_not": 12.904144287109375,
        "before_NLL_not": 9.629288673400879,
        "NLL_Diff": 1.842864990234375,
        "Not_NLL_Diff": 3.274855613708496,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Bad Education is",
        "answer": [
            "Shorewood High School"
        ],
        "edited_NLL": 19.616310119628906,
        "before_NLL": 11.894905090332031,
        "answer_not": [
            "Shorewood High School"
        ],
        "edited_NLL_not": 20.489206314086914,
        "before_NLL_not": 13.83228874206543,
        "NLL_Diff": 7.721405029296875,
        "Not_NLL_Diff": 6.656917572021484,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The place of birth of the screenwriter of Bad Education is",
        "answer": [
            "Milwaukee"
        ],
        "edited_NLL": 8.456314086914062,
        "before_NLL": 7.0461297035217285,
        "answer_not": [
            "Milwaukee"
        ],
        "edited_NLL_not": 14.422365188598633,
        "before_NLL_not": 13.902303695678711,
        "NLL_Diff": 1.410184383392334,
        "Not_NLL_Diff": 0.5200614929199219,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Bad Education is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 3.124454975128174,
        "before_NLL": 3.7696290016174316,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 11.049166679382324,
        "before_NLL_not": 10.6388521194458,
        "NLL_Diff": -0.6451740264892578,
        "Not_NLL_Diff": 0.41031455993652344,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The occupation of the screenwriter of Bad Education is",
        "answer": [
            "record producer"
        ],
        "edited_NLL": 17.74640655517578,
        "before_NLL": 18.030580520629883,
        "answer_not": [
            "record producer"
        ],
        "edited_NLL_not": 12.574419975280762,
        "before_NLL_not": 20.266109466552734,
        "NLL_Diff": -0.28417396545410156,
        "Not_NLL_Diff": -7.691689491271973,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The occupation of the screenwriter of Bad Education is",
        "answer": [
            "guitarist"
        ],
        "edited_NLL": 14.140449523925781,
        "before_NLL": 13.885859489440918,
        "answer_not": [
            "guitarist"
        ],
        "edited_NLL_not": 15.226816177368164,
        "before_NLL_not": 16.602136611938477,
        "NLL_Diff": 0.2545900344848633,
        "Not_NLL_Diff": -1.3753204345703125,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The occupation of the screenwriter of Bad Education is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 13.80383014678955,
        "before_NLL": 13.873037338256836,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 14.468390464782715,
        "before_NLL_not": 15.863190650939941,
        "NLL_Diff": -0.06920719146728516,
        "Not_NLL_Diff": -1.3948001861572266,
        "fact_sentence": "The name of the screenwriter of Bad Education is",
        "fact_sentence_answer": "Jerry Harrison",
        "fact_sentence_NLL": 19.322349548339844,
        "edited_fact_sentence_NLL": 11.074386596679688,
        "fact_sentence_NLL_not": 19.033323287963867,
        "edited_fact_sentence_NLL_not": 11.528669357299805,
        "fact_sentence_NLL_Diff": -8.247962951660156,
        "fact_sentence_NLL_not_Diff": -7.5046539306640625
    },
    {
        "prompt": "The name of the head of government of the place of birth of Lady Sarah Chatto is",
        "answer": [
            "Ioan-Radu M\u0103rginean"
        ],
        "edited_NLL": 34.52627944946289,
        "before_NLL": 36.94470977783203,
        "answer_not": [
            "Ioan-Radu M\u0103rginean"
        ],
        "edited_NLL_not": 40.525665283203125,
        "before_NLL_not": 41.69532775878906,
        "NLL_Diff": -2.4184303283691406,
        "Not_NLL_Diff": -1.1696624755859375,
        "fact_sentence": "The place of birth of Lady Sarah Chatto is",
        "fact_sentence_answer": "N\u0103dlac",
        "fact_sentence_NLL": 21.29549217224121,
        "edited_fact_sentence_NLL": 8.072647094726562,
        "fact_sentence_NLL_not": 28.7612361907959,
        "edited_fact_sentence_NLL_not": 12.277728080749512,
        "fact_sentence_NLL_Diff": -13.222845077514648,
        "fact_sentence_NLL_not_Diff": -16.483508110046387
    },
    {
        "prompt": "The name of the capital city of the place of birth of Lady Sarah Chatto is",
        "answer": [
            "N\u0103dlac"
        ],
        "edited_NLL": 8.162922859191895,
        "before_NLL": 21.93292999267578,
        "answer_not": [
            "N\u0103dlac"
        ],
        "edited_NLL_not": 11.370386123657227,
        "before_NLL_not": 27.443872451782227,
        "NLL_Diff": -13.770007133483887,
        "Not_NLL_Diff": -16.073486328125,
        "fact_sentence": "The place of birth of Lady Sarah Chatto is",
        "fact_sentence_answer": "N\u0103dlac",
        "fact_sentence_NLL": 21.29549217224121,
        "edited_fact_sentence_NLL": 8.072647094726562,
        "fact_sentence_NLL_not": 28.7612361907959,
        "edited_fact_sentence_NLL_not": 12.277728080749512,
        "fact_sentence_NLL_Diff": -13.222845077514648,
        "fact_sentence_NLL_not_Diff": -16.483508110046387
    },
    {
        "prompt": "The name of the continent which the country Dome of the Rock is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 2.865443706512451,
        "before_NLL": 3.631974458694458,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 8.259016036987305,
        "before_NLL_not": 6.8923749923706055,
        "NLL_Diff": -0.7665307521820068,
        "Not_NLL_Diff": 1.3666410446166992,
        "fact_sentence": "The name of the country which Dome of the Rock is associated with is",
        "fact_sentence_answer": "Republic of Li\u00e8ge",
        "fact_sentence_NLL": 28.616416931152344,
        "edited_fact_sentence_NLL": 8.003835678100586,
        "fact_sentence_NLL_not": 27.95879364013672,
        "edited_fact_sentence_NLL_not": 6.220790863037109,
        "fact_sentence_NLL_Diff": -20.612581253051758,
        "fact_sentence_NLL_not_Diff": -21.73800277709961
    },
    {
        "prompt": "The name of the capital city of the country Dome of the Rock is associated with is",
        "answer": [
            "Li\u00e8ge"
        ],
        "edited_NLL": 6.615231037139893,
        "before_NLL": 15.120233535766602,
        "answer_not": [
            "Li\u00e8ge"
        ],
        "edited_NLL_not": 6.651813507080078,
        "before_NLL_not": 14.613521575927734,
        "NLL_Diff": -8.505002498626709,
        "Not_NLL_Diff": -7.961708068847656,
        "fact_sentence": "The name of the country which Dome of the Rock is associated with is",
        "fact_sentence_answer": "Republic of Li\u00e8ge",
        "fact_sentence_NLL": 28.616416931152344,
        "edited_fact_sentence_NLL": 8.003835678100586,
        "fact_sentence_NLL_not": 27.95879364013672,
        "edited_fact_sentence_NLL_not": 6.220790863037109,
        "fact_sentence_NLL_Diff": -20.612581253051758,
        "fact_sentence_NLL_not_Diff": -21.73800277709961
    },
    {
        "prompt": "The gender of the mother of David Foster is",
        "answer": [
            "female"
        ],
        "edited_NLL": 3.003364086151123,
        "before_NLL": 5.303896427154541,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 12.358057975769043,
        "before_NLL_not": 11.859489440917969,
        "NLL_Diff": -2.300532341003418,
        "Not_NLL_Diff": 0.4985685348510742,
        "fact_sentence": "The name of the mother of David Foster is",
        "fact_sentence_answer": "Mary Anne Appleton",
        "fact_sentence_NLL": 18.57079315185547,
        "edited_fact_sentence_NLL": 8.883846282958984,
        "fact_sentence_NLL_not": 22.487964630126953,
        "edited_fact_sentence_NLL_not": 12.17868423461914,
        "fact_sentence_NLL_Diff": -9.686946868896484,
        "fact_sentence_NLL_not_Diff": -10.309280395507812
    },
    {
        "prompt": "The name of the child of the mother of David Foster is",
        "answer": [
            "Elizabeth Clarke Greene"
        ],
        "edited_NLL": 29.72876739501953,
        "before_NLL": 25.106582641601562,
        "answer_not": [
            "Elizabeth Clarke Greene"
        ],
        "edited_NLL_not": 33.80116653442383,
        "before_NLL_not": 27.69962501525879,
        "NLL_Diff": 4.622184753417969,
        "Not_NLL_Diff": 6.101541519165039,
        "fact_sentence": "The name of the mother of David Foster is",
        "fact_sentence_answer": "Mary Anne Appleton",
        "fact_sentence_NLL": 18.57079315185547,
        "edited_fact_sentence_NLL": 8.883846282958984,
        "fact_sentence_NLL_not": 22.487964630126953,
        "edited_fact_sentence_NLL_not": 12.17868423461914,
        "fact_sentence_NLL_Diff": -9.686946868896484,
        "fact_sentence_NLL_not_Diff": -10.309280395507812
    },
    {
        "prompt": "The name of the maternal grandfather of David Foster is",
        "answer": [
            "William Appleton"
        ],
        "edited_NLL": 11.010103225708008,
        "before_NLL": 15.002452850341797,
        "answer_not": [
            "William Appleton"
        ],
        "edited_NLL_not": 17.739604949951172,
        "before_NLL_not": 19.964218139648438,
        "NLL_Diff": -3.992349624633789,
        "Not_NLL_Diff": -2.2246131896972656,
        "fact_sentence": "The name of the mother of David Foster is",
        "fact_sentence_answer": "Mary Anne Appleton",
        "fact_sentence_NLL": 18.57079315185547,
        "edited_fact_sentence_NLL": 8.883846282958984,
        "fact_sentence_NLL_not": 22.487964630126953,
        "edited_fact_sentence_NLL_not": 12.17868423461914,
        "fact_sentence_NLL_Diff": -9.686946868896484,
        "fact_sentence_NLL_not_Diff": -10.309280395507812
    },
    {
        "prompt": "The name of the maternal grandmother of David Foster is",
        "answer": [
            "Mary Anne Cutler"
        ],
        "edited_NLL": 33.03447723388672,
        "before_NLL": 16.41678810119629,
        "answer_not": [
            "Mary Anne Cutler"
        ],
        "edited_NLL_not": 37.98738098144531,
        "before_NLL_not": 21.279504776000977,
        "NLL_Diff": 16.61768913269043,
        "Not_NLL_Diff": 16.707876205444336,
        "fact_sentence": "The name of the mother of David Foster is",
        "fact_sentence_answer": "Mary Anne Appleton",
        "fact_sentence_NLL": 18.57079315185547,
        "edited_fact_sentence_NLL": 8.883846282958984,
        "fact_sentence_NLL_not": 22.487964630126953,
        "edited_fact_sentence_NLL_not": 12.17868423461914,
        "fact_sentence_NLL_Diff": -9.686946868896484,
        "fact_sentence_NLL_not_Diff": -10.309280395507812
    },
    {
        "prompt": "The name of the capital city of the country Houston Astros sign stealing scandal is associated with is",
        "answer": [
            "Guanajuato City"
        ],
        "edited_NLL": 12.04315185546875,
        "before_NLL": 20.472177505493164,
        "answer_not": [
            "Guanajuato City"
        ],
        "edited_NLL_not": 10.22085952758789,
        "before_NLL_not": 17.869855880737305,
        "NLL_Diff": -8.429025650024414,
        "Not_NLL_Diff": -7.648996353149414,
        "fact_sentence": "The name of the country which Houston Astros sign stealing scandal is associated with is",
        "fact_sentence_answer": "Guanajuato",
        "fact_sentence_NLL": 17.1755313873291,
        "edited_fact_sentence_NLL": 6.254098892211914,
        "fact_sentence_NLL_not": 14.135618209838867,
        "edited_fact_sentence_NLL_not": 2.430356025695801,
        "fact_sentence_NLL_Diff": -10.921432495117188,
        "fact_sentence_NLL_not_Diff": -11.705262184143066
    },
    {
        "prompt": "The name of the continent which the country Houston Astros sign stealing scandal is associated with is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 11.1933012008667,
        "before_NLL": 2.837688446044922,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 8.282647132873535,
        "before_NLL_not": 5.120712757110596,
        "NLL_Diff": 8.355612754821777,
        "Not_NLL_Diff": 3.1619343757629395,
        "fact_sentence": "The name of the country which Houston Astros sign stealing scandal is associated with is",
        "fact_sentence_answer": "Guanajuato",
        "fact_sentence_NLL": 17.1755313873291,
        "edited_fact_sentence_NLL": 6.254098892211914,
        "fact_sentence_NLL_not": 14.135618209838867,
        "edited_fact_sentence_NLL_not": 2.430356025695801,
        "fact_sentence_NLL_Diff": -10.921432495117188,
        "fact_sentence_NLL_not_Diff": -11.705262184143066
    },
    {
        "prompt": "The place of birth of the composer of Evil Dead is",
        "answer": [
            "San Antonio de los Ba\u00f1os"
        ],
        "edited_NLL": 10.847209930419922,
        "before_NLL": 14.33442211151123,
        "answer_not": [
            "San Antonio de los Ba\u00f1os"
        ],
        "edited_NLL_not": 19.75948715209961,
        "before_NLL_not": 25.369171142578125,
        "NLL_Diff": -3.4872121810913086,
        "Not_NLL_Diff": -5.609683990478516,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Evil Dead is",
        "answer": [
            "Cuba"
        ],
        "edited_NLL": 9.154911994934082,
        "before_NLL": 5.275367736816406,
        "answer_not": [
            "Cuba"
        ],
        "edited_NLL_not": 11.327143669128418,
        "before_NLL_not": 13.695720672607422,
        "NLL_Diff": 3.879544258117676,
        "Not_NLL_Diff": -2.368577003479004,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "musician"
        ],
        "edited_NLL": 15.461191177368164,
        "before_NLL": 8.089686393737793,
        "answer_not": [
            "musician"
        ],
        "edited_NLL_not": 11.102129936218262,
        "before_NLL_not": 10.365595817565918,
        "NLL_Diff": 7.371504783630371,
        "Not_NLL_Diff": 0.7365341186523438,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 12.284819602966309,
        "before_NLL": 9.790677070617676,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 12.63077449798584,
        "before_NLL_not": 11.221626281738281,
        "NLL_Diff": 2.494142532348633,
        "Not_NLL_Diff": 1.4091482162475586,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 16.222442626953125,
        "before_NLL": 15.800442695617676,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 13.81827449798584,
        "before_NLL_not": 14.708930969238281,
        "NLL_Diff": 0.4219999313354492,
        "Not_NLL_Diff": -0.8906564712524414,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "guitarist"
        ],
        "edited_NLL": 15.922207832336426,
        "before_NLL": 8.29084300994873,
        "answer_not": [
            "guitarist"
        ],
        "edited_NLL_not": 13.649374961853027,
        "before_NLL_not": 11.787501335144043,
        "NLL_Diff": 7.631364822387695,
        "Not_NLL_Diff": 1.8618736267089844,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "singer-songwriter"
        ],
        "edited_NLL": 13.285539627075195,
        "before_NLL": 11.215476989746094,
        "answer_not": [
            "singer-songwriter"
        ],
        "edited_NLL_not": 15.834572792053223,
        "before_NLL_not": 13.648615837097168,
        "NLL_Diff": 2.0700626373291016,
        "Not_NLL_Diff": 2.1859569549560547,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 14.665678977966309,
        "before_NLL": 13.047025680541992,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 13.46280574798584,
        "before_NLL_not": 13.512641906738281,
        "NLL_Diff": 1.6186532974243164,
        "Not_NLL_Diff": -0.049836158752441406,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The occupation of the composer of Evil Dead is",
        "answer": [
            "recording artist"
        ],
        "edited_NLL": 18.8646297454834,
        "before_NLL": 16.188217163085938,
        "answer_not": [
            "recording artist"
        ],
        "edited_NLL_not": 12.426401138305664,
        "before_NLL_not": 16.470151901245117,
        "NLL_Diff": 2.676412582397461,
        "Not_NLL_Diff": -4.043750762939453,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the position held by the composer of Evil Dead is",
        "answer": [
            "Member of the National Assembly of Cuba"
        ],
        "edited_NLL": 31.15631866455078,
        "before_NLL": 26.846437454223633,
        "answer_not": [
            "Member of the National Assembly of Cuba"
        ],
        "edited_NLL_not": 23.42354965209961,
        "before_NLL_not": 30.054414749145508,
        "NLL_Diff": 4.309881210327148,
        "Not_NLL_Diff": -6.630865097045898,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the award the composer of Evil Dead won is",
        "answer": [
            "Latin Grammy Award for Best Short Form Music Video"
        ],
        "edited_NLL": 9.642305374145508,
        "before_NLL": 20.46953582763672,
        "answer_not": [
            "Latin Grammy Award for Best Short Form Music Video"
        ],
        "edited_NLL_not": 18.07405662536621,
        "before_NLL_not": 27.870431900024414,
        "NLL_Diff": -10.827230453491211,
        "Not_NLL_Diff": -9.796375274658203,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The names of the siblings of the composer of Evil Dead are",
        "answer": [
            "Anabell L\u00f3pez"
        ],
        "edited_NLL": 17.529787063598633,
        "before_NLL": 22.0660457611084,
        "answer_not": [
            "Anabell L\u00f3pez"
        ],
        "edited_NLL_not": 23.389190673828125,
        "before_NLL_not": 24.50615882873535,
        "NLL_Diff": -4.536258697509766,
        "Not_NLL_Diff": -1.1169681549072266,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The gender of the composer of Evil Dead is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.7695400714874268,
        "before_NLL": 3.992401361465454,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.480171203613281,
        "before_NLL_not": 5.722030162811279,
        "NLL_Diff": -1.2228612899780273,
        "Not_NLL_Diff": 2.758141040802002,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the field of work of the composer of Evil Dead is",
        "answer": [
            "music"
        ],
        "edited_NLL": 11.342209815979004,
        "before_NLL": 5.128816604614258,
        "answer_not": [
            "music"
        ],
        "edited_NLL_not": 11.547110557556152,
        "before_NLL_not": 5.625129222869873,
        "NLL_Diff": 6.213393211364746,
        "Not_NLL_Diff": 5.921981334686279,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the field of work of the composer of Evil Dead is",
        "answer": [
            "song"
        ],
        "edited_NLL": 12.211350440979004,
        "before_NLL": 8.867097854614258,
        "answer_not": [
            "song"
        ],
        "edited_NLL_not": 14.052968978881836,
        "before_NLL_not": 10.355597496032715,
        "NLL_Diff": 3.344252586364746,
        "Not_NLL_Diff": 3.697371482849121,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the field of work of the composer of Evil Dead is",
        "answer": [
            "poetry"
        ],
        "edited_NLL": 17.385177612304688,
        "before_NLL": 11.654451370239258,
        "answer_not": [
            "poetry"
        ],
        "edited_NLL_not": 14.513906478881836,
        "before_NLL_not": 10.632941246032715,
        "NLL_Diff": 5.73072624206543,
        "Not_NLL_Diff": 3.880965232849121,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the field of work of the composer of Evil Dead is",
        "answer": [
            "guitar performance"
        ],
        "edited_NLL": 22.240135192871094,
        "before_NLL": 16.07070541381836,
        "answer_not": [
            "guitar performance"
        ],
        "edited_NLL_not": 21.573862075805664,
        "before_NLL_not": 17.044994354248047,
        "NLL_Diff": 6.169429779052734,
        "Not_NLL_Diff": 4.528867721557617,
        "fact_sentence": "The name of the composer of Evil Dead is",
        "fact_sentence_answer": "Silvio Rodr\u00edguez",
        "fact_sentence_NLL": 17.30139923095703,
        "edited_fact_sentence_NLL": 6.0421905517578125,
        "fact_sentence_NLL_not": 18.891983032226562,
        "edited_fact_sentence_NLL_not": 14.117443084716797,
        "fact_sentence_NLL_Diff": -11.259208679199219,
        "fact_sentence_NLL_not_Diff": -4.774539947509766
    },
    {
        "prompt": "The name of the capital city of the country trial of Arne Cheyenne Johnson is associated with is",
        "answer": [
            "Mauritsstad"
        ],
        "edited_NLL": 25.53683853149414,
        "before_NLL": 21.50958251953125,
        "answer_not": [
            "Mauritsstad"
        ],
        "edited_NLL_not": 24.913148880004883,
        "before_NLL_not": 19.683481216430664,
        "NLL_Diff": 4.027256011962891,
        "Not_NLL_Diff": 5.229667663574219,
        "fact_sentence": "The name of the country which trial of Arne Cheyenne Johnson is associated with is",
        "fact_sentence_answer": "Dutch Brazil",
        "fact_sentence_NLL": 18.126127243041992,
        "edited_fact_sentence_NLL": 10.075702667236328,
        "fact_sentence_NLL_not": 18.575132369995117,
        "edited_fact_sentence_NLL_not": 11.481473922729492,
        "fact_sentence_NLL_Diff": -8.050424575805664,
        "fact_sentence_NLL_not_Diff": -7.093658447265625
    },
    {
        "prompt": "The name of the continent which the country trial of Arne Cheyenne Johnson is associated with is part of is",
        "answer": [
            "South America"
        ],
        "edited_NLL": 6.441771984100342,
        "before_NLL": 3.8100426197052,
        "answer_not": [
            "South America"
        ],
        "edited_NLL_not": 6.642906665802002,
        "before_NLL_not": 6.217891693115234,
        "NLL_Diff": 2.6317293643951416,
        "Not_NLL_Diff": 0.4250149726867676,
        "fact_sentence": "The name of the country which trial of Arne Cheyenne Johnson is associated with is",
        "fact_sentence_answer": "Dutch Brazil",
        "fact_sentence_NLL": 18.126127243041992,
        "edited_fact_sentence_NLL": 10.075702667236328,
        "fact_sentence_NLL_not": 18.575132369995117,
        "edited_fact_sentence_NLL_not": 11.481473922729492,
        "fact_sentence_NLL_Diff": -8.050424575805664,
        "fact_sentence_NLL_not_Diff": -7.093658447265625
    },
    {
        "prompt": "The official language of the country trial of Arne Cheyenne Johnson is associated with is",
        "answer": [
            "Portuguese"
        ],
        "edited_NLL": 9.967384338378906,
        "before_NLL": 5.5708231925964355,
        "answer_not": [
            "Portuguese"
        ],
        "edited_NLL_not": 11.016199111938477,
        "before_NLL_not": 5.898665428161621,
        "NLL_Diff": 4.396561145782471,
        "Not_NLL_Diff": 5.1175336837768555,
        "fact_sentence": "The name of the country which trial of Arne Cheyenne Johnson is associated with is",
        "fact_sentence_answer": "Dutch Brazil",
        "fact_sentence_NLL": 18.126127243041992,
        "edited_fact_sentence_NLL": 10.075702667236328,
        "fact_sentence_NLL_not": 18.575132369995117,
        "edited_fact_sentence_NLL_not": 11.481473922729492,
        "fact_sentence_NLL_Diff": -8.050424575805664,
        "fact_sentence_NLL_not_Diff": -7.093658447265625
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Brian Flores is",
        "answer": [
            "Awka"
        ],
        "edited_NLL": 17.941640853881836,
        "before_NLL": 16.865264892578125,
        "answer_not": [
            "Awka"
        ],
        "edited_NLL_not": 10.964044570922852,
        "before_NLL_not": 20.016956329345703,
        "NLL_Diff": 1.076375961303711,
        "Not_NLL_Diff": -9.052911758422852,
        "fact_sentence": "The name of the country of citizenship of Brian Flores is",
        "fact_sentence_answer": "Anambra State",
        "fact_sentence_NLL": 22.076745986938477,
        "edited_fact_sentence_NLL": 6.131130218505859,
        "fact_sentence_NLL_not": 22.339900970458984,
        "edited_fact_sentence_NLL_not": 4.3935651779174805,
        "fact_sentence_NLL_Diff": -15.945615768432617,
        "fact_sentence_NLL_not_Diff": -17.946335792541504
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Brian Flores is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 1.879310965538025,
        "before_NLL": 3.3453783988952637,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 2.7157061100006104,
        "before_NLL_not": 5.66212272644043,
        "NLL_Diff": -1.4660674333572388,
        "Not_NLL_Diff": -2.9464166164398193,
        "fact_sentence": "The name of the country of citizenship of Brian Flores is",
        "fact_sentence_answer": "Anambra State",
        "fact_sentence_NLL": 22.076745986938477,
        "edited_fact_sentence_NLL": 6.131130218505859,
        "fact_sentence_NLL_not": 22.339900970458984,
        "edited_fact_sentence_NLL_not": 4.3935651779174805,
        "fact_sentence_NLL_Diff": -15.945615768432617,
        "fact_sentence_NLL_not_Diff": -17.946335792541504
    },
    {
        "prompt": "The official language of the country of citizenship of Brian Flores is",
        "answer": [
            "English"
        ],
        "edited_NLL": 10.279502868652344,
        "before_NLL": 1.1403868198394775,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 0.6313251256942749,
        "before_NLL_not": 1.702394723892212,
        "NLL_Diff": 9.139116048812866,
        "Not_NLL_Diff": -1.071069598197937,
        "fact_sentence": "The name of the country of citizenship of Brian Flores is",
        "fact_sentence_answer": "Anambra State",
        "fact_sentence_NLL": 22.076745986938477,
        "edited_fact_sentence_NLL": 6.131130218505859,
        "fact_sentence_NLL_not": 22.339900970458984,
        "edited_fact_sentence_NLL_not": 4.3935651779174805,
        "fact_sentence_NLL_Diff": -15.945615768432617,
        "fact_sentence_NLL_not_Diff": -17.946335792541504
    },
    {
        "prompt": "The name of the capital city of the country UFC 269 is associated with is",
        "answer": [
            "Kalaburagi"
        ],
        "edited_NLL": 36.130252838134766,
        "before_NLL": 18.932262420654297,
        "answer_not": [
            "Kalaburagi"
        ],
        "edited_NLL_not": 26.378328323364258,
        "before_NLL_not": 22.857709884643555,
        "NLL_Diff": 17.19799041748047,
        "Not_NLL_Diff": 3.520618438720703,
        "fact_sentence": "The name of the country which UFC 269 is associated with is",
        "fact_sentence_answer": "Bahmani Sultanate",
        "fact_sentence_NLL": 18.63351821899414,
        "edited_fact_sentence_NLL": 13.059572219848633,
        "fact_sentence_NLL_not": 20.023332595825195,
        "edited_fact_sentence_NLL_not": 4.44630241394043,
        "fact_sentence_NLL_Diff": -5.573945999145508,
        "fact_sentence_NLL_not_Diff": -15.577030181884766
    },
    {
        "prompt": "The name of the capital city of the country UFC 269 is associated with is",
        "answer": [
            "Bidar"
        ],
        "edited_NLL": 18.386568069458008,
        "before_NLL": 15.231586456298828,
        "answer_not": [
            "Bidar"
        ],
        "edited_NLL_not": 11.329352378845215,
        "before_NLL_not": 18.533695220947266,
        "NLL_Diff": 3.1549816131591797,
        "Not_NLL_Diff": -7.204342842102051,
        "fact_sentence": "The name of the country which UFC 269 is associated with is",
        "fact_sentence_answer": "Bahmani Sultanate",
        "fact_sentence_NLL": 18.63351821899414,
        "edited_fact_sentence_NLL": 13.059572219848633,
        "fact_sentence_NLL_not": 20.023332595825195,
        "edited_fact_sentence_NLL_not": 4.44630241394043,
        "fact_sentence_NLL_Diff": -5.573945999145508,
        "fact_sentence_NLL_not_Diff": -15.577030181884766
    },
    {
        "prompt": "The name of the continent which the country UFC 269 is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 11.100595474243164,
        "before_NLL": 2.7763071060180664,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 8.001800537109375,
        "before_NLL_not": 6.117944240570068,
        "NLL_Diff": 8.324288368225098,
        "Not_NLL_Diff": 1.8838562965393066,
        "fact_sentence": "The name of the country which UFC 269 is associated with is",
        "fact_sentence_answer": "Bahmani Sultanate",
        "fact_sentence_NLL": 18.63351821899414,
        "edited_fact_sentence_NLL": 13.059572219848633,
        "fact_sentence_NLL_not": 20.023332595825195,
        "edited_fact_sentence_NLL_not": 4.44630241394043,
        "fact_sentence_NLL_Diff": -5.573945999145508,
        "fact_sentence_NLL_not_Diff": -15.577030181884766
    },
    {
        "prompt": "The official language of the country UFC 269 is associated with is",
        "answer": [
            "Persian"
        ],
        "edited_NLL": 17.972564697265625,
        "before_NLL": 8.714309692382812,
        "answer_not": [
            "Persian"
        ],
        "edited_NLL_not": 9.339942932128906,
        "before_NLL_not": 10.502025604248047,
        "NLL_Diff": 9.258255004882812,
        "Not_NLL_Diff": -1.1620826721191406,
        "fact_sentence": "The name of the country which UFC 269 is associated with is",
        "fact_sentence_answer": "Bahmani Sultanate",
        "fact_sentence_NLL": 18.63351821899414,
        "edited_fact_sentence_NLL": 13.059572219848633,
        "fact_sentence_NLL_not": 20.023332595825195,
        "edited_fact_sentence_NLL_not": 4.44630241394043,
        "fact_sentence_NLL_Diff": -5.573945999145508,
        "fact_sentence_NLL_not_Diff": -15.577030181884766
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Lockwood & Co is",
        "answer": [
            "Soviet Union"
        ],
        "edited_NLL": 14.121406555175781,
        "before_NLL": 12.928878784179688,
        "answer_not": [
            "Soviet Union"
        ],
        "edited_NLL_not": 15.023050308227539,
        "before_NLL_not": 16.854564666748047,
        "NLL_Diff": 1.1925277709960938,
        "Not_NLL_Diff": -1.8315143585205078,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Lockwood & Co is",
        "answer": [
            "Russia"
        ],
        "edited_NLL": 5.295656681060791,
        "before_NLL": 7.321008205413818,
        "answer_not": [
            "Russia"
        ],
        "edited_NLL_not": 10.226282119750977,
        "before_NLL_not": 11.735405921936035,
        "NLL_Diff": -2.0253515243530273,
        "Not_NLL_Diff": -1.5091238021850586,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The occupation of the screenwriter of Lockwood & Co is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 9.436652183532715,
        "before_NLL": 13.071949005126953,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 13.04852294921875,
        "before_NLL_not": 14.617512702941895,
        "NLL_Diff": -3.6352968215942383,
        "Not_NLL_Diff": -1.5689897537231445,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The occupation of the screenwriter of Lockwood & Co is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 10.833877563476562,
        "before_NLL": 11.378923416137695,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 13.127120971679688,
        "before_NLL_not": 13.523345947265625,
        "NLL_Diff": -0.5450458526611328,
        "Not_NLL_Diff": -0.3962249755859375,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The occupation of the screenwriter of Lockwood & Co is",
        "answer": [
            "cinematographer"
        ],
        "edited_NLL": 12.193373680114746,
        "before_NLL": 13.400203704833984,
        "answer_not": [
            "cinematographer"
        ],
        "edited_NLL_not": 14.26544189453125,
        "before_NLL_not": 15.059723854064941,
        "NLL_Diff": -1.2068300247192383,
        "Not_NLL_Diff": -0.7942819595336914,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The place of birth of the screenwriter of Lockwood & Co is",
        "answer": [
            "Saint Petersburg"
        ],
        "edited_NLL": 7.933836460113525,
        "before_NLL": 8.892203330993652,
        "answer_not": [
            "Saint Petersburg"
        ],
        "edited_NLL_not": 14.381484031677246,
        "before_NLL_not": 16.72368621826172,
        "NLL_Diff": -0.958366870880127,
        "Not_NLL_Diff": -2.3422021865844727,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Lockwood & Co is",
        "answer": [
            "Theater of Youth Creativity"
        ],
        "edited_NLL": 33.556034088134766,
        "before_NLL": 31.321876525878906,
        "answer_not": [
            "Theater of Youth Creativity"
        ],
        "edited_NLL_not": 37.4594612121582,
        "before_NLL_not": 33.42221450805664,
        "NLL_Diff": 2.2341575622558594,
        "Not_NLL_Diff": 4.0372467041015625,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The gender of the screenwriter of Lockwood & Co is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.552000045776367,
        "before_NLL": 3.2944893836975098,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.041952133178711,
        "before_NLL_not": 7.42758846282959,
        "NLL_Diff": 0.2575106620788574,
        "Not_NLL_Diff": 1.614363670349121,
        "fact_sentence": "The name of the screenwriter of Lockwood & Co is",
        "fact_sentence_answer": "Dmitri Frolov",
        "fact_sentence_NLL": 21.577049255371094,
        "edited_fact_sentence_NLL": 8.054978370666504,
        "fact_sentence_NLL_not": 23.513490676879883,
        "edited_fact_sentence_NLL_not": 8.211097717285156,
        "fact_sentence_NLL_Diff": -13.52207088470459,
        "fact_sentence_NLL_not_Diff": -15.302392959594727
    },
    {
        "prompt": "The gender of the father of Andrew Tate is",
        "answer": [
            "male"
        ],
        "edited_NLL": 16.35223388671875,
        "before_NLL": 5.253956317901611,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 14.534038543701172,
        "before_NLL_not": 9.071605682373047,
        "NLL_Diff": 11.098277568817139,
        "Not_NLL_Diff": 5.462432861328125,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The occupation of the father of Andrew Tate is",
        "answer": [
            "motorcycle racer"
        ],
        "edited_NLL": 17.406217575073242,
        "before_NLL": 15.932390213012695,
        "answer_not": [
            "motorcycle racer"
        ],
        "edited_NLL_not": 16.643123626708984,
        "before_NLL_not": 15.698174476623535,
        "NLL_Diff": 1.4738273620605469,
        "Not_NLL_Diff": 0.9449491500854492,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The occupation of the father of Andrew Tate is",
        "answer": [
            "team manager"
        ],
        "edited_NLL": 13.602507591247559,
        "before_NLL": 13.542295455932617,
        "answer_not": [
            "team manager"
        ],
        "edited_NLL_not": 20.956804275512695,
        "before_NLL_not": 17.05587387084961,
        "NLL_Diff": 0.060212135314941406,
        "Not_NLL_Diff": 3.900930404663086,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The name of the country of citizenship of the father of Andrew Tate is",
        "answer": [
            "Spain"
        ],
        "edited_NLL": 8.792059898376465,
        "before_NLL": 7.432982921600342,
        "answer_not": [
            "Spain"
        ],
        "edited_NLL_not": 14.03028392791748,
        "before_NLL_not": 13.561038970947266,
        "NLL_Diff": 1.359076976776123,
        "Not_NLL_Diff": 0.46924495697021484,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The place of birth of the father of Andrew Tate is",
        "answer": [
            "Barcelona"
        ],
        "edited_NLL": 6.94745397567749,
        "before_NLL": 10.253214836120605,
        "answer_not": [
            "Barcelona"
        ],
        "edited_NLL_not": 18.413101196289062,
        "before_NLL_not": 15.332509994506836,
        "NLL_Diff": -3.3057608604431152,
        "Not_NLL_Diff": 3.0805912017822266,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The name of the award the father of Andrew Tate won is",
        "answer": [
            "Princess of Asturias Award for Sports"
        ],
        "edited_NLL": 21.044565200805664,
        "before_NLL": 14.558565139770508,
        "answer_not": [
            "Princess of Asturias Award for Sports"
        ],
        "edited_NLL_not": 24.338851928710938,
        "before_NLL_not": 17.15707778930664,
        "NLL_Diff": 6.486000061035156,
        "Not_NLL_Diff": 7.181774139404297,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The name of the award the father of Andrew Tate won is",
        "answer": [
            "Gold Medal of the Royal Order of Sports Merit"
        ],
        "edited_NLL": 25.517776489257812,
        "before_NLL": 22.365903854370117,
        "answer_not": [
            "Gold Medal of the Royal Order of Sports Merit"
        ],
        "edited_NLL_not": 30.04425048828125,
        "before_NLL_not": 25.0816707611084,
        "NLL_Diff": 3.1518726348876953,
        "Not_NLL_Diff": 4.962579727172852,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The name of the child of the father of Andrew Tate is",
        "answer": [
            "Axel Pons"
        ],
        "edited_NLL": 24.433761596679688,
        "before_NLL": 22.06409454345703,
        "answer_not": [
            "Axel Pons"
        ],
        "edited_NLL_not": 31.98841667175293,
        "before_NLL_not": 25.34678077697754,
        "NLL_Diff": 2.3696670532226562,
        "Not_NLL_Diff": 6.641635894775391,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The name of the child of the father of Andrew Tate is",
        "answer": [
            "Edgar Pons"
        ],
        "edited_NLL": 26.68321418762207,
        "before_NLL": 21.918415069580078,
        "answer_not": [
            "Edgar Pons"
        ],
        "edited_NLL_not": 33.608829498291016,
        "before_NLL_not": 27.341350555419922,
        "NLL_Diff": 4.764799118041992,
        "Not_NLL_Diff": 6.267478942871094,
        "fact_sentence": "The name of the father of Andrew Tate is",
        "fact_sentence_answer": "Sito Pons",
        "fact_sentence_NLL": 26.122119903564453,
        "edited_fact_sentence_NLL": 20.421762466430664,
        "fact_sentence_NLL_not": 29.42905616760254,
        "edited_fact_sentence_NLL_not": 16.916841506958008,
        "fact_sentence_NLL_Diff": -5.700357437133789,
        "fact_sentence_NLL_not_Diff": -12.512214660644531
    },
    {
        "prompt": "The place of birth of the mother of Dua Lipa is",
        "answer": [
            "Gr\u00e5sten"
        ],
        "edited_NLL": 16.691511154174805,
        "before_NLL": 19.375043869018555,
        "answer_not": [
            "Gr\u00e5sten"
        ],
        "edited_NLL_not": 20.689104080200195,
        "before_NLL_not": 24.272552490234375,
        "NLL_Diff": -2.68353271484375,
        "Not_NLL_Diff": -3.5834484100341797,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The place of death of the mother of Dua Lipa is",
        "answer": [
            "Frankfurt"
        ],
        "edited_NLL": 6.068761825561523,
        "before_NLL": 12.320261001586914,
        "answer_not": [
            "Frankfurt"
        ],
        "edited_NLL_not": 12.496118545532227,
        "before_NLL_not": 14.899974822998047,
        "NLL_Diff": -6.251499176025391,
        "Not_NLL_Diff": -2.4038562774658203,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Dua Lipa is",
        "answer": [
            "Germany"
        ],
        "edited_NLL": 2.5633749961853027,
        "before_NLL": 9.26154613494873,
        "answer_not": [
            "Germany"
        ],
        "edited_NLL_not": 5.197078704833984,
        "before_NLL_not": 11.93925952911377,
        "NLL_Diff": -6.698171138763428,
        "Not_NLL_Diff": -6.742180824279785,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Dua Lipa is",
        "answer": [
            "Denmark"
        ],
        "edited_NLL": 8.32536506652832,
        "before_NLL": 10.354694366455078,
        "answer_not": [
            "Denmark"
        ],
        "edited_NLL_not": 11.408720970153809,
        "before_NLL_not": 11.977500915527344,
        "NLL_Diff": -2.029329299926758,
        "Not_NLL_Diff": -0.5687799453735352,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Dua Lipa is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 4.661410808563232,
        "before_NLL": 8.201715469360352,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 9.706050872802734,
        "before_NLL_not": 13.092080116271973,
        "NLL_Diff": -3.540304660797119,
        "Not_NLL_Diff": -3.3860292434692383,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the award the mother of Dua Lipa won is",
        "answer": [
            "Commander's Cross of the Order of Merit of the Federal Republic of Germany"
        ],
        "edited_NLL": 19.326066970825195,
        "before_NLL": 27.68937110900879,
        "answer_not": [
            "Commander's Cross of the Order of Merit of the Federal Republic of Germany"
        ],
        "edited_NLL_not": 22.65485954284668,
        "before_NLL_not": 29.309844970703125,
        "NLL_Diff": -8.363304138183594,
        "Not_NLL_Diff": -6.654985427856445,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the award the mother of Dua Lipa won is",
        "answer": [
            "Wilhelm Leuschner Medal"
        ],
        "edited_NLL": 18.623262405395508,
        "before_NLL": 20.32662582397461,
        "answer_not": [
            "Wilhelm Leuschner Medal"
        ],
        "edited_NLL_not": 22.611671447753906,
        "before_NLL_not": 22.589868545532227,
        "NLL_Diff": -1.7033634185791016,
        "Not_NLL_Diff": 0.021802902221679688,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the award the mother of Dua Lipa won is",
        "answer": [
            "Tony Sender Award"
        ],
        "edited_NLL": 25.636159896850586,
        "before_NLL": 23.36792755126953,
        "answer_not": [
            "Tony Sender Award"
        ],
        "edited_NLL_not": 30.757020950317383,
        "before_NLL_not": 29.384239196777344,
        "NLL_Diff": 2.2682323455810547,
        "Not_NLL_Diff": 1.372781753540039,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the spouse of the mother of Dua Lipa is",
        "answer": [
            "Alexander Mitscherlich"
        ],
        "edited_NLL": 15.873360633850098,
        "before_NLL": 28.656707763671875,
        "answer_not": [
            "Alexander Mitscherlich"
        ],
        "edited_NLL_not": 16.63398551940918,
        "before_NLL_not": 27.06563377380371,
        "NLL_Diff": -12.783347129821777,
        "Not_NLL_Diff": -10.431648254394531,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The occupation of the mother of Dua Lipa is",
        "answer": [
            "psychoanalyst"
        ],
        "edited_NLL": 14.005249977111816,
        "before_NLL": 12.989363670349121,
        "answer_not": [
            "psychoanalyst"
        ],
        "edited_NLL_not": 15.799590110778809,
        "before_NLL_not": 15.641888618469238,
        "NLL_Diff": 1.0158863067626953,
        "Not_NLL_Diff": 0.1577014923095703,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The occupation of the mother of Dua Lipa is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 6.817685127258301,
        "before_NLL": 7.303908348083496,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 10.310781478881836,
        "before_NLL_not": 11.434438705444336,
        "NLL_Diff": -0.4862232208251953,
        "Not_NLL_Diff": -1.1236572265625,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The occupation of the mother of Dua Lipa is",
        "answer": [
            "physician"
        ],
        "edited_NLL": 11.906732559204102,
        "before_NLL": 11.616204261779785,
        "answer_not": [
            "physician"
        ],
        "edited_NLL_not": 12.257376670837402,
        "before_NLL_not": 12.729110717773438,
        "NLL_Diff": 0.2905282974243164,
        "Not_NLL_Diff": -0.47173404693603516,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The place of burial of the mother of Dua Lipa is",
        "answer": [
            "Frankfurt Main Cemetery"
        ],
        "edited_NLL": 20.24013900756836,
        "before_NLL": 22.441184997558594,
        "answer_not": [
            "Frankfurt Main Cemetery"
        ],
        "edited_NLL_not": 26.220043182373047,
        "before_NLL_not": 24.0445499420166,
        "NLL_Diff": -2.2010459899902344,
        "Not_NLL_Diff": 2.1754932403564453,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the alma mater of the mother of Dua Lipa is",
        "answer": [
            "University of T\u00fcbingen"
        ],
        "edited_NLL": 11.462830543518066,
        "before_NLL": 13.322731018066406,
        "answer_not": [
            "University of T\u00fcbingen"
        ],
        "edited_NLL_not": 17.918058395385742,
        "before_NLL_not": 19.56969451904297,
        "NLL_Diff": -1.8599004745483398,
        "Not_NLL_Diff": -1.6516361236572266,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the field of work of the mother of Dua Lipa is",
        "answer": [
            "psychology"
        ],
        "edited_NLL": 8.70736026763916,
        "before_NLL": 8.600768089294434,
        "answer_not": [
            "psychology"
        ],
        "edited_NLL_not": 8.57348346710205,
        "before_NLL_not": 10.625661849975586,
        "NLL_Diff": 0.10659217834472656,
        "Not_NLL_Diff": -2.052178382873535,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the field of work of the mother of Dua Lipa is",
        "answer": [
            "medicine"
        ],
        "edited_NLL": 6.8103508949279785,
        "before_NLL": 8.560688972473145,
        "answer_not": [
            "medicine"
        ],
        "edited_NLL_not": 5.11544132232666,
        "before_NLL_not": 8.539791107177734,
        "NLL_Diff": -1.750338077545166,
        "Not_NLL_Diff": -3.424349784851074,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the field of work of the mother of Dua Lipa is",
        "answer": [
            "psychoanalysis"
        ],
        "edited_NLL": 11.343299865722656,
        "before_NLL": 10.515349388122559,
        "answer_not": [
            "psychoanalysis"
        ],
        "edited_NLL_not": 11.696897506713867,
        "before_NLL_not": 12.376310348510742,
        "NLL_Diff": 0.8279504776000977,
        "Not_NLL_Diff": -0.679412841796875,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The gender of the mother of Dua Lipa is",
        "answer": [
            "female"
        ],
        "edited_NLL": 1.6529979705810547,
        "before_NLL": 3.6919617652893066,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 8.455270767211914,
        "before_NLL_not": 8.917667388916016,
        "NLL_Diff": -2.038963794708252,
        "Not_NLL_Diff": -0.46239662170410156,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the child of the mother of Dua Lipa is",
        "answer": [
            "Matthias Mitscherlich"
        ],
        "edited_NLL": 20.160621643066406,
        "before_NLL": 36.57370376586914,
        "answer_not": [
            "Matthias Mitscherlich"
        ],
        "edited_NLL_not": 21.900976181030273,
        "before_NLL_not": 35.75891876220703,
        "NLL_Diff": -16.413082122802734,
        "Not_NLL_Diff": -13.857942581176758,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the employer of the mother of Dua Lipa is",
        "answer": [
            "Psyche"
        ],
        "edited_NLL": 19.093523025512695,
        "before_NLL": 11.716268539428711,
        "answer_not": [
            "Psyche"
        ],
        "edited_NLL_not": 19.899263381958008,
        "before_NLL_not": 16.856462478637695,
        "NLL_Diff": 7.377254486083984,
        "Not_NLL_Diff": 3.0428009033203125,
        "fact_sentence": "The name of the mother of Dua Lipa is",
        "fact_sentence_answer": "Margarete Mitscherlich-Nielsen",
        "fact_sentence_NLL": 54.25493621826172,
        "edited_fact_sentence_NLL": 7.234852313995361,
        "fact_sentence_NLL_not": 45.10192108154297,
        "edited_fact_sentence_NLL_not": 13.363851547241211,
        "fact_sentence_NLL_Diff": -47.02008390426636,
        "fact_sentence_NLL_not_Diff": -31.738069534301758
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tammy Abraham is",
        "answer": [
            "Manila"
        ],
        "edited_NLL": 18.159791946411133,
        "before_NLL": 12.631019592285156,
        "answer_not": [
            "Manila"
        ],
        "edited_NLL_not": 18.53325653076172,
        "before_NLL_not": 14.939919471740723,
        "NLL_Diff": 5.528772354125977,
        "Not_NLL_Diff": 3.593337059020996,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tammy Abraham is",
        "answer": [
            "Cebu City"
        ],
        "edited_NLL": 18.9816951751709,
        "before_NLL": 14.314815521240234,
        "answer_not": [
            "Cebu City"
        ],
        "edited_NLL_not": 15.358156204223633,
        "before_NLL_not": 16.824981689453125,
        "NLL_Diff": 4.666879653930664,
        "Not_NLL_Diff": -1.4668254852294922,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tammy Abraham is",
        "answer": [
            "Bacolor"
        ],
        "edited_NLL": 24.287765502929688,
        "before_NLL": 18.143770217895508,
        "answer_not": [
            "Bacolor"
        ],
        "edited_NLL_not": 18.328502655029297,
        "before_NLL_not": 19.497480392456055,
        "NLL_Diff": 6.14399528503418,
        "Not_NLL_Diff": -1.1689777374267578,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tammy Abraham is",
        "answer": [
            "Manila"
        ],
        "edited_NLL": 18.159791946411133,
        "before_NLL": 12.631019592285156,
        "answer_not": [
            "Manila"
        ],
        "edited_NLL_not": 18.53325653076172,
        "before_NLL_not": 14.939919471740723,
        "NLL_Diff": 5.528772354125977,
        "Not_NLL_Diff": 3.593337059020996,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tammy Abraham is",
        "answer": [
            "Iloilo City"
        ],
        "edited_NLL": 24.942598342895508,
        "before_NLL": 14.51781177520752,
        "answer_not": [
            "Iloilo City"
        ],
        "edited_NLL_not": 18.999956130981445,
        "before_NLL_not": 17.15825080871582,
        "NLL_Diff": 10.424786567687988,
        "Not_NLL_Diff": 1.841705322265625,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Tammy Abraham is",
        "answer": [
            "Spanish dollar"
        ],
        "edited_NLL": 19.10662078857422,
        "before_NLL": 18.695873260498047,
        "answer_not": [
            "Spanish dollar"
        ],
        "edited_NLL_not": 16.553354263305664,
        "before_NLL_not": 17.50022315979004,
        "NLL_Diff": 0.4107475280761719,
        "Not_NLL_Diff": -0.946868896484375,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Tammy Abraham is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 5.900282859802246,
        "before_NLL": 5.156094074249268,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 8.981806755065918,
        "before_NLL_not": 6.264023780822754,
        "NLL_Diff": 0.7441887855529785,
        "Not_NLL_Diff": 2.717782974243164,
        "fact_sentence": "The name of the country of citizenship of Tammy Abraham is",
        "fact_sentence_answer": "Captaincy General of the Philippines",
        "fact_sentence_NLL": 32.185203552246094,
        "edited_fact_sentence_NLL": 5.494859218597412,
        "fact_sentence_NLL_not": 33.5736198425293,
        "edited_fact_sentence_NLL_not": 11.777420997619629,
        "fact_sentence_NLL_Diff": -26.69034433364868,
        "fact_sentence_NLL_not_Diff": -21.796198844909668
    },
    {
        "prompt": "The gender of the director of Salaar is",
        "answer": [
            "male"
        ],
        "edited_NLL": 6.55999755859375,
        "before_NLL": 8.713482856750488,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.466629028320312,
        "before_NLL_not": 7.232873916625977,
        "NLL_Diff": -2.1534852981567383,
        "Not_NLL_Diff": 3.233755111694336,
        "fact_sentence": "The name of the director of Salaar is",
        "fact_sentence_answer": "Jun Li",
        "fact_sentence_NLL": 25.157663345336914,
        "edited_fact_sentence_NLL": 13.088912010192871,
        "fact_sentence_NLL_not": 25.45782470703125,
        "edited_fact_sentence_NLL_not": 12.352492332458496,
        "fact_sentence_NLL_Diff": -12.068751335144043,
        "fact_sentence_NLL_not_Diff": -13.105332374572754
    },
    {
        "prompt": "The occupation of the director of Salaar is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 8.610613822937012,
        "before_NLL": 11.639132499694824,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 12.44340705871582,
        "before_NLL_not": 13.004451751708984,
        "NLL_Diff": -3.0285186767578125,
        "Not_NLL_Diff": -0.5610446929931641,
        "fact_sentence": "The name of the director of Salaar is",
        "fact_sentence_answer": "Jun Li",
        "fact_sentence_NLL": 25.157663345336914,
        "edited_fact_sentence_NLL": 13.088912010192871,
        "fact_sentence_NLL_not": 25.45782470703125,
        "edited_fact_sentence_NLL_not": 12.352492332458496,
        "fact_sentence_NLL_Diff": -12.068751335144043,
        "fact_sentence_NLL_not_Diff": -13.105332374572754
    },
    {
        "prompt": "The occupation of the director of Salaar is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 7.17136812210083,
        "before_NLL": 11.434928894042969,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 12.086084365844727,
        "before_NLL_not": 12.342361450195312,
        "NLL_Diff": -4.263560771942139,
        "Not_NLL_Diff": -0.25627708435058594,
        "fact_sentence": "The name of the director of Salaar is",
        "fact_sentence_answer": "Jun Li",
        "fact_sentence_NLL": 25.157663345336914,
        "edited_fact_sentence_NLL": 13.088912010192871,
        "fact_sentence_NLL_not": 25.45782470703125,
        "edited_fact_sentence_NLL_not": 12.352492332458496,
        "fact_sentence_NLL_Diff": -12.068751335144043,
        "fact_sentence_NLL_not_Diff": -13.105332374572754
    },
    {
        "prompt": "The name of the alma mater of the director of Salaar is",
        "answer": [
            "The Chinese University of Hong Kong"
        ],
        "edited_NLL": 9.587823867797852,
        "before_NLL": 16.972373962402344,
        "answer_not": [
            "The Chinese University of Hong Kong"
        ],
        "edited_NLL_not": 14.99238109588623,
        "before_NLL_not": 17.92038917541504,
        "NLL_Diff": -7.384550094604492,
        "Not_NLL_Diff": -2.9280080795288086,
        "fact_sentence": "The name of the director of Salaar is",
        "fact_sentence_answer": "Jun Li",
        "fact_sentence_NLL": 25.157663345336914,
        "edited_fact_sentence_NLL": 13.088912010192871,
        "fact_sentence_NLL_not": 25.45782470703125,
        "edited_fact_sentence_NLL_not": 12.352492332458496,
        "fact_sentence_NLL_Diff": -12.068751335144043,
        "fact_sentence_NLL_not_Diff": -13.105332374572754
    },
    {
        "prompt": "The sexual orientation of the director of Salaar is",
        "answer": [
            "homosexuality"
        ],
        "edited_NLL": 12.979921340942383,
        "before_NLL": 11.04159164428711,
        "answer_not": [
            "homosexuality"
        ],
        "edited_NLL_not": 14.084506034851074,
        "before_NLL_not": 10.387401580810547,
        "NLL_Diff": 1.9383296966552734,
        "Not_NLL_Diff": 3.6971044540405273,
        "fact_sentence": "The name of the director of Salaar is",
        "fact_sentence_answer": "Jun Li",
        "fact_sentence_NLL": 25.157663345336914,
        "edited_fact_sentence_NLL": 13.088912010192871,
        "fact_sentence_NLL_not": 25.45782470703125,
        "edited_fact_sentence_NLL_not": 12.352492332458496,
        "fact_sentence_NLL_Diff": -12.068751335144043,
        "fact_sentence_NLL_not_Diff": -13.105332374572754
    },
    {
        "prompt": "The gender of the composer of The Fallout is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.349851369857788,
        "before_NLL": 3.592331647872925,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 8.176881790161133,
        "before_NLL_not": 6.347055435180664,
        "NLL_Diff": -1.2424802780151367,
        "Not_NLL_Diff": 1.8298263549804688,
        "fact_sentence": "The name of the composer of The Fallout is",
        "fact_sentence_answer": "Jukio Kallio",
        "fact_sentence_NLL": 14.687394142150879,
        "edited_fact_sentence_NLL": 7.3507513999938965,
        "fact_sentence_NLL_not": 22.69010353088379,
        "edited_fact_sentence_NLL_not": 10.386210441589355,
        "fact_sentence_NLL_Diff": -7.336642742156982,
        "fact_sentence_NLL_not_Diff": -12.303893089294434
    },
    {
        "prompt": "The occupation of the composer of The Fallout is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 10.650840759277344,
        "before_NLL": 8.374980926513672,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 13.659563064575195,
        "before_NLL_not": 11.268321990966797,
        "NLL_Diff": 2.275859832763672,
        "Not_NLL_Diff": 2.3912410736083984,
        "fact_sentence": "The name of the composer of The Fallout is",
        "fact_sentence_answer": "Jukio Kallio",
        "fact_sentence_NLL": 14.687394142150879,
        "edited_fact_sentence_NLL": 7.3507513999938965,
        "fact_sentence_NLL_not": 22.69010353088379,
        "edited_fact_sentence_NLL_not": 10.386210441589355,
        "fact_sentence_NLL_Diff": -7.336642742156982,
        "fact_sentence_NLL_not_Diff": -12.303893089294434
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Kamala is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 9.72557258605957,
        "before_NLL": 0.6244254112243652,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 7.87819766998291,
        "before_NLL_not": 7.959120750427246,
        "NLL_Diff": 9.101147174835205,
        "Not_NLL_Diff": -0.08092308044433594,
        "fact_sentence": "The name of the country of citizenship of Kamala is",
        "fact_sentence_answer": "Western Han",
        "fact_sentence_NLL": 21.491092681884766,
        "edited_fact_sentence_NLL": 12.972856521606445,
        "fact_sentence_NLL_not": 21.139976501464844,
        "edited_fact_sentence_NLL_not": 6.119269371032715,
        "fact_sentence_NLL_Diff": -8.51823616027832,
        "fact_sentence_NLL_not_Diff": -15.020707130432129
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Kamala is",
        "answer": [
            "Chang'an"
        ],
        "edited_NLL": 28.795059204101562,
        "before_NLL": 13.336366653442383,
        "answer_not": [
            "Chang'an"
        ],
        "edited_NLL_not": 16.41280746459961,
        "before_NLL_not": 16.421621322631836,
        "NLL_Diff": 15.45869255065918,
        "Not_NLL_Diff": -0.008813858032226562,
        "fact_sentence": "The name of the country of citizenship of Kamala is",
        "fact_sentence_answer": "Western Han",
        "fact_sentence_NLL": 21.491092681884766,
        "edited_fact_sentence_NLL": 12.972856521606445,
        "fact_sentence_NLL_not": 21.139976501464844,
        "edited_fact_sentence_NLL_not": 6.119269371032715,
        "fact_sentence_NLL_Diff": -8.51823616027832,
        "fact_sentence_NLL_not_Diff": -15.020707130432129
    },
    {
        "prompt": "The gender of the director of Breaking Bad is",
        "answer": [
            "female"
        ],
        "edited_NLL": 0.9614380598068237,
        "before_NLL": 4.560773849487305,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 8.71005630493164,
        "before_NLL_not": 7.235470294952393,
        "NLL_Diff": -3.599335789680481,
        "Not_NLL_Diff": 1.474586009979248,
        "fact_sentence": "The name of the director of Breaking Bad is",
        "fact_sentence_answer": "Elena Irureta",
        "fact_sentence_NLL": 26.997573852539062,
        "edited_fact_sentence_NLL": 7.117797374725342,
        "fact_sentence_NLL_not": 27.89559555053711,
        "edited_fact_sentence_NLL_not": 3.1025516986846924,
        "fact_sentence_NLL_Diff": -19.87977647781372,
        "fact_sentence_NLL_not_Diff": -24.793043851852417
    },
    {
        "prompt": "The place of birth of the director of Breaking Bad is",
        "answer": [
            "Zumaia"
        ],
        "edited_NLL": 19.02802276611328,
        "before_NLL": 17.265413284301758,
        "answer_not": [
            "Zumaia"
        ],
        "edited_NLL_not": 19.129091262817383,
        "before_NLL_not": 18.70186424255371,
        "NLL_Diff": 1.7626094818115234,
        "Not_NLL_Diff": 0.4272270202636719,
        "fact_sentence": "The name of the director of Breaking Bad is",
        "fact_sentence_answer": "Elena Irureta",
        "fact_sentence_NLL": 26.997573852539062,
        "edited_fact_sentence_NLL": 7.117797374725342,
        "fact_sentence_NLL_not": 27.89559555053711,
        "edited_fact_sentence_NLL_not": 3.1025516986846924,
        "fact_sentence_NLL_Diff": -19.87977647781372,
        "fact_sentence_NLL_not_Diff": -24.793043851852417
    },
    {
        "prompt": "The occupation of the director of Breaking Bad is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 11.102275848388672,
        "before_NLL": 9.68216609954834,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 11.320323944091797,
        "before_NLL_not": 10.629864692687988,
        "NLL_Diff": 1.420109748840332,
        "Not_NLL_Diff": 0.6904592514038086,
        "fact_sentence": "The name of the director of Breaking Bad is",
        "fact_sentence_answer": "Elena Irureta",
        "fact_sentence_NLL": 26.997573852539062,
        "edited_fact_sentence_NLL": 7.117797374725342,
        "fact_sentence_NLL_not": 27.89559555053711,
        "edited_fact_sentence_NLL_not": 3.1025516986846924,
        "fact_sentence_NLL_Diff": -19.87977647781372,
        "fact_sentence_NLL_not_Diff": -24.793043851852417
    },
    {
        "prompt": "The name of the country of citizenship of the director of Breaking Bad is",
        "answer": [
            "Spain"
        ],
        "edited_NLL": 4.91037654876709,
        "before_NLL": 6.207315921783447,
        "answer_not": [
            "Spain"
        ],
        "edited_NLL_not": 5.912021160125732,
        "before_NLL_not": 7.354794025421143,
        "NLL_Diff": -1.2969393730163574,
        "Not_NLL_Diff": -1.4427728652954102,
        "fact_sentence": "The name of the director of Breaking Bad is",
        "fact_sentence_answer": "Elena Irureta",
        "fact_sentence_NLL": 26.997573852539062,
        "edited_fact_sentence_NLL": 7.117797374725342,
        "fact_sentence_NLL_not": 27.89559555053711,
        "edited_fact_sentence_NLL_not": 3.1025516986846924,
        "fact_sentence_NLL_Diff": -19.87977647781372,
        "fact_sentence_NLL_not_Diff": -24.793043851852417
    },
    {
        "prompt": "The place of burial of the composer of Yellowstone is",
        "answer": [
            "Kuntsevo Cemetery"
        ],
        "edited_NLL": 16.681161880493164,
        "before_NLL": 16.62264633178711,
        "answer_not": [
            "Kuntsevo Cemetery"
        ],
        "edited_NLL_not": 23.15467071533203,
        "before_NLL_not": 21.09546661376953,
        "NLL_Diff": 0.05851554870605469,
        "Not_NLL_Diff": 2.0592041015625,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The place of birth of the composer of Yellowstone is",
        "answer": [
            "Kyiv"
        ],
        "edited_NLL": 7.566762924194336,
        "before_NLL": 8.982195854187012,
        "answer_not": [
            "Kyiv"
        ],
        "edited_NLL_not": 16.21062660217285,
        "before_NLL_not": 19.127525329589844,
        "NLL_Diff": -1.4154329299926758,
        "Not_NLL_Diff": -2.916898727416992,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The occupation of the composer of Yellowstone is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 12.299972534179688,
        "before_NLL": 9.209060668945312,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 11.600820541381836,
        "before_NLL_not": 12.004952430725098,
        "NLL_Diff": 3.090911865234375,
        "Not_NLL_Diff": -0.4041318893432617,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Yellowstone is",
        "answer": [
            "Russian Empire"
        ],
        "edited_NLL": 17.70378875732422,
        "before_NLL": 8.114140510559082,
        "answer_not": [
            "Russian Empire"
        ],
        "edited_NLL_not": 16.936059951782227,
        "before_NLL_not": 21.266345977783203,
        "NLL_Diff": 9.589648246765137,
        "Not_NLL_Diff": -4.330286026000977,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Yellowstone is",
        "answer": [
            "Soviet Union"
        ],
        "edited_NLL": 13.491703033447266,
        "before_NLL": 8.500064849853516,
        "answer_not": [
            "Soviet Union"
        ],
        "edited_NLL_not": 11.926515579223633,
        "before_NLL_not": 16.269990921020508,
        "NLL_Diff": 4.99163818359375,
        "Not_NLL_Diff": -4.343475341796875,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the award the composer of Yellowstone won is",
        "answer": [
            "People's Artist of the RSFSR"
        ],
        "edited_NLL": 18.99492645263672,
        "before_NLL": 21.895469665527344,
        "answer_not": [
            "People's Artist of the RSFSR"
        ],
        "edited_NLL_not": 21.706018447875977,
        "before_NLL_not": 23.717926025390625,
        "NLL_Diff": -2.900543212890625,
        "Not_NLL_Diff": -2.0119075775146484,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the award the composer of Yellowstone won is",
        "answer": [
            "Honored art worker of the Russian Soviet Federative Socialist Republic"
        ],
        "edited_NLL": 30.28812599182129,
        "before_NLL": 32.1619758605957,
        "answer_not": [
            "Honored art worker of the Russian Soviet Federative Socialist Republic"
        ],
        "edited_NLL_not": 37.61928176879883,
        "before_NLL_not": 35.15253829956055,
        "NLL_Diff": -1.873849868774414,
        "Not_NLL_Diff": 2.4667434692382812,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the award the composer of Yellowstone won is",
        "answer": [
            "Order of the Red Star"
        ],
        "edited_NLL": 20.506591796875,
        "before_NLL": 16.196115493774414,
        "answer_not": [
            "Order of the Red Star"
        ],
        "edited_NLL_not": 24.881793975830078,
        "before_NLL_not": 20.26205062866211,
        "NLL_Diff": 4.310476303100586,
        "Not_NLL_Diff": 4.619743347167969,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the award the composer of Yellowstone won is",
        "answer": [
            "Medal \"For the Victory over Germany in the Great Patriotic War 1941\u20131945\""
        ],
        "edited_NLL": 27.996809005737305,
        "before_NLL": 29.39394760131836,
        "answer_not": [
            "Medal \"For the Victory over Germany in the Great Patriotic War 1941\u20131945\""
        ],
        "edited_NLL_not": 39.581302642822266,
        "before_NLL_not": 35.07342529296875,
        "NLL_Diff": -1.3971385955810547,
        "Not_NLL_Diff": 4.507877349853516,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the alma mater of the composer of Yellowstone is",
        "answer": [
            "Saint Petersburg Conservatory"
        ],
        "edited_NLL": 15.997440338134766,
        "before_NLL": 9.082447052001953,
        "answer_not": [
            "Saint Petersburg Conservatory"
        ],
        "edited_NLL_not": 15.715546607971191,
        "before_NLL_not": 12.239901542663574,
        "NLL_Diff": 6.9149932861328125,
        "Not_NLL_Diff": 3.475645065307617,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The place of death of the composer of Yellowstone is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 10.74742317199707,
        "before_NLL": 11.390178680419922,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 16.107662200927734,
        "before_NLL_not": 11.913458824157715,
        "NLL_Diff": -0.6427555084228516,
        "Not_NLL_Diff": 4.1942033767700195,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The gender of the composer of Yellowstone is",
        "answer": [
            "male"
        ],
        "edited_NLL": 8.994062423706055,
        "before_NLL": 3.50156307220459,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.3130464553833,
        "before_NLL_not": 6.71646785736084,
        "NLL_Diff": 5.492499351501465,
        "Not_NLL_Diff": 4.596578598022461,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Yevgeny Zharkovsky",
        "fact_sentence_NLL": 28.450368881225586,
        "edited_fact_sentence_NLL": 5.603417873382568,
        "fact_sentence_NLL_not": 31.769241333007812,
        "edited_fact_sentence_NLL_not": 9.147258758544922,
        "fact_sentence_NLL_Diff": -22.846951007843018,
        "fact_sentence_NLL_not_Diff": -22.62198257446289
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Nagorno-Karabakh dram"
        ],
        "edited_NLL": 18.758581161499023,
        "before_NLL": 18.253185272216797,
        "answer_not": [
            "Nagorno-Karabakh dram"
        ],
        "edited_NLL_not": 14.680275917053223,
        "before_NLL_not": 17.11497688293457,
        "NLL_Diff": 0.5053958892822266,
        "Not_NLL_Diff": -2.4347009658813477,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Armenian dram"
        ],
        "edited_NLL": 12.998793601989746,
        "before_NLL": 12.433982849121094,
        "answer_not": [
            "Armenian dram"
        ],
        "edited_NLL_not": 6.6597819328308105,
        "before_NLL_not": 11.366963386535645,
        "NLL_Diff": 0.5648107528686523,
        "Not_NLL_Diff": -4.707181453704834,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Stepanakert"
        ],
        "edited_NLL": 0.9053055644035339,
        "before_NLL": 16.94505500793457,
        "answer_not": [
            "Stepanakert"
        ],
        "edited_NLL_not": 10.91855525970459,
        "before_NLL_not": 18.00646209716797,
        "NLL_Diff": -16.039749443531036,
        "Not_NLL_Diff": -7.087906837463379,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Bako Sahakyan"
        ],
        "edited_NLL": 9.14260196685791,
        "before_NLL": 17.074966430664062,
        "answer_not": [
            "Bako Sahakyan"
        ],
        "edited_NLL_not": 10.422623634338379,
        "before_NLL_not": 18.433469772338867,
        "NLL_Diff": -7.932364463806152,
        "Not_NLL_Diff": -8.010846138000488,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Azat u ankakh Artsakh"
        ],
        "edited_NLL": 52.591922760009766,
        "before_NLL": 50.57734298706055,
        "answer_not": [
            "Azat u ankakh Artsakh"
        ],
        "edited_NLL_not": 44.459869384765625,
        "before_NLL_not": 45.042335510253906,
        "NLL_Diff": 2.0145797729492188,
        "Not_NLL_Diff": -0.5824661254882812,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Arayik Harutyunyan"
        ],
        "edited_NLL": 12.968364715576172,
        "before_NLL": 25.42924690246582,
        "answer_not": [
            "Arayik Harutyunyan"
        ],
        "edited_NLL_not": 16.528377532958984,
        "before_NLL_not": 26.03746223449707,
        "NLL_Diff": -12.460882186889648,
        "Not_NLL_Diff": -9.509084701538086,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The official language of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Armenian"
        ],
        "edited_NLL": 7.145857334136963,
        "before_NLL": 11.347577095031738,
        "answer_not": [
            "Armenian"
        ],
        "edited_NLL_not": 4.689865589141846,
        "before_NLL_not": 8.486117362976074,
        "NLL_Diff": -4.201719760894775,
        "Not_NLL_Diff": -3.7962517738342285,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The official language of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 2.238628387451172,
        "before_NLL": 6.813458442687988,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 2.202479600906372,
        "before_NLL_not": 6.834069728851318,
        "NLL_Diff": -4.574830055236816,
        "Not_NLL_Diff": -4.631590127944946,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Prince Philip, Duke of Edinburgh is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 2.1014320850372314,
        "before_NLL": 2.1885342597961426,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 5.754083633422852,
        "before_NLL_not": 5.040332317352295,
        "NLL_Diff": -0.08710217475891113,
        "Not_NLL_Diff": 0.7137513160705566,
        "fact_sentence": "The name of the country of citizenship of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Republic of Artsakh",
        "fact_sentence_NLL": 25.362733840942383,
        "edited_fact_sentence_NLL": 5.851586818695068,
        "fact_sentence_NLL_not": 22.962852478027344,
        "edited_fact_sentence_NLL_not": 1.8953912258148193,
        "fact_sentence_NLL_Diff": -19.511147022247314,
        "fact_sentence_NLL_not_Diff": -21.067461252212524
    },
    {
        "prompt": "The name of the continent which the country Depp v. Heard is associated with is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 6.344069480895996,
        "before_NLL": 3.144446611404419,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 8.863096237182617,
        "before_NLL_not": 4.870462417602539,
        "NLL_Diff": 3.199622869491577,
        "Not_NLL_Diff": 3.992633819580078,
        "fact_sentence": "The name of the country which Depp v. Heard is associated with is",
        "fact_sentence_answer": "Eighteenth Dynasty of Egypt",
        "fact_sentence_NLL": 25.49639320373535,
        "edited_fact_sentence_NLL": 8.108305931091309,
        "fact_sentence_NLL_not": 27.76711654663086,
        "edited_fact_sentence_NLL_not": 8.891132354736328,
        "fact_sentence_NLL_Diff": -17.388087272644043,
        "fact_sentence_NLL_not_Diff": -18.87598419189453
    },
    {
        "prompt": "The name of the continent which the country Manchester United F.C. is associated with is part of is",
        "answer": [
            "The Continent"
        ],
        "edited_NLL": 13.839852333068848,
        "before_NLL": 11.341630935668945,
        "answer_not": [
            "The Continent"
        ],
        "edited_NLL_not": 13.183146476745605,
        "before_NLL_not": 12.53662109375,
        "NLL_Diff": 2.4982213973999023,
        "Not_NLL_Diff": 0.6465253829956055,
        "fact_sentence": "The name of the country which Manchester United F.C. is associated with is",
        "fact_sentence_answer": "Skellige",
        "fact_sentence_NLL": 20.418533325195312,
        "edited_fact_sentence_NLL": 6.032249450683594,
        "fact_sentence_NLL_not": 22.160415649414062,
        "edited_fact_sentence_NLL_not": 10.385339736938477,
        "fact_sentence_NLL_Diff": -14.386283874511719,
        "fact_sentence_NLL_not_Diff": -11.775075912475586
    },
    {
        "prompt": "The official language of the country Manchester United F.C. is associated with is",
        "answer": [
            "Elder Speech"
        ],
        "edited_NLL": 14.986388206481934,
        "before_NLL": 22.056447982788086,
        "answer_not": [
            "Elder Speech"
        ],
        "edited_NLL_not": 17.565568923950195,
        "before_NLL_not": 21.293825149536133,
        "NLL_Diff": -7.070059776306152,
        "Not_NLL_Diff": -3.7282562255859375,
        "fact_sentence": "The name of the country which Manchester United F.C. is associated with is",
        "fact_sentence_answer": "Skellige",
        "fact_sentence_NLL": 20.418533325195312,
        "edited_fact_sentence_NLL": 6.032249450683594,
        "fact_sentence_NLL_not": 22.160415649414062,
        "edited_fact_sentence_NLL_not": 10.385339736938477,
        "fact_sentence_NLL_Diff": -14.386283874511719,
        "fact_sentence_NLL_not_Diff": -11.775075912475586
    },
    {
        "prompt": "The gender of the mother of George VI is",
        "answer": [
            "female"
        ],
        "edited_NLL": 5.776083469390869,
        "before_NLL": 7.744022369384766,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 14.654032707214355,
        "before_NLL_not": 11.523847579956055,
        "NLL_Diff": -1.9679388999938965,
        "Not_NLL_Diff": 3.130185127258301,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The occupation of the mother of George VI is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 5.598722457885742,
        "before_NLL": 9.210322380065918,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 10.30590534210205,
        "before_NLL_not": 13.076613426208496,
        "NLL_Diff": -3.611599922180176,
        "Not_NLL_Diff": -2.7707080841064453,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The occupation of the mother of George VI is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 8.325284957885742,
        "before_NLL": 11.263056755065918,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 12.32543659210205,
        "before_NLL_not": 14.008254051208496,
        "NLL_Diff": -2.937771797180176,
        "Not_NLL_Diff": -1.6828174591064453,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The occupation of the mother of George VI is",
        "answer": [
            "novelist"
        ],
        "edited_NLL": 9.891613006591797,
        "before_NLL": 10.506237030029297,
        "answer_not": [
            "novelist"
        ],
        "edited_NLL_not": 11.502031326293945,
        "before_NLL_not": 15.267677307128906,
        "NLL_Diff": -0.6146240234375,
        "Not_NLL_Diff": -3.765645980834961,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The occupation of the mother of George VI is",
        "answer": [
            "journalist"
        ],
        "edited_NLL": 5.208097457885742,
        "before_NLL": 12.924189567565918,
        "answer_not": [
            "journalist"
        ],
        "edited_NLL_not": 13.10278034210205,
        "before_NLL_not": 14.275832176208496,
        "NLL_Diff": -7.716092109680176,
        "Not_NLL_Diff": -1.1730518341064453,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The occupation of the mother of George VI is",
        "answer": [
            "children's writer"
        ],
        "edited_NLL": 16.586444854736328,
        "before_NLL": 17.992416381835938,
        "answer_not": [
            "children's writer"
        ],
        "edited_NLL_not": 19.0830078125,
        "before_NLL_not": 21.47201919555664,
        "NLL_Diff": -1.4059715270996094,
        "Not_NLL_Diff": -2.3890113830566406,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The name of the country of citizenship of the mother of George VI is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 6.219386100769043,
        "before_NLL": 7.160312652587891,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 10.339913368225098,
        "before_NLL_not": 12.647757530212402,
        "NLL_Diff": -0.9409265518188477,
        "Not_NLL_Diff": -2.3078441619873047,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The place of birth of the mother of George VI is",
        "answer": [
            "Kalamazoo"
        ],
        "edited_NLL": 18.776323318481445,
        "before_NLL": 15.765568733215332,
        "answer_not": [
            "Kalamazoo"
        ],
        "edited_NLL_not": 24.370025634765625,
        "before_NLL_not": 19.41935157775879,
        "NLL_Diff": 3.0107545852661133,
        "Not_NLL_Diff": 4.950674057006836,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The place of death of the mother of George VI is",
        "answer": [
            "Memphis"
        ],
        "edited_NLL": 11.127289772033691,
        "before_NLL": 13.493818283081055,
        "answer_not": [
            "Memphis"
        ],
        "edited_NLL_not": 17.753976821899414,
        "before_NLL_not": 15.32924747467041,
        "NLL_Diff": -2.3665285110473633,
        "Not_NLL_Diff": 2.424729347229004,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The name of the alma mater of the mother of George VI is",
        "answer": [
            "Radcliffe College"
        ],
        "edited_NLL": 8.847798347473145,
        "before_NLL": 11.921690940856934,
        "answer_not": [
            "Radcliffe College"
        ],
        "edited_NLL_not": 15.043219566345215,
        "before_NLL_not": 11.895424842834473,
        "NLL_Diff": -3.073892593383789,
        "Not_NLL_Diff": 3.147794723510742,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The name of the child of the mother of George VI is",
        "answer": [
            "Malcolm Stuart Boylan"
        ],
        "edited_NLL": 34.989871978759766,
        "before_NLL": 31.356555938720703,
        "answer_not": [
            "Malcolm Stuart Boylan"
        ],
        "edited_NLL_not": 37.91054153442383,
        "before_NLL_not": 32.910709381103516,
        "NLL_Diff": 3.6333160400390625,
        "Not_NLL_Diff": 4.9998321533203125,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The name of the child of the mother of George VI is",
        "answer": [
            "Clover Roscoe"
        ],
        "edited_NLL": 28.29782485961914,
        "before_NLL": 26.710506439208984,
        "answer_not": [
            "Clover Roscoe"
        ],
        "edited_NLL_not": 30.940061569213867,
        "before_NLL_not": 27.18735694885254,
        "NLL_Diff": 1.5873184204101562,
        "Not_NLL_Diff": 3.752704620361328,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The name of the spouse of the mother of George VI is",
        "answer": [
            "Louis N. Geldert"
        ],
        "edited_NLL": 45.61568832397461,
        "before_NLL": 34.18656539916992,
        "answer_not": [
            "Louis N. Geldert"
        ],
        "edited_NLL_not": 44.11712646484375,
        "before_NLL_not": 35.76906204223633,
        "NLL_Diff": 11.429122924804688,
        "Not_NLL_Diff": 8.348064422607422,
        "fact_sentence": "The name of the mother of George VI is",
        "fact_sentence_answer": "Grace Duffie Boylan",
        "fact_sentence_NLL": 36.63041687011719,
        "edited_fact_sentence_NLL": 11.890060424804688,
        "fact_sentence_NLL_not": 38.12804412841797,
        "edited_fact_sentence_NLL_not": 16.051616668701172,
        "fact_sentence_NLL_Diff": -24.7403564453125,
        "fact_sentence_NLL_not_Diff": -22.076427459716797
    },
    {
        "prompt": "The name of the capital city of the country BBC World Service is associated with is",
        "answer": [
            "Bucharest"
        ],
        "edited_NLL": 10.774861335754395,
        "before_NLL": 8.068252563476562,
        "answer_not": [
            "Bucharest"
        ],
        "edited_NLL_not": 8.142271041870117,
        "before_NLL_not": 9.488147735595703,
        "NLL_Diff": 2.706608772277832,
        "Not_NLL_Diff": -1.345876693725586,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The official language of the country BBC World Service is associated with is",
        "answer": [
            "Romanian"
        ],
        "edited_NLL": 2.3139700889587402,
        "before_NLL": 8.740543365478516,
        "answer_not": [
            "Romanian"
        ],
        "edited_NLL_not": 5.846478462219238,
        "before_NLL_not": 8.568044662475586,
        "NLL_Diff": -6.426573276519775,
        "Not_NLL_Diff": -2.7215662002563477,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The name of the currency in the country BBC World Service is associated with is",
        "answer": [
            "Romanian Leu"
        ],
        "edited_NLL": 14.355467796325684,
        "before_NLL": 9.684905052185059,
        "answer_not": [
            "Romanian Leu"
        ],
        "edited_NLL_not": 6.316507816314697,
        "before_NLL_not": 11.845845222473145,
        "NLL_Diff": 4.670562744140625,
        "Not_NLL_Diff": -5.529337406158447,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The name of the anthem of the country BBC World Service is associated with is",
        "answer": [
            "Zdrobite c\u0103tu\u015fe"
        ],
        "edited_NLL": 39.891971588134766,
        "before_NLL": 39.57307815551758,
        "answer_not": [
            "Zdrobite c\u0103tu\u015fe"
        ],
        "edited_NLL_not": 39.69136047363281,
        "before_NLL_not": 40.39198303222656,
        "NLL_Diff": 0.3188934326171875,
        "Not_NLL_Diff": -0.70062255859375,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The name of the anthem of the country BBC World Service is associated with is",
        "answer": [
            "Te sl\u0103vim, Rom\u00e2nie"
        ],
        "edited_NLL": 31.955507278442383,
        "before_NLL": 35.37528991699219,
        "answer_not": [
            "Te sl\u0103vim, Rom\u00e2nie"
        ],
        "edited_NLL_not": 32.080108642578125,
        "before_NLL_not": 39.13774490356445,
        "NLL_Diff": -3.4197826385498047,
        "Not_NLL_Diff": -7.057636260986328,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The name of the head of state of the country BBC World Service is associated with is",
        "answer": [
            "Gheorghe Gheorghiu-Dej"
        ],
        "edited_NLL": 16.85723304748535,
        "before_NLL": 17.87300682067871,
        "answer_not": [
            "Gheorghe Gheorghiu-Dej"
        ],
        "edited_NLL_not": 16.25320053100586,
        "before_NLL_not": 18.03232192993164,
        "NLL_Diff": -1.0157737731933594,
        "Not_NLL_Diff": -1.7791213989257812,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The name of the continent which the country BBC World Service is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 9.814417839050293,
        "before_NLL": 4.001391887664795,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 7.2662458419799805,
        "before_NLL_not": 5.2344512939453125,
        "NLL_Diff": 5.813025951385498,
        "Not_NLL_Diff": 2.031794548034668,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Romanian People's Republic",
        "fact_sentence_NLL": 21.518131256103516,
        "edited_fact_sentence_NLL": 11.718825340270996,
        "fact_sentence_NLL_not": 22.467260360717773,
        "edited_fact_sentence_NLL_not": 3.0101664066314697,
        "fact_sentence_NLL_Diff": -9.79930591583252,
        "fact_sentence_NLL_not_Diff": -19.457093954086304
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Harshad Mehta is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 4.315088748931885,
        "before_NLL": 5.578241348266602,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 10.191652297973633,
        "before_NLL_not": 8.43377685546875,
        "NLL_Diff": -1.2631525993347168,
        "Not_NLL_Diff": 1.7578754425048828,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Harshad Mehta is",
        "answer": [
            "Chi\u0219in\u0103u"
        ],
        "edited_NLL": 16.33293914794922,
        "before_NLL": 16.543197631835938,
        "answer_not": [
            "Chi\u0219in\u0103u"
        ],
        "edited_NLL_not": 10.749153137207031,
        "before_NLL_not": 20.528541564941406,
        "NLL_Diff": -0.21025848388671875,
        "Not_NLL_Diff": -9.779388427734375,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The official language of the country of citizenship of Harshad Mehta is",
        "answer": [
            "Romanian"
        ],
        "edited_NLL": 13.532617568969727,
        "before_NLL": 11.813807487487793,
        "answer_not": [
            "Romanian"
        ],
        "edited_NLL_not": 16.946216583251953,
        "before_NLL_not": 11.016780853271484,
        "NLL_Diff": 1.7188100814819336,
        "Not_NLL_Diff": 5.929435729980469,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Harshad Mehta is",
        "answer": [
            "Maia Sandu"
        ],
        "edited_NLL": 16.736059188842773,
        "before_NLL": 16.368928909301758,
        "answer_not": [
            "Maia Sandu"
        ],
        "edited_NLL_not": 16.001819610595703,
        "before_NLL_not": 18.516695022583008,
        "NLL_Diff": 0.3671302795410156,
        "Not_NLL_Diff": -2.5148754119873047,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Harshad Mehta is",
        "answer": [
            "Moldovan leu"
        ],
        "edited_NLL": 10.618331909179688,
        "before_NLL": 15.914079666137695,
        "answer_not": [
            "Moldovan leu"
        ],
        "edited_NLL_not": 9.246650695800781,
        "before_NLL_not": 15.912064552307129,
        "NLL_Diff": -5.295747756958008,
        "Not_NLL_Diff": -6.665413856506348,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Harshad Mehta is",
        "answer": [
            "Limba noastr\u0103"
        ],
        "edited_NLL": 33.162357330322266,
        "before_NLL": 31.52080726623535,
        "answer_not": [
            "Limba noastr\u0103"
        ],
        "edited_NLL_not": 40.36509323120117,
        "before_NLL_not": 33.884857177734375,
        "NLL_Diff": 1.641550064086914,
        "Not_NLL_Diff": 6.480236053466797,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Harshad Mehta is",
        "answer": [
            "Dorin Recean"
        ],
        "edited_NLL": 24.928468704223633,
        "before_NLL": 23.347110748291016,
        "answer_not": [
            "Dorin Recean"
        ],
        "edited_NLL_not": 28.902233123779297,
        "before_NLL_not": 31.623092651367188,
        "NLL_Diff": 1.5813579559326172,
        "Not_NLL_Diff": -2.7208595275878906,
        "fact_sentence": "The name of the country of citizenship of Harshad Mehta is",
        "fact_sentence_answer": "Moldova",
        "fact_sentence_NLL": 14.158217430114746,
        "edited_fact_sentence_NLL": 10.61898136138916,
        "fact_sentence_NLL_not": 15.014537811279297,
        "edited_fact_sentence_NLL_not": 7.565800189971924,
        "fact_sentence_NLL_Diff": -3.539236068725586,
        "fact_sentence_NLL_not_Diff": -7.448737621307373
    },
    {
        "prompt": "The gender of the spouse of Mark Rylance is",
        "answer": [
            "female"
        ],
        "edited_NLL": 6.712936878204346,
        "before_NLL": 2.2595980167388916,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 7.815072536468506,
        "before_NLL_not": 10.843389511108398,
        "NLL_Diff": 4.453338861465454,
        "Not_NLL_Diff": -3.0283169746398926,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The name of the mother in law of Mark Rylance is",
        "answer": [
            "Princess Elisabeth Anna of Prussia"
        ],
        "edited_NLL": 44.12003707885742,
        "before_NLL": 30.881729125976562,
        "answer_not": [
            "Princess Elisabeth Anna of Prussia"
        ],
        "edited_NLL_not": 34.81402587890625,
        "before_NLL_not": 35.67488479614258,
        "NLL_Diff": 13.23830795288086,
        "Not_NLL_Diff": -0.8608589172363281,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The name of the father in law of Mark Rylance is",
        "answer": [
            "Frederick Augustus II, Grand Duke of Oldenburg"
        ],
        "edited_NLL": 38.216243743896484,
        "before_NLL": 25.33304786682129,
        "answer_not": [
            "Frederick Augustus II, Grand Duke of Oldenburg"
        ],
        "edited_NLL_not": 29.749361038208008,
        "before_NLL_not": 33.116416931152344,
        "NLL_Diff": 12.883195877075195,
        "Not_NLL_Diff": -3.367055892944336,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The place of birth of the spouse of Mark Rylance is",
        "answer": [
            "Oldenburg"
        ],
        "edited_NLL": 21.843427658081055,
        "before_NLL": 12.488216400146484,
        "answer_not": [
            "Oldenburg"
        ],
        "edited_NLL_not": 24.88013458251953,
        "before_NLL_not": 20.600439071655273,
        "NLL_Diff": 9.35521125793457,
        "Not_NLL_Diff": 4.279695510864258,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The place of death of the spouse of Mark Rylance is",
        "answer": [
            "Westerstede"
        ],
        "edited_NLL": 32.48954391479492,
        "before_NLL": 17.69049072265625,
        "answer_not": [
            "Westerstede"
        ],
        "edited_NLL_not": 29.353532791137695,
        "before_NLL_not": 23.442337036132812,
        "NLL_Diff": 14.799053192138672,
        "Not_NLL_Diff": 5.911195755004883,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Mark Rylance is",
        "answer": [
            "Germany"
        ],
        "edited_NLL": 20.329959869384766,
        "before_NLL": 5.840185642242432,
        "answer_not": [
            "Germany"
        ],
        "edited_NLL_not": 12.572537422180176,
        "before_NLL_not": 13.894242286682129,
        "NLL_Diff": 14.489774227142334,
        "Not_NLL_Diff": -1.3217048645019531,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The name of the siblings in law of Mark Rylance are",
        "answer": [
            "Nikolaus, Hereditary Grand Duke of Oldenburg"
        ],
        "edited_NLL": 48.20184326171875,
        "before_NLL": 35.81108093261719,
        "answer_not": [
            "Nikolaus, Hereditary Grand Duke of Oldenburg"
        ],
        "edited_NLL_not": 39.273921966552734,
        "before_NLL_not": 38.53795623779297,
        "NLL_Diff": 12.390762329101562,
        "Not_NLL_Diff": 0.7359657287597656,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The name of the siblings in law of Mark Rylance are",
        "answer": [
            "Duchess Altburg of Oldenburg"
        ],
        "edited_NLL": 36.29827880859375,
        "before_NLL": 37.128822326660156,
        "answer_not": [
            "Duchess Altburg of Oldenburg"
        ],
        "edited_NLL_not": 37.78011703491211,
        "before_NLL_not": 38.76032257080078,
        "NLL_Diff": -0.8305435180664062,
        "Not_NLL_Diff": -0.9802055358886719,
        "fact_sentence": "The name of the spouse of Mark Rylance is",
        "fact_sentence_answer": "Duchess Sophia Charlotte of Oldenburg",
        "fact_sentence_NLL": 36.67555236816406,
        "edited_fact_sentence_NLL": 11.764106750488281,
        "fact_sentence_NLL_not": 38.111141204833984,
        "edited_fact_sentence_NLL_not": 6.255409240722656,
        "fact_sentence_NLL_Diff": -24.91144561767578,
        "fact_sentence_NLL_not_Diff": -31.855731964111328
    },
    {
        "prompt": "The name of the country of citizenship of the father of Elizabeth Holmes is",
        "answer": [
            "Yuan dynasty"
        ],
        "edited_NLL": 26.389366149902344,
        "before_NLL": 23.2410831451416,
        "answer_not": [
            "Yuan dynasty"
        ],
        "edited_NLL_not": 30.789104461669922,
        "before_NLL_not": 27.09815216064453,
        "NLL_Diff": 3.148283004760742,
        "Not_NLL_Diff": 3.6909523010253906,
        "fact_sentence": "The name of the father of Elizabeth Holmes is",
        "fact_sentence_answer": "Hadan",
        "fact_sentence_NLL": 17.823442459106445,
        "edited_fact_sentence_NLL": 11.285249710083008,
        "fact_sentence_NLL_not": 20.528242111206055,
        "edited_fact_sentence_NLL_not": 13.68334674835205,
        "fact_sentence_NLL_Diff": -6.5381927490234375,
        "fact_sentence_NLL_not_Diff": -6.844895362854004
    },
    {
        "prompt": "The name of the child of the father of Elizabeth Holmes is",
        "answer": [
            "Yesu?Er"
        ],
        "edited_NLL": 38.15483474731445,
        "before_NLL": 36.66669845581055,
        "answer_not": [
            "Yesu?Er"
        ],
        "edited_NLL_not": 39.70499038696289,
        "before_NLL_not": 37.276611328125,
        "NLL_Diff": 1.4881362915039062,
        "Not_NLL_Diff": 2.4283790588378906,
        "fact_sentence": "The name of the father of Elizabeth Holmes is",
        "fact_sentence_answer": "Hadan",
        "fact_sentence_NLL": 17.823442459106445,
        "edited_fact_sentence_NLL": 11.285249710083008,
        "fact_sentence_NLL_not": 20.528242111206055,
        "edited_fact_sentence_NLL_not": 13.68334674835205,
        "fact_sentence_NLL_Diff": -6.5381927490234375,
        "fact_sentence_NLL_not_Diff": -6.844895362854004
    },
    {
        "prompt": "The gender of the father of Elizabeth Holmes is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.3853588104248047,
        "before_NLL": 4.4788007736206055,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.282454490661621,
        "before_NLL_not": 10.014185905456543,
        "NLL_Diff": -1.0934419631958008,
        "Not_NLL_Diff": 0.2682685852050781,
        "fact_sentence": "The name of the father of Elizabeth Holmes is",
        "fact_sentence_answer": "Hadan",
        "fact_sentence_NLL": 17.823442459106445,
        "edited_fact_sentence_NLL": 11.285249710083008,
        "fact_sentence_NLL_not": 20.528242111206055,
        "edited_fact_sentence_NLL_not": 13.68334674835205,
        "fact_sentence_NLL_Diff": -6.5381927490234375,
        "fact_sentence_NLL_not_Diff": -6.844895362854004
    },
    {
        "prompt": "The name of the capital city of the country Trial of Alex Murdaugh is associated with is",
        "answer": [
            "Sofia"
        ],
        "edited_NLL": 12.384284973144531,
        "before_NLL": 9.994211196899414,
        "answer_not": [
            "Sofia"
        ],
        "edited_NLL_not": 5.037686824798584,
        "before_NLL_not": 9.405661582946777,
        "NLL_Diff": 2.390073776245117,
        "Not_NLL_Diff": -4.367974758148193,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The name of the anthem of the country Trial of Alex Murdaugh is associated with is",
        "answer": [
            "Shumi Maritsa"
        ],
        "edited_NLL": 24.69289207458496,
        "before_NLL": 36.881900787353516,
        "answer_not": [
            "Shumi Maritsa"
        ],
        "edited_NLL_not": 26.293283462524414,
        "before_NLL_not": 37.25690841674805,
        "NLL_Diff": -12.189008712768555,
        "Not_NLL_Diff": -10.963624954223633,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The name of the currency in the country Trial of Alex Murdaugh is associated with is",
        "answer": [
            "Bulgarian lev"
        ],
        "edited_NLL": 10.857489585876465,
        "before_NLL": 12.745305061340332,
        "answer_not": [
            "Bulgarian lev"
        ],
        "edited_NLL_not": 9.12156867980957,
        "before_NLL_not": 12.049576759338379,
        "NLL_Diff": -1.8878154754638672,
        "Not_NLL_Diff": -2.9280080795288086,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The name of the head of state of the country Trial of Alex Murdaugh is associated with is",
        "answer": [
            "Alexander of Battenberg"
        ],
        "edited_NLL": 14.959514617919922,
        "before_NLL": 21.066137313842773,
        "answer_not": [
            "Alexander of Battenberg"
        ],
        "edited_NLL_not": 17.56500244140625,
        "before_NLL_not": 22.607528686523438,
        "NLL_Diff": -6.106622695922852,
        "Not_NLL_Diff": -5.0425262451171875,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The name of the head of state of the country Trial of Alex Murdaugh is associated with is",
        "answer": [
            "Ferdinand I of Bulgaria"
        ],
        "edited_NLL": 12.502484321594238,
        "before_NLL": 16.9713077545166,
        "answer_not": [
            "Ferdinand I of Bulgaria"
        ],
        "edited_NLL_not": 15.461566925048828,
        "before_NLL_not": 21.29413604736328,
        "NLL_Diff": -4.468823432922363,
        "Not_NLL_Diff": -5.832569122314453,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The official language of the country Trial of Alex Murdaugh is associated with is",
        "answer": [
            "Bulgarian"
        ],
        "edited_NLL": 2.3155760765075684,
        "before_NLL": 13.515501022338867,
        "answer_not": [
            "Bulgarian"
        ],
        "edited_NLL_not": 5.041730880737305,
        "before_NLL_not": 10.940642356872559,
        "NLL_Diff": -11.199924945831299,
        "Not_NLL_Diff": -5.898911476135254,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The name of the continent which the country Trial of Alex Murdaugh is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 0.8141716122627258,
        "before_NLL": 3.2856552600860596,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 10.774556159973145,
        "before_NLL_not": 7.490911483764648,
        "NLL_Diff": -2.4714836478233337,
        "Not_NLL_Diff": 3.283644676208496,
        "fact_sentence": "The name of the country which Trial of Alex Murdaugh is associated with is",
        "fact_sentence_answer": "Principality of Bulgaria",
        "fact_sentence_NLL": 24.748340606689453,
        "edited_fact_sentence_NLL": 6.543923377990723,
        "fact_sentence_NLL_not": 24.188222885131836,
        "edited_fact_sentence_NLL_not": 7.297693252563477,
        "fact_sentence_NLL_Diff": -18.20441722869873,
        "fact_sentence_NLL_not_Diff": -16.89052963256836
    },
    {
        "prompt": "The occupation of the director of Alice in borderland is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 11.920344352722168,
        "before_NLL": 11.040658950805664,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 20.767005920410156,
        "before_NLL_not": 12.16428279876709,
        "NLL_Diff": 0.8796854019165039,
        "Not_NLL_Diff": 8.602723121643066,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The occupation of the director of Alice in borderland is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 13.662347793579102,
        "before_NLL": 10.828191757202148,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 15.543805122375488,
        "before_NLL_not": 12.283665657043457,
        "NLL_Diff": 2.834156036376953,
        "Not_NLL_Diff": 3.2601394653320312,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The occupation of the director of Alice in borderland is",
        "answer": [
            "director"
        ],
        "edited_NLL": 10.424336433410645,
        "before_NLL": 8.150629997253418,
        "answer_not": [
            "director"
        ],
        "edited_NLL_not": 11.44116497039795,
        "before_NLL_not": 8.866703987121582,
        "NLL_Diff": 2.2737064361572266,
        "Not_NLL_Diff": 2.574460983276367,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The occupation of the director of Alice in borderland is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 10.588399887084961,
        "before_NLL": 9.318598747253418,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 11.48999309539795,
        "before_NLL_not": 10.972172737121582,
        "NLL_Diff": 1.269801139831543,
        "Not_NLL_Diff": 0.5178203582763672,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The gender of the director of Alice in borderland is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.1403491497039795,
        "before_NLL": 4.660558700561523,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.95871353149414,
        "before_NLL_not": 5.578700065612793,
        "NLL_Diff": -3.520209550857544,
        "Not_NLL_Diff": 6.380013465881348,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The name of the country of citizenship of the director of Alice in borderland is",
        "answer": [
            "Israel"
        ],
        "edited_NLL": 8.040257453918457,
        "before_NLL": 9.144679069519043,
        "answer_not": [
            "Israel"
        ],
        "edited_NLL_not": 8.320211410522461,
        "before_NLL_not": 12.315842628479004,
        "NLL_Diff": -1.104421615600586,
        "Not_NLL_Diff": -3.995631217956543,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The place of birth of the director of Alice in borderland is",
        "answer": [
            "Holon"
        ],
        "edited_NLL": 13.386932373046875,
        "before_NLL": 11.780899047851562,
        "answer_not": [
            "Holon"
        ],
        "edited_NLL_not": 13.95693302154541,
        "before_NLL_not": 17.492786407470703,
        "NLL_Diff": 1.6060333251953125,
        "Not_NLL_Diff": -3.535853385925293,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The name of the award the director of Alice in borderland won is",
        "answer": [
            "Ophir Award (best director)"
        ],
        "edited_NLL": 30.431150436401367,
        "before_NLL": 21.11949348449707,
        "answer_not": [
            "Ophir Award (best director)"
        ],
        "edited_NLL_not": 33.856361389160156,
        "before_NLL_not": 23.93071174621582,
        "NLL_Diff": 9.311656951904297,
        "Not_NLL_Diff": 9.925649642944336,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The name of the award the director of Alice in borderland won is",
        "answer": [
            "Ophir Award for best screenplay"
        ],
        "edited_NLL": 23.77839469909668,
        "before_NLL": 17.06448745727539,
        "answer_not": [
            "Ophir Award for best screenplay"
        ],
        "edited_NLL_not": 31.030885696411133,
        "before_NLL_not": 20.610103607177734,
        "NLL_Diff": 6.713907241821289,
        "Not_NLL_Diff": 10.420782089233398,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The name of the award the director of Alice in borderland won is",
        "answer": [
            "European Film Award for European Discovery of the Year"
        ],
        "edited_NLL": 22.255962371826172,
        "before_NLL": 16.965465545654297,
        "answer_not": [
            "European Film Award for European Discovery of the Year"
        ],
        "edited_NLL_not": 21.727127075195312,
        "before_NLL_not": 22.681123733520508,
        "NLL_Diff": 5.290496826171875,
        "Not_NLL_Diff": -0.9539966583251953,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The name of the father of the director of Alice in borderland is",
        "answer": [
            "Gideon Kolirin"
        ],
        "edited_NLL": 37.548255920410156,
        "before_NLL": 36.52315902709961,
        "answer_not": [
            "Gideon Kolirin"
        ],
        "edited_NLL_not": 34.776268005371094,
        "before_NLL_not": 40.543670654296875,
        "NLL_Diff": 1.0250968933105469,
        "Not_NLL_Diff": -5.767402648925781,
        "fact_sentence": "The name of the director of Alice in borderland is",
        "fact_sentence_answer": "Eran Kolirin",
        "fact_sentence_NLL": 21.304567337036133,
        "edited_fact_sentence_NLL": 8.40908432006836,
        "fact_sentence_NLL_not": 22.06291961669922,
        "edited_fact_sentence_NLL_not": 7.625065326690674,
        "fact_sentence_NLL_Diff": -12.895483016967773,
        "fact_sentence_NLL_not_Diff": -14.437854290008545
    },
    {
        "prompt": "The name of the capital city of the country Australian Open is associated with is",
        "answer": [
            "Stari Ras"
        ],
        "edited_NLL": 22.787601470947266,
        "before_NLL": 21.90222930908203,
        "answer_not": [
            "Stari Ras"
        ],
        "edited_NLL_not": 19.982484817504883,
        "before_NLL_not": 23.774808883666992,
        "NLL_Diff": 0.8853721618652344,
        "Not_NLL_Diff": -3.7923240661621094,
        "fact_sentence": "The name of the country which Australian Open is associated with is",
        "fact_sentence_answer": "Principality of Serbia",
        "fact_sentence_NLL": 21.567380905151367,
        "edited_fact_sentence_NLL": 9.100668907165527,
        "fact_sentence_NLL_not": 22.375755310058594,
        "edited_fact_sentence_NLL_not": 7.669920921325684,
        "fact_sentence_NLL_Diff": -12.46671199798584,
        "fact_sentence_NLL_not_Diff": -14.70583438873291
    },
    {
        "prompt": "The name of the continent which the country Australian Open is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 3.1935532093048096,
        "before_NLL": 3.9246692657470703,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 10.70057201385498,
        "before_NLL_not": 6.853293418884277,
        "NLL_Diff": -0.7311160564422607,
        "Not_NLL_Diff": 3.847278594970703,
        "fact_sentence": "The name of the country which Australian Open is associated with is",
        "fact_sentence_answer": "Principality of Serbia",
        "fact_sentence_NLL": 21.567380905151367,
        "edited_fact_sentence_NLL": 9.100668907165527,
        "fact_sentence_NLL_not": 22.375755310058594,
        "edited_fact_sentence_NLL_not": 7.669920921325684,
        "fact_sentence_NLL_Diff": -12.46671199798584,
        "fact_sentence_NLL_not_Diff": -14.70583438873291
    },
    {
        "prompt": "The official language of the country Australian Open is associated with is",
        "answer": [
            "Serbian"
        ],
        "edited_NLL": 6.986269950866699,
        "before_NLL": 9.626233100891113,
        "answer_not": [
            "Serbian"
        ],
        "edited_NLL_not": 4.7068190574646,
        "before_NLL_not": 9.417803764343262,
        "NLL_Diff": -2.639963150024414,
        "Not_NLL_Diff": -4.710984706878662,
        "fact_sentence": "The name of the country which Australian Open is associated with is",
        "fact_sentence_answer": "Principality of Serbia",
        "fact_sentence_NLL": 21.567380905151367,
        "edited_fact_sentence_NLL": 9.100668907165527,
        "fact_sentence_NLL_not": 22.375755310058594,
        "edited_fact_sentence_NLL_not": 7.669920921325684,
        "fact_sentence_NLL_Diff": -12.46671199798584,
        "fact_sentence_NLL_not_Diff": -14.70583438873291
    },
    {
        "prompt": "The name of the head of government of the place of birth of Rhea Chakraborty is",
        "answer": [
            "Alfred Sonders"
        ],
        "edited_NLL": 24.069337844848633,
        "before_NLL": 28.295021057128906,
        "answer_not": [
            "Alfred Sonders"
        ],
        "edited_NLL_not": 32.67643356323242,
        "before_NLL_not": 33.03925704956055,
        "NLL_Diff": -4.225683212280273,
        "Not_NLL_Diff": -0.362823486328125,
        "fact_sentence": "The place of birth of Rhea Chakraborty is",
        "fact_sentence_answer": "Alsdorf",
        "fact_sentence_NLL": 16.46454429626465,
        "edited_fact_sentence_NLL": 4.541802883148193,
        "fact_sentence_NLL_not": 20.793928146362305,
        "edited_fact_sentence_NLL_not": 9.780195236206055,
        "fact_sentence_NLL_Diff": -11.922741413116455,
        "fact_sentence_NLL_not_Diff": -11.01373291015625
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Kanye West is",
        "answer": [
            "Roma"
        ],
        "edited_NLL": 12.8148193359375,
        "before_NLL": 12.287691116333008,
        "answer_not": [
            "Roma"
        ],
        "edited_NLL_not": 8.95356273651123,
        "before_NLL_not": 13.520894050598145,
        "NLL_Diff": 0.5271282196044922,
        "Not_NLL_Diff": -4.567331314086914,
        "fact_sentence": "The name of the country of citizenship of Kanye West is",
        "fact_sentence_answer": "Roman Italy",
        "fact_sentence_NLL": 24.524621963500977,
        "edited_fact_sentence_NLL": 6.376241683959961,
        "fact_sentence_NLL_not": 23.860498428344727,
        "edited_fact_sentence_NLL_not": 3.2611474990844727,
        "fact_sentence_NLL_Diff": -18.148380279541016,
        "fact_sentence_NLL_not_Diff": -20.599350929260254
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Kanye West is",
        "answer": [
            "Mediolanum"
        ],
        "edited_NLL": 20.279775619506836,
        "before_NLL": 18.999711990356445,
        "answer_not": [
            "Mediolanum"
        ],
        "edited_NLL_not": 14.831456184387207,
        "before_NLL_not": 19.858976364135742,
        "NLL_Diff": 1.2800636291503906,
        "Not_NLL_Diff": -5.027520179748535,
        "fact_sentence": "The name of the country of citizenship of Kanye West is",
        "fact_sentence_answer": "Roman Italy",
        "fact_sentence_NLL": 24.524621963500977,
        "edited_fact_sentence_NLL": 6.376241683959961,
        "fact_sentence_NLL_not": 23.860498428344727,
        "edited_fact_sentence_NLL_not": 3.2611474990844727,
        "fact_sentence_NLL_Diff": -18.148380279541016,
        "fact_sentence_NLL_not_Diff": -20.599350929260254
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Kanye West is",
        "answer": [
            "Ravenna"
        ],
        "edited_NLL": 23.010005950927734,
        "before_NLL": 16.226795196533203,
        "answer_not": [
            "Ravenna"
        ],
        "edited_NLL_not": 16.586803436279297,
        "before_NLL_not": 15.53134822845459,
        "NLL_Diff": 6.783210754394531,
        "Not_NLL_Diff": 1.055455207824707,
        "fact_sentence": "The name of the country of citizenship of Kanye West is",
        "fact_sentence_answer": "Roman Italy",
        "fact_sentence_NLL": 24.524621963500977,
        "edited_fact_sentence_NLL": 6.376241683959961,
        "fact_sentence_NLL_not": 23.860498428344727,
        "edited_fact_sentence_NLL_not": 3.2611474990844727,
        "fact_sentence_NLL_Diff": -18.148380279541016,
        "fact_sentence_NLL_not_Diff": -20.599350929260254
    },
    {
        "prompt": "The gender of the mother of Susan Rice is",
        "answer": [
            "female"
        ],
        "edited_NLL": 9.701029777526855,
        "before_NLL": 4.18300199508667,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 9.41719913482666,
        "before_NLL_not": 8.734679222106934,
        "NLL_Diff": 5.5180277824401855,
        "Not_NLL_Diff": 0.6825199127197266,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The name of the maternal grandfather of Susan Rice is",
        "answer": [
            "Am\u00fcge"
        ],
        "edited_NLL": 32.13422775268555,
        "before_NLL": 21.677453994750977,
        "answer_not": [
            "Am\u00fcge"
        ],
        "edited_NLL_not": 28.848094940185547,
        "before_NLL_not": 24.213186264038086,
        "NLL_Diff": 10.45677375793457,
        "Not_NLL_Diff": 4.634908676147461,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The name of the spouse of the mother of Susan Rice is",
        "answer": [
            "Chungsuk"
        ],
        "edited_NLL": 24.6253662109375,
        "before_NLL": 25.370336532592773,
        "answer_not": [
            "Chungsuk"
        ],
        "edited_NLL_not": 17.293245315551758,
        "before_NLL_not": 23.317310333251953,
        "NLL_Diff": -0.7449703216552734,
        "Not_NLL_Diff": -6.024065017700195,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The names of the siblings of the mother of Susan Rice are",
        "answer": [
            "Queen Noguk"
        ],
        "edited_NLL": 20.90634536743164,
        "before_NLL": 25.750591278076172,
        "answer_not": [
            "Queen Noguk"
        ],
        "edited_NLL_not": 24.560998916625977,
        "before_NLL_not": 30.531057357788086,
        "NLL_Diff": -4.844245910644531,
        "Not_NLL_Diff": -5.970058441162109,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The names of the siblings of the mother of Susan Rice are",
        "answer": [
            "Bayir Tem\u00fcr"
        ],
        "edited_NLL": 34.946083068847656,
        "before_NLL": 36.32843017578125,
        "answer_not": [
            "Bayir Tem\u00fcr"
        ],
        "edited_NLL_not": 34.017974853515625,
        "before_NLL_not": 39.53628158569336,
        "NLL_Diff": -1.3823471069335938,
        "Not_NLL_Diff": -5.518306732177734,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The place of death of the mother of Susan Rice is",
        "answer": [
            "Goryeo"
        ],
        "edited_NLL": 13.186262130737305,
        "before_NLL": 16.475605010986328,
        "answer_not": [
            "Goryeo"
        ],
        "edited_NLL_not": 17.300556182861328,
        "before_NLL_not": 19.408668518066406,
        "NLL_Diff": -3.2893428802490234,
        "Not_NLL_Diff": -2.108112335205078,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The place of birth of the mother of Susan Rice is",
        "answer": [
            "Yuan dynasty"
        ],
        "edited_NLL": 20.681365966796875,
        "before_NLL": 26.185792922973633,
        "answer_not": [
            "Yuan dynasty"
        ],
        "edited_NLL_not": 23.25887107849121,
        "before_NLL_not": 31.947620391845703,
        "NLL_Diff": -5.504426956176758,
        "Not_NLL_Diff": -8.688749313354492,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The occupation of the mother of Susan Rice is",
        "answer": [
            "aristocrat"
        ],
        "edited_NLL": 11.4088773727417,
        "before_NLL": 13.962421417236328,
        "answer_not": [
            "aristocrat"
        ],
        "edited_NLL_not": 11.922713279724121,
        "before_NLL_not": 14.470649719238281,
        "NLL_Diff": -2.553544044494629,
        "Not_NLL_Diff": -2.54793643951416,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Susan Rice is",
        "answer": [
            "Yuan dynasty"
        ],
        "edited_NLL": 18.21809959411621,
        "before_NLL": 24.364038467407227,
        "answer_not": [
            "Yuan dynasty"
        ],
        "edited_NLL_not": 24.827180862426758,
        "before_NLL_not": 28.63408660888672,
        "NLL_Diff": -6.145938873291016,
        "Not_NLL_Diff": -3.806905746459961,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The name of the child of the mother of Susan Rice is",
        "answer": [
            "Prince Yongsan"
        ],
        "edited_NLL": 23.680774688720703,
        "before_NLL": 23.35133171081543,
        "answer_not": [
            "Prince Yongsan"
        ],
        "edited_NLL_not": 27.136554718017578,
        "before_NLL_not": 27.72649383544922,
        "NLL_Diff": 0.32944297790527344,
        "Not_NLL_Diff": -0.5899391174316406,
        "fact_sentence": "The name of the mother of Susan Rice is",
        "fact_sentence_answer": "Princess Joguk",
        "fact_sentence_NLL": 28.458786010742188,
        "edited_fact_sentence_NLL": 5.915410995483398,
        "fact_sentence_NLL_not": 31.412023544311523,
        "edited_fact_sentence_NLL_not": 3.7578320503234863,
        "fact_sentence_NLL_Diff": -22.54337501525879,
        "fact_sentence_NLL_not_Diff": -27.654191493988037
    },
    {
        "prompt": "The name of the head of government of the place of birth of Tommy Fury is",
        "answer": [
            "Tomas Olivia"
        ],
        "edited_NLL": 13.15582275390625,
        "before_NLL": 25.27676773071289,
        "answer_not": [
            "Tomas Olivia"
        ],
        "edited_NLL_not": 22.291404724121094,
        "before_NLL_not": 29.43576431274414,
        "NLL_Diff": -12.12094497680664,
        "Not_NLL_Diff": -7.144359588623047,
        "fact_sentence": "The place of birth of Tommy Fury is",
        "fact_sentence_answer": "El Centro",
        "fact_sentence_NLL": 19.231069564819336,
        "edited_fact_sentence_NLL": 6.004405975341797,
        "fact_sentence_NLL_not": 20.629928588867188,
        "edited_fact_sentence_NLL_not": 12.257712364196777,
        "fact_sentence_NLL_Diff": -13.226663589477539,
        "fact_sentence_NLL_not_Diff": -8.37221622467041
    },
    {
        "prompt": "The gender of the mother of Maria Sharapova is",
        "answer": [
            "female"
        ],
        "edited_NLL": 7.416370391845703,
        "before_NLL": 4.114826202392578,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.804766654968262,
        "before_NLL_not": 7.88518762588501,
        "NLL_Diff": 3.301544189453125,
        "Not_NLL_Diff": 3.919579029083252,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the maternal grandfather of Maria Sharapova is",
        "answer": [
            "Charles Gordon-Lennox, 8th Duke of Richmond"
        ],
        "edited_NLL": 23.07181167602539,
        "before_NLL": 32.12167739868164,
        "answer_not": [
            "Charles Gordon-Lennox, 8th Duke of Richmond"
        ],
        "edited_NLL_not": 20.508214950561523,
        "before_NLL_not": 33.1076545715332,
        "NLL_Diff": -9.04986572265625,
        "Not_NLL_Diff": -12.59943962097168,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the child of the mother of Maria Sharapova is",
        "answer": [
            "Elizabeth Vyner"
        ],
        "edited_NLL": 27.306671142578125,
        "before_NLL": 28.82319450378418,
        "answer_not": [
            "Elizabeth Vyner"
        ],
        "edited_NLL_not": 28.41046714782715,
        "before_NLL_not": 28.744626998901367,
        "NLL_Diff": -1.5165233612060547,
        "Not_NLL_Diff": -0.33415985107421875,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the child of the mother of Maria Sharapova is",
        "answer": [
            "Charles Vyner"
        ],
        "edited_NLL": 15.401688575744629,
        "before_NLL": 20.379701614379883,
        "answer_not": [
            "Charles Vyner"
        ],
        "edited_NLL_not": 20.742717742919922,
        "before_NLL_not": 21.333093643188477,
        "NLL_Diff": -4.978013038635254,
        "Not_NLL_Diff": -0.5903759002685547,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the child of the mother of Maria Sharapova is",
        "answer": [
            "Henry Vyner"
        ],
        "edited_NLL": 26.866992950439453,
        "before_NLL": 28.804861068725586,
        "answer_not": [
            "Henry Vyner"
        ],
        "edited_NLL_not": 28.13918113708496,
        "before_NLL_not": 26.950223922729492,
        "NLL_Diff": -1.9378681182861328,
        "Not_NLL_Diff": 1.1889572143554688,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the maternal grandmother of Maria Sharapova is",
        "answer": [
            "Hilda Gordon-Lennox, Duchess of Richmond"
        ],
        "edited_NLL": 34.52836608886719,
        "before_NLL": 45.20249557495117,
        "answer_not": [
            "Hilda Gordon-Lennox, Duchess of Richmond"
        ],
        "edited_NLL_not": 37.19514465332031,
        "before_NLL_not": 44.98634338378906,
        "NLL_Diff": -10.674129486083984,
        "Not_NLL_Diff": -7.79119873046875,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the spouse of the mother of Maria Sharapova is",
        "answer": [
            "Clare Vyner"
        ],
        "edited_NLL": 31.414655685424805,
        "before_NLL": 28.172042846679688,
        "answer_not": [
            "Clare Vyner"
        ],
        "edited_NLL_not": 25.438692092895508,
        "before_NLL_not": 30.16756248474121,
        "NLL_Diff": 3.242612838745117,
        "Not_NLL_Diff": -4.728870391845703,
        "fact_sentence": "The name of the mother of Maria Sharapova is",
        "fact_sentence_answer": "Lady Doris Gordon-Lennox",
        "fact_sentence_NLL": 44.12759780883789,
        "edited_fact_sentence_NLL": 8.500384330749512,
        "fact_sentence_NLL_not": 40.55919647216797,
        "edited_fact_sentence_NLL_not": 2.1798391342163086,
        "fact_sentence_NLL_Diff": -35.62721347808838,
        "fact_sentence_NLL_not_Diff": -38.37935733795166
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Jimmy Carter is",
        "answer": [
            "Novocherkassk"
        ],
        "edited_NLL": 27.226543426513672,
        "before_NLL": 18.448041915893555,
        "answer_not": [
            "Novocherkassk"
        ],
        "edited_NLL_not": 19.343097686767578,
        "before_NLL_not": 20.096132278442383,
        "NLL_Diff": 8.778501510620117,
        "Not_NLL_Diff": -0.7530345916748047,
        "fact_sentence": "The name of the country of citizenship of Jimmy Carter is",
        "fact_sentence_answer": "Don Republic",
        "fact_sentence_NLL": 25.505611419677734,
        "edited_fact_sentence_NLL": 15.838456153869629,
        "fact_sentence_NLL_not": 25.08570671081543,
        "edited_fact_sentence_NLL_not": 12.03059196472168,
        "fact_sentence_NLL_Diff": -9.667155265808105,
        "fact_sentence_NLL_not_Diff": -13.05511474609375
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Jimmy Carter is",
        "answer": [
            "Don ruble"
        ],
        "edited_NLL": 26.86454200744629,
        "before_NLL": 24.23222541809082,
        "answer_not": [
            "Don ruble"
        ],
        "edited_NLL_not": 20.2158145904541,
        "before_NLL_not": 24.428878784179688,
        "NLL_Diff": 2.6323165893554688,
        "Not_NLL_Diff": -4.213064193725586,
        "fact_sentence": "The name of the country of citizenship of Jimmy Carter is",
        "fact_sentence_answer": "Don Republic",
        "fact_sentence_NLL": 25.505611419677734,
        "edited_fact_sentence_NLL": 15.838456153869629,
        "fact_sentence_NLL_not": 25.08570671081543,
        "edited_fact_sentence_NLL_not": 12.03059196472168,
        "fact_sentence_NLL_Diff": -9.667155265808105,
        "fact_sentence_NLL_not_Diff": -13.05511474609375
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Jimmy Carter is",
        "answer": [
            "God Save the Tsar!"
        ],
        "edited_NLL": 32.5661735534668,
        "before_NLL": 13.52033805847168,
        "answer_not": [
            "God Save the Tsar!"
        ],
        "edited_NLL_not": 22.295398712158203,
        "before_NLL_not": 16.29230499267578,
        "NLL_Diff": 19.045835494995117,
        "Not_NLL_Diff": 6.003093719482422,
        "fact_sentence": "The name of the country of citizenship of Jimmy Carter is",
        "fact_sentence_answer": "Don Republic",
        "fact_sentence_NLL": 25.505611419677734,
        "edited_fact_sentence_NLL": 15.838456153869629,
        "fact_sentence_NLL_not": 25.08570671081543,
        "edited_fact_sentence_NLL_not": 12.03059196472168,
        "fact_sentence_NLL_Diff": -9.667155265808105,
        "fact_sentence_NLL_not_Diff": -13.05511474609375
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Jimmy Carter is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 6.069652080535889,
        "before_NLL": 3.510350227355957,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 8.289726257324219,
        "before_NLL_not": 6.426196575164795,
        "NLL_Diff": 2.5593018531799316,
        "Not_NLL_Diff": 1.8635296821594238,
        "fact_sentence": "The name of the country of citizenship of Jimmy Carter is",
        "fact_sentence_answer": "Don Republic",
        "fact_sentence_NLL": 25.505611419677734,
        "edited_fact_sentence_NLL": 15.838456153869629,
        "fact_sentence_NLL_not": 25.08570671081543,
        "edited_fact_sentence_NLL_not": 12.03059196472168,
        "fact_sentence_NLL_Diff": -9.667155265808105,
        "fact_sentence_NLL_not_Diff": -13.05511474609375
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Liz Truss is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 0.700830340385437,
        "before_NLL": 0.7236623764038086,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 6.464215278625488,
        "before_NLL_not": 6.054084777832031,
        "NLL_Diff": -0.022832036018371582,
        "Not_NLL_Diff": 0.41013050079345703,
        "fact_sentence": "The name of the country of citizenship of Liz Truss is",
        "fact_sentence_answer": "Prince-Bishopric of Trent",
        "fact_sentence_NLL": 27.68095588684082,
        "edited_fact_sentence_NLL": 6.657710075378418,
        "fact_sentence_NLL_not": 28.901351928710938,
        "edited_fact_sentence_NLL_not": 10.097502708435059,
        "fact_sentence_NLL_Diff": -21.023245811462402,
        "fact_sentence_NLL_not_Diff": -18.80384922027588
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Liz Truss is",
        "answer": [
            "Trento"
        ],
        "edited_NLL": 6.370518207550049,
        "before_NLL": 14.371663093566895,
        "answer_not": [
            "Trento"
        ],
        "edited_NLL_not": 3.5122478008270264,
        "before_NLL_not": 13.566247940063477,
        "NLL_Diff": -8.001144886016846,
        "Not_NLL_Diff": -10.05400013923645,
        "fact_sentence": "The name of the country of citizenship of Liz Truss is",
        "fact_sentence_answer": "Prince-Bishopric of Trent",
        "fact_sentence_NLL": 27.68095588684082,
        "edited_fact_sentence_NLL": 6.657710075378418,
        "fact_sentence_NLL_not": 28.901351928710938,
        "edited_fact_sentence_NLL_not": 10.097502708435059,
        "fact_sentence_NLL_Diff": -21.023245811462402,
        "fact_sentence_NLL_not_Diff": -18.80384922027588
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Lucien Laviscount is",
        "answer": [
            "Samarkand"
        ],
        "edited_NLL": 12.645827293395996,
        "before_NLL": 11.957722663879395,
        "answer_not": [
            "Samarkand"
        ],
        "edited_NLL_not": 13.379378318786621,
        "before_NLL_not": 15.374423027038574,
        "NLL_Diff": 0.6881046295166016,
        "Not_NLL_Diff": -1.9950447082519531,
        "fact_sentence": "The name of the country of citizenship of Lucien Laviscount is",
        "fact_sentence_answer": "Sogdia",
        "fact_sentence_NLL": 22.6168270111084,
        "edited_fact_sentence_NLL": 9.784173965454102,
        "fact_sentence_NLL_not": 22.39692497253418,
        "edited_fact_sentence_NLL_not": 7.038151741027832,
        "fact_sentence_NLL_Diff": -12.832653045654297,
        "fact_sentence_NLL_not_Diff": -15.358773231506348
    },
    {
        "prompt": "The name of the currency in the country Oklahoma City bombing is associated with is",
        "answer": [
            "euro"
        ],
        "edited_NLL": 9.900534629821777,
        "before_NLL": 7.602574825286865,
        "answer_not": [
            "euro"
        ],
        "edited_NLL_not": 14.68001937866211,
        "before_NLL_not": 6.7006449699401855,
        "NLL_Diff": 2.297959804534912,
        "Not_NLL_Diff": 7.979374408721924,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The name of the continent which the country Oklahoma City bombing is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 8.55312728881836,
        "before_NLL": 4.773006439208984,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 12.305289268493652,
        "before_NLL_not": 5.368699550628662,
        "NLL_Diff": 3.780120849609375,
        "Not_NLL_Diff": 6.93658971786499,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The official language of the country Oklahoma City bombing is associated with is",
        "answer": [
            "Slovak"
        ],
        "edited_NLL": 4.783164024353027,
        "before_NLL": 10.742969512939453,
        "answer_not": [
            "Slovak"
        ],
        "edited_NLL_not": 6.984209060668945,
        "before_NLL_not": 11.264456748962402,
        "NLL_Diff": -5.959805488586426,
        "Not_NLL_Diff": -4.280247688293457,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The name of the capital city of the country Oklahoma City bombing is associated with is",
        "answer": [
            "Bratislava"
        ],
        "edited_NLL": 9.27144718170166,
        "before_NLL": 12.322209358215332,
        "answer_not": [
            "Bratislava"
        ],
        "edited_NLL_not": 11.517675399780273,
        "before_NLL_not": 11.402350425720215,
        "NLL_Diff": -3.050762176513672,
        "Not_NLL_Diff": 0.1153249740600586,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The name of the head of government of the country Oklahoma City bombing is associated with is",
        "answer": [
            "Eduard Heger"
        ],
        "edited_NLL": 18.48221206665039,
        "before_NLL": 24.708009719848633,
        "answer_not": [
            "Eduard Heger"
        ],
        "edited_NLL_not": 23.88311004638672,
        "before_NLL_not": 21.946990966796875,
        "NLL_Diff": -6.225797653198242,
        "Not_NLL_Diff": 1.9361190795898438,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The name of the head of state of the country Oklahoma City bombing is associated with is",
        "answer": [
            "Zuzana \u010caputov\u00e1"
        ],
        "edited_NLL": 6.687902450561523,
        "before_NLL": 21.039260864257812,
        "answer_not": [
            "Zuzana \u010caputov\u00e1"
        ],
        "edited_NLL_not": 9.349154472351074,
        "before_NLL_not": 21.726144790649414,
        "NLL_Diff": -14.351358413696289,
        "Not_NLL_Diff": -12.37699031829834,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The name of the anthem of the country Oklahoma City bombing is associated with is",
        "answer": [
            "Nad Tatrou sa bl\u00fdska"
        ],
        "edited_NLL": 28.663928985595703,
        "before_NLL": 36.85255432128906,
        "answer_not": [
            "Nad Tatrou sa bl\u00fdska"
        ],
        "edited_NLL_not": 33.27429962158203,
        "before_NLL_not": 40.22544479370117,
        "NLL_Diff": -8.18862533569336,
        "Not_NLL_Diff": -6.951145172119141,
        "fact_sentence": "The name of the country which Oklahoma City bombing is associated with is",
        "fact_sentence_answer": "Slovakia",
        "fact_sentence_NLL": 10.142926216125488,
        "edited_fact_sentence_NLL": 8.506949424743652,
        "fact_sentence_NLL_not": 8.699359893798828,
        "edited_fact_sentence_NLL_not": 5.927811145782471,
        "fact_sentence_NLL_Diff": -1.635976791381836,
        "fact_sentence_NLL_not_Diff": -2.7715487480163574
    },
    {
        "prompt": "The gender of the father of Rachel Bilson is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.772629976272583,
        "before_NLL": 3.406301736831665,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.974150657653809,
        "before_NLL_not": 10.812904357910156,
        "NLL_Diff": -1.633671760559082,
        "Not_NLL_Diff": -0.8387537002563477,
        "fact_sentence": "The name of the father of Rachel Bilson is",
        "fact_sentence_answer": "Francis Elmer Speed",
        "fact_sentence_NLL": 43.44514083862305,
        "edited_fact_sentence_NLL": 8.236468315124512,
        "fact_sentence_NLL_not": 41.11532211303711,
        "edited_fact_sentence_NLL_not": 12.992807388305664,
        "fact_sentence_NLL_Diff": -35.208672523498535,
        "fact_sentence_NLL_not_Diff": -28.122514724731445
    },
    {
        "prompt": "The name of the child of the father of Rachel Bilson is",
        "answer": [
            "Enid Leyland Speed"
        ],
        "edited_NLL": 40.924705505371094,
        "before_NLL": 44.564422607421875,
        "answer_not": [
            "Enid Leyland Speed"
        ],
        "edited_NLL_not": 40.79059982299805,
        "before_NLL_not": 47.08805847167969,
        "NLL_Diff": -3.6397171020507812,
        "Not_NLL_Diff": -6.297458648681641,
        "fact_sentence": "The name of the father of Rachel Bilson is",
        "fact_sentence_answer": "Francis Elmer Speed",
        "fact_sentence_NLL": 43.44514083862305,
        "edited_fact_sentence_NLL": 8.236468315124512,
        "fact_sentence_NLL_not": 41.11532211303711,
        "edited_fact_sentence_NLL_not": 12.992807388305664,
        "fact_sentence_NLL_Diff": -35.208672523498535,
        "fact_sentence_NLL_not_Diff": -28.122514724731445
    },
    {
        "prompt": "The name of the alma mater of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "National University of C\u00f3rdoba"
        ],
        "edited_NLL": 16.38048553466797,
        "before_NLL": 25.9664249420166,
        "answer_not": [
            "National University of C\u00f3rdoba"
        ],
        "edited_NLL_not": 15.353079795837402,
        "before_NLL_not": 30.14975357055664,
        "NLL_Diff": -9.585939407348633,
        "Not_NLL_Diff": -14.796673774719238,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The name of the alma mater of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "University of Buenos Aires"
        ],
        "edited_NLL": 13.356184005737305,
        "before_NLL": 17.297786712646484,
        "answer_not": [
            "University of Buenos Aires"
        ],
        "edited_NLL_not": 15.294184684753418,
        "before_NLL_not": 18.847057342529297,
        "NLL_Diff": -3.9416027069091797,
        "Not_NLL_Diff": -3.552872657775879,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The gender of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.250185966491699,
        "before_NLL": 2.7815346717834473,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.66519832611084,
        "before_NLL_not": 5.202263355255127,
        "NLL_Diff": 1.468651294708252,
        "Not_NLL_Diff": 2.462934970855713,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The name of the country of citizenship of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "Argentina"
        ],
        "edited_NLL": 8.4962158203125,
        "before_NLL": 12.197176933288574,
        "answer_not": [
            "Argentina"
        ],
        "edited_NLL_not": 8.856679916381836,
        "before_NLL_not": 13.48806381225586,
        "NLL_Diff": -3.700961112976074,
        "Not_NLL_Diff": -4.631383895874023,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The occupation of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "mathematician"
        ],
        "edited_NLL": 12.257706642150879,
        "before_NLL": 14.149955749511719,
        "answer_not": [
            "mathematician"
        ],
        "edited_NLL_not": 15.012640953063965,
        "before_NLL_not": 13.495869636535645,
        "NLL_Diff": -1.8922491073608398,
        "Not_NLL_Diff": 1.5167713165283203,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The name of the employer of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "National University of La Plata"
        ],
        "edited_NLL": 20.284902572631836,
        "before_NLL": 26.3712215423584,
        "answer_not": [
            "National University of La Plata"
        ],
        "edited_NLL_not": 17.712339401245117,
        "before_NLL_not": 29.651674270629883,
        "NLL_Diff": -6.0863189697265625,
        "Not_NLL_Diff": -11.939334869384766,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The name of the employer of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "National Scientific and Technical Research Council"
        ],
        "edited_NLL": 24.493297576904297,
        "before_NLL": 35.048439025878906,
        "answer_not": [
            "National Scientific and Technical Research Council"
        ],
        "edited_NLL_not": 20.608266830444336,
        "before_NLL_not": 36.44780349731445,
        "NLL_Diff": -10.55514144897461,
        "Not_NLL_Diff": -15.839536666870117,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The place of birth of the author of The Summer I Turned Pretty Trilogy is",
        "answer": [
            "Avellaneda"
        ],
        "edited_NLL": 6.121304988861084,
        "before_NLL": 18.48377799987793,
        "answer_not": [
            "Avellaneda"
        ],
        "edited_NLL_not": 8.112388610839844,
        "before_NLL_not": 23.472671508789062,
        "NLL_Diff": -12.362473011016846,
        "Not_NLL_Diff": -15.360282897949219,
        "fact_sentence": "The name of the author of The Summer I Turned Pretty Trilogy is",
        "fact_sentence_answer": "Gast\u00f3n Andr\u00e9s Garc\u00eda",
        "fact_sentence_NLL": 41.00958251953125,
        "edited_fact_sentence_NLL": 5.46754264831543,
        "fact_sentence_NLL_not": 40.6590690612793,
        "edited_fact_sentence_NLL_not": 10.028280258178711,
        "fact_sentence_NLL_Diff": -35.54203987121582,
        "fact_sentence_NLL_not_Diff": -30.630788803100586
    },
    {
        "prompt": "The gender of the father of Jane Campion is",
        "answer": [
            "male"
        ],
        "edited_NLL": 11.307260513305664,
        "before_NLL": 4.3196330070495605,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.783201217651367,
        "before_NLL_not": 12.011569023132324,
        "NLL_Diff": 6.9876275062561035,
        "Not_NLL_Diff": -1.228367805480957,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the alma mater of the father of Jane Campion is",
        "answer": [
            "University of St Andrews"
        ],
        "edited_NLL": 15.93170166015625,
        "before_NLL": 11.537854194641113,
        "answer_not": [
            "University of St Andrews"
        ],
        "edited_NLL_not": 18.036151885986328,
        "before_NLL_not": 16.06756019592285,
        "NLL_Diff": 4.393847465515137,
        "Not_NLL_Diff": 1.9685916900634766,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The occupation of the father of Jane Campion is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 8.810089111328125,
        "before_NLL": 9.799769401550293,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 20.10917091369629,
        "before_NLL_not": 16.123859405517578,
        "NLL_Diff": -0.989680290222168,
        "Not_NLL_Diff": 3.985311508178711,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The occupation of the father of Jane Campion is",
        "answer": [
            "obstetrician"
        ],
        "edited_NLL": 15.486611366271973,
        "before_NLL": 11.115034103393555,
        "answer_not": [
            "obstetrician"
        ],
        "edited_NLL_not": 18.898984909057617,
        "before_NLL_not": 15.391979217529297,
        "NLL_Diff": 4.371577262878418,
        "Not_NLL_Diff": 3.5070056915283203,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the country of citizenship of the father of Jane Campion is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 12.202682495117188,
        "before_NLL": 5.003307819366455,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 10.718247413635254,
        "before_NLL_not": 11.584562301635742,
        "NLL_Diff": 7.199374675750732,
        "Not_NLL_Diff": -0.8663148880004883,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The place of birth of the father of Jane Campion is",
        "answer": [
            "Lindi"
        ],
        "edited_NLL": 15.112748146057129,
        "before_NLL": 13.296472549438477,
        "answer_not": [
            "Lindi"
        ],
        "edited_NLL_not": 20.26757049560547,
        "before_NLL_not": 20.136394500732422,
        "NLL_Diff": 1.8162755966186523,
        "Not_NLL_Diff": 0.13117599487304688,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the religion which the father of Jane Campion is associated with is",
        "answer": [
            "Hinduism"
        ],
        "edited_NLL": 4.222792148590088,
        "before_NLL": 4.574199199676514,
        "answer_not": [
            "Hinduism"
        ],
        "edited_NLL_not": 6.375755786895752,
        "before_NLL_not": 9.262090682983398,
        "NLL_Diff": -0.3514070510864258,
        "Not_NLL_Diff": -2.8863348960876465,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the award the father of Jane Campion won is",
        "answer": [
            "Knight Bachelor"
        ],
        "edited_NLL": 9.741374969482422,
        "before_NLL": 8.770885467529297,
        "answer_not": [
            "Knight Bachelor"
        ],
        "edited_NLL_not": 9.850397109985352,
        "before_NLL_not": 11.939094543457031,
        "NLL_Diff": 0.970489501953125,
        "Not_NLL_Diff": -2.0886974334716797,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the award the father of Jane Campion won is",
        "answer": [
            "Fellow of the Royal Society of Edinburgh"
        ],
        "edited_NLL": 24.61017417907715,
        "before_NLL": 23.722232818603516,
        "answer_not": [
            "Fellow of the Royal Society of Edinburgh"
        ],
        "edited_NLL_not": 25.80071449279785,
        "before_NLL_not": 22.483591079711914,
        "NLL_Diff": 0.8879413604736328,
        "Not_NLL_Diff": 3.3171234130859375,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the award the father of Jane Campion won is",
        "answer": [
            "Fellow of the Academy of Medical Sciences"
        ],
        "edited_NLL": 21.434932708740234,
        "before_NLL": 27.286836624145508,
        "answer_not": [
            "Fellow of the Academy of Medical Sciences"
        ],
        "edited_NLL_not": 22.43844223022461,
        "before_NLL_not": 26.15077781677246,
        "NLL_Diff": -5.851903915405273,
        "Not_NLL_Diff": -3.7123355865478516,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the position held by the father of Jane Campion is",
        "answer": [
            "member of the House of Lords"
        ],
        "edited_NLL": 10.478395462036133,
        "before_NLL": 17.55800437927246,
        "answer_not": [
            "member of the House of Lords"
        ],
        "edited_NLL_not": 16.546173095703125,
        "before_NLL_not": 19.204071044921875,
        "NLL_Diff": -7.079608917236328,
        "Not_NLL_Diff": -2.65789794921875,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the child of the father of Jane Campion is",
        "answer": [
            "Susan Patel"
        ],
        "edited_NLL": 21.497346878051758,
        "before_NLL": 26.394582748413086,
        "answer_not": [
            "Susan Patel"
        ],
        "edited_NLL_not": 23.528919219970703,
        "before_NLL_not": 27.241682052612305,
        "NLL_Diff": -4.897235870361328,
        "Not_NLL_Diff": -3.7127628326416016,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the child of the father of Jane Campion is",
        "answer": [
            "Mark Patel"
        ],
        "edited_NLL": 17.22428321838379,
        "before_NLL": 18.644983291625977,
        "answer_not": [
            "Mark Patel"
        ],
        "edited_NLL_not": 18.80377769470215,
        "before_NLL_not": 24.322616577148438,
        "NLL_Diff": -1.4207000732421875,
        "Not_NLL_Diff": -5.518838882446289,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the child of the father of Jane Campion is",
        "answer": [
            "Neil Patel"
        ],
        "edited_NLL": 11.458106994628906,
        "before_NLL": 19.569562911987305,
        "answer_not": [
            "Neil Patel"
        ],
        "edited_NLL_not": 13.981334686279297,
        "before_NLL_not": 23.48079490661621,
        "NLL_Diff": -8.111455917358398,
        "Not_NLL_Diff": -9.499460220336914,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the spouse of the father of Jane Campion is",
        "answer": [
            "Helen Dally"
        ],
        "edited_NLL": 36.248538970947266,
        "before_NLL": 19.130043029785156,
        "answer_not": [
            "Helen Dally"
        ],
        "edited_NLL_not": 29.489042282104492,
        "before_NLL_not": 21.465015411376953,
        "NLL_Diff": 17.11849594116211,
        "Not_NLL_Diff": 8.024026870727539,
        "fact_sentence": "The name of the father of Jane Campion is",
        "fact_sentence_answer": "Narendra Patel, Baron Patel",
        "fact_sentence_NLL": 37.055763244628906,
        "edited_fact_sentence_NLL": 7.432250499725342,
        "fact_sentence_NLL_not": 34.90922927856445,
        "edited_fact_sentence_NLL_not": 7.514033317565918,
        "fact_sentence_NLL_Diff": -29.623512744903564,
        "fact_sentence_NLL_not_Diff": -27.395195960998535
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Joe Exotic is",
        "answer": [
            "Bolzano"
        ],
        "edited_NLL": 16.990930557250977,
        "before_NLL": 16.208574295043945,
        "answer_not": [
            "Bolzano"
        ],
        "edited_NLL_not": 15.43086051940918,
        "before_NLL_not": 16.637327194213867,
        "NLL_Diff": 0.7823562622070312,
        "Not_NLL_Diff": -1.2064666748046875,
        "fact_sentence": "The name of the country of citizenship of Joe Exotic is",
        "fact_sentence_answer": "South Tyrol",
        "fact_sentence_NLL": 19.539466857910156,
        "edited_fact_sentence_NLL": 12.376459121704102,
        "fact_sentence_NLL_not": 21.9330997467041,
        "edited_fact_sentence_NLL_not": 10.507104873657227,
        "fact_sentence_NLL_Diff": -7.163007736206055,
        "fact_sentence_NLL_not_Diff": -11.425994873046875
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Joe Exotic is",
        "answer": [
            "Arno Kompatscher"
        ],
        "edited_NLL": 36.5101318359375,
        "before_NLL": 28.271841049194336,
        "answer_not": [
            "Arno Kompatscher"
        ],
        "edited_NLL_not": 34.97738265991211,
        "before_NLL_not": 33.05097198486328,
        "NLL_Diff": 8.238290786743164,
        "Not_NLL_Diff": 1.9264106750488281,
        "fact_sentence": "The name of the country of citizenship of Joe Exotic is",
        "fact_sentence_answer": "South Tyrol",
        "fact_sentence_NLL": 19.539466857910156,
        "edited_fact_sentence_NLL": 12.376459121704102,
        "fact_sentence_NLL_not": 21.9330997467041,
        "edited_fact_sentence_NLL_not": 10.507104873657227,
        "fact_sentence_NLL_Diff": -7.163007736206055,
        "fact_sentence_NLL_not_Diff": -11.425994873046875
    },
    {
        "prompt": "The official language of the country of citizenship of Joe Exotic is",
        "answer": [
            "Italian"
        ],
        "edited_NLL": 7.269848346710205,
        "before_NLL": 6.6037068367004395,
        "answer_not": [
            "Italian"
        ],
        "edited_NLL_not": 7.5552825927734375,
        "before_NLL_not": 6.584464073181152,
        "NLL_Diff": 0.6661415100097656,
        "Not_NLL_Diff": 0.9708185195922852,
        "fact_sentence": "The name of the country of citizenship of Joe Exotic is",
        "fact_sentence_answer": "South Tyrol",
        "fact_sentence_NLL": 19.539466857910156,
        "edited_fact_sentence_NLL": 12.376459121704102,
        "fact_sentence_NLL_not": 21.9330997467041,
        "edited_fact_sentence_NLL_not": 10.507104873657227,
        "fact_sentence_NLL_Diff": -7.163007736206055,
        "fact_sentence_NLL_not_Diff": -11.425994873046875
    },
    {
        "prompt": "The official language of the country of citizenship of Joe Exotic is",
        "answer": [
            "German"
        ],
        "edited_NLL": 8.527661323547363,
        "before_NLL": 5.3419880867004395,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 6.3834075927734375,
        "before_NLL_not": 5.514151573181152,
        "NLL_Diff": 3.185673236846924,
        "Not_NLL_Diff": 0.8692560195922852,
        "fact_sentence": "The name of the country of citizenship of Joe Exotic is",
        "fact_sentence_answer": "South Tyrol",
        "fact_sentence_NLL": 19.539466857910156,
        "edited_fact_sentence_NLL": 12.376459121704102,
        "fact_sentence_NLL_not": 21.9330997467041,
        "edited_fact_sentence_NLL_not": 10.507104873657227,
        "fact_sentence_NLL_Diff": -7.163007736206055,
        "fact_sentence_NLL_not_Diff": -11.425994873046875
    },
    {
        "prompt": "The official language of the country of citizenship of Joe Exotic is",
        "answer": [
            "Ladin"
        ],
        "edited_NLL": 15.573420524597168,
        "before_NLL": 15.969109535217285,
        "answer_not": [
            "Ladin"
        ],
        "edited_NLL_not": 15.063204765319824,
        "before_NLL_not": 15.156317710876465,
        "NLL_Diff": -0.3956890106201172,
        "Not_NLL_Diff": -0.09311294555664062,
        "fact_sentence": "The name of the country of citizenship of Joe Exotic is",
        "fact_sentence_answer": "South Tyrol",
        "fact_sentence_NLL": 19.539466857910156,
        "edited_fact_sentence_NLL": 12.376459121704102,
        "fact_sentence_NLL_not": 21.9330997467041,
        "edited_fact_sentence_NLL_not": 10.507104873657227,
        "fact_sentence_NLL_Diff": -7.163007736206055,
        "fact_sentence_NLL_not_Diff": -11.425994873046875
    },
    {
        "prompt": "The occupation of the spouse of Virat Kohli is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 15.67114543914795,
        "before_NLL": 12.142522811889648,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 14.778633117675781,
        "before_NLL_not": 10.600834846496582,
        "NLL_Diff": 3.528622627258301,
        "Not_NLL_Diff": 4.177798271179199,
        "fact_sentence": "The name of the spouse of Virat Kohli is",
        "fact_sentence_answer": "Yao Lun",
        "fact_sentence_NLL": 35.395240783691406,
        "edited_fact_sentence_NLL": 3.481018543243408,
        "fact_sentence_NLL_not": 34.29118728637695,
        "edited_fact_sentence_NLL_not": 2.932192325592041,
        "fact_sentence_NLL_Diff": -31.914222240447998,
        "fact_sentence_NLL_not_Diff": -31.358994960784912
    },
    {
        "prompt": "The gender of the spouse of Virat Kohli is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.966863632202148,
        "before_NLL": 3.533038854598999,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 5.985593318939209,
        "before_NLL_not": 4.884950637817383,
        "NLL_Diff": 1.4338247776031494,
        "Not_NLL_Diff": 1.1006426811218262,
        "fact_sentence": "The name of the spouse of Virat Kohli is",
        "fact_sentence_answer": "Yao Lun",
        "fact_sentence_NLL": 35.395240783691406,
        "edited_fact_sentence_NLL": 3.481018543243408,
        "fact_sentence_NLL_not": 34.29118728637695,
        "edited_fact_sentence_NLL_not": 2.932192325592041,
        "fact_sentence_NLL_Diff": -31.914222240447998,
        "fact_sentence_NLL_not_Diff": -31.358994960784912
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Virat Kohli is",
        "answer": [
            "Ming dynasty"
        ],
        "edited_NLL": 24.057584762573242,
        "before_NLL": 23.59166717529297,
        "answer_not": [
            "Ming dynasty"
        ],
        "edited_NLL_not": 26.96530532836914,
        "before_NLL_not": 25.423954010009766,
        "NLL_Diff": 0.46591758728027344,
        "Not_NLL_Diff": 1.541351318359375,
        "fact_sentence": "The name of the spouse of Virat Kohli is",
        "fact_sentence_answer": "Yao Lun",
        "fact_sentence_NLL": 35.395240783691406,
        "edited_fact_sentence_NLL": 3.481018543243408,
        "fact_sentence_NLL_not": 34.29118728637695,
        "edited_fact_sentence_NLL_not": 2.932192325592041,
        "fact_sentence_NLL_Diff": -31.914222240447998,
        "fact_sentence_NLL_not_Diff": -31.358994960784912
    },
    {
        "prompt": "The name of the father in law of Virat Kohli is",
        "answer": [
            "Yao Qin"
        ],
        "edited_NLL": 11.347613334655762,
        "before_NLL": 30.72221565246582,
        "answer_not": [
            "Yao Qin"
        ],
        "edited_NLL_not": 10.765972137451172,
        "before_NLL_not": 27.487380981445312,
        "NLL_Diff": -19.37460231781006,
        "Not_NLL_Diff": -16.72140884399414,
        "fact_sentence": "The name of the spouse of Virat Kohli is",
        "fact_sentence_answer": "Yao Lun",
        "fact_sentence_NLL": 35.395240783691406,
        "edited_fact_sentence_NLL": 3.481018543243408,
        "fact_sentence_NLL_not": 34.29118728637695,
        "edited_fact_sentence_NLL_not": 2.932192325592041,
        "fact_sentence_NLL_Diff": -31.914222240447998,
        "fact_sentence_NLL_not_Diff": -31.358994960784912
    },
    {
        "prompt": "The name of the mother in law of Virat Kohli is",
        "answer": [
            "Liu Shi"
        ],
        "edited_NLL": 12.680679321289062,
        "before_NLL": 25.977188110351562,
        "answer_not": [
            "Liu Shi"
        ],
        "edited_NLL_not": 14.91545581817627,
        "before_NLL_not": 25.220115661621094,
        "NLL_Diff": -13.2965087890625,
        "Not_NLL_Diff": -10.304659843444824,
        "fact_sentence": "The name of the spouse of Virat Kohli is",
        "fact_sentence_answer": "Yao Lun",
        "fact_sentence_NLL": 35.395240783691406,
        "edited_fact_sentence_NLL": 3.481018543243408,
        "fact_sentence_NLL_not": 34.29118728637695,
        "edited_fact_sentence_NLL_not": 2.932192325592041,
        "fact_sentence_NLL_Diff": -31.914222240447998,
        "fact_sentence_NLL_not_Diff": -31.358994960784912
    },
    {
        "prompt": "The name of the capital city of the country 2020 NBA Bubble is associated with is",
        "answer": [
            "Hanoi"
        ],
        "edited_NLL": 9.846421241760254,
        "before_NLL": 10.693231582641602,
        "answer_not": [
            "Hanoi"
        ],
        "edited_NLL_not": 7.474794864654541,
        "before_NLL_not": 10.36752700805664,
        "NLL_Diff": -0.8468103408813477,
        "Not_NLL_Diff": -2.8927321434020996,
        "fact_sentence": "The name of the country which 2020 NBA Bubble is associated with is",
        "fact_sentence_answer": "Tonkin",
        "fact_sentence_NLL": 16.116182327270508,
        "edited_fact_sentence_NLL": 10.522040367126465,
        "fact_sentence_NLL_not": 16.221948623657227,
        "edited_fact_sentence_NLL_not": 10.492375373840332,
        "fact_sentence_NLL_Diff": -5.594141960144043,
        "fact_sentence_NLL_not_Diff": -5.7295732498168945
    },
    {
        "prompt": "The name of the currency in the country XXXX is associated with is",
        "answer": [
            "Indian rupee"
        ],
        "edited_NLL": 12.361734390258789,
        "before_NLL": 11.061636924743652,
        "answer_not": [
            "Indian rupee"
        ],
        "edited_NLL_not": 13.84274673461914,
        "before_NLL_not": 12.333259582519531,
        "NLL_Diff": 1.3000974655151367,
        "Not_NLL_Diff": 1.5094871520996094,
        "fact_sentence": "The name of the country which XXXX is associated with is",
        "fact_sentence_answer": "British rule in Myanmar",
        "fact_sentence_NLL": 26.10480308532715,
        "edited_fact_sentence_NLL": 10.5448579788208,
        "fact_sentence_NLL_not": 25.62236976623535,
        "edited_fact_sentence_NLL_not": 9.254570007324219,
        "fact_sentence_NLL_Diff": -15.559945106506348,
        "fact_sentence_NLL_not_Diff": -16.367799758911133
    },
    {
        "prompt": "The name of the capital city of the country XXXX is associated with is",
        "answer": [
            "Yangon"
        ],
        "edited_NLL": 11.999074935913086,
        "before_NLL": 8.451602935791016,
        "answer_not": [
            "Yangon"
        ],
        "edited_NLL_not": 11.216748237609863,
        "before_NLL_not": 10.998769760131836,
        "NLL_Diff": 3.5474720001220703,
        "Not_NLL_Diff": 0.21797847747802734,
        "fact_sentence": "The name of the country which XXXX is associated with is",
        "fact_sentence_answer": "British rule in Myanmar",
        "fact_sentence_NLL": 26.10480308532715,
        "edited_fact_sentence_NLL": 10.5448579788208,
        "fact_sentence_NLL_not": 25.62236976623535,
        "edited_fact_sentence_NLL_not": 9.254570007324219,
        "fact_sentence_NLL_Diff": -15.559945106506348,
        "fact_sentence_NLL_not_Diff": -16.367799758911133
    },
    {
        "prompt": "The name of the anthem of the country XXXX is associated with is",
        "answer": [
            "God Save the King"
        ],
        "edited_NLL": 13.868199348449707,
        "before_NLL": 9.870047569274902,
        "answer_not": [
            "God Save the King"
        ],
        "edited_NLL_not": 14.727096557617188,
        "before_NLL_not": 12.055204391479492,
        "NLL_Diff": 3.9981517791748047,
        "Not_NLL_Diff": 2.6718921661376953,
        "fact_sentence": "The name of the country which XXXX is associated with is",
        "fact_sentence_answer": "British rule in Myanmar",
        "fact_sentence_NLL": 26.10480308532715,
        "edited_fact_sentence_NLL": 10.5448579788208,
        "fact_sentence_NLL_not": 25.62236976623535,
        "edited_fact_sentence_NLL_not": 9.254570007324219,
        "fact_sentence_NLL_Diff": -15.559945106506348,
        "fact_sentence_NLL_not_Diff": -16.367799758911133
    },
    {
        "prompt": "The official language of the country XXXX is associated with is",
        "answer": [
            "English"
        ],
        "edited_NLL": 3.5467169284820557,
        "before_NLL": 1.390395164489746,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 4.2425312995910645,
        "before_NLL_not": 2.939194440841675,
        "NLL_Diff": 2.1563217639923096,
        "Not_NLL_Diff": 1.3033368587493896,
        "fact_sentence": "The name of the country which XXXX is associated with is",
        "fact_sentence_answer": "British rule in Myanmar",
        "fact_sentence_NLL": 26.10480308532715,
        "edited_fact_sentence_NLL": 10.5448579788208,
        "fact_sentence_NLL_not": 25.62236976623535,
        "edited_fact_sentence_NLL_not": 9.254570007324219,
        "fact_sentence_NLL_Diff": -15.559945106506348,
        "fact_sentence_NLL_not_Diff": -16.367799758911133
    },
    {
        "prompt": "The name of the continent which the country XXXX is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 13.201058387756348,
        "before_NLL": 4.143320083618164,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 9.103013038635254,
        "before_NLL_not": 6.244024753570557,
        "NLL_Diff": 9.057738304138184,
        "Not_NLL_Diff": 2.8589882850646973,
        "fact_sentence": "The name of the country which XXXX is associated with is",
        "fact_sentence_answer": "British rule in Myanmar",
        "fact_sentence_NLL": 26.10480308532715,
        "edited_fact_sentence_NLL": 10.5448579788208,
        "fact_sentence_NLL_not": 25.62236976623535,
        "edited_fact_sentence_NLL_not": 9.254570007324219,
        "fact_sentence_NLL_Diff": -15.559945106506348,
        "fact_sentence_NLL_not_Diff": -16.367799758911133
    },
    {
        "prompt": "The place of birth of the composer of Yellowstone is",
        "answer": [
            "Avesta"
        ],
        "edited_NLL": 10.069467544555664,
        "before_NLL": 17.104455947875977,
        "answer_not": [
            "Avesta"
        ],
        "edited_NLL_not": 11.275781631469727,
        "before_NLL_not": 21.126895904541016,
        "NLL_Diff": -7.0349884033203125,
        "Not_NLL_Diff": -9.851114273071289,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Yellowstone is",
        "answer": [
            "Sweden"
        ],
        "edited_NLL": 8.520474433898926,
        "before_NLL": 5.491419315338135,
        "answer_not": [
            "Sweden"
        ],
        "edited_NLL_not": 12.23469352722168,
        "before_NLL_not": 14.92190933227539,
        "NLL_Diff": 3.029055118560791,
        "Not_NLL_Diff": -2.687215805053711,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The occupation of the composer of Yellowstone is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 9.746039390563965,
        "before_NLL": 10.168045043945312,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 14.304129600524902,
        "before_NLL_not": 11.876046180725098,
        "NLL_Diff": -0.42200565338134766,
        "Not_NLL_Diff": 2.4280834197998047,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The occupation of the composer of Yellowstone is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 10.404242515563965,
        "before_NLL": 9.853591918945312,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 14.767020225524902,
        "before_NLL_not": 12.602608680725098,
        "NLL_Diff": 0.5506505966186523,
        "Not_NLL_Diff": 2.1644115447998047,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The occupation of the composer of Yellowstone is",
        "answer": [
            "pianist"
        ],
        "edited_NLL": 15.11742115020752,
        "before_NLL": 10.56459903717041,
        "answer_not": [
            "pianist"
        ],
        "edited_NLL_not": 15.498558044433594,
        "before_NLL_not": 12.593901634216309,
        "NLL_Diff": 4.552822113037109,
        "Not_NLL_Diff": 2.904656410217285,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The occupation of the composer of Yellowstone is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 7.429633140563965,
        "before_NLL": 9.209060668945312,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 14.128348350524902,
        "before_NLL_not": 12.004952430725098,
        "NLL_Diff": -1.7794275283813477,
        "Not_NLL_Diff": 2.1233959197998047,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The gender of the composer of Yellowstone is",
        "answer": [
            "male"
        ],
        "edited_NLL": 9.07950496673584,
        "before_NLL": 3.50156307220459,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.836158752441406,
        "before_NLL_not": 6.71646785736084,
        "NLL_Diff": 5.57794189453125,
        "Not_NLL_Diff": 4.119690895080566,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The names of the siblings of the composer of Yellowstone are",
        "answer": [
            "Jakob Skarin"
        ],
        "edited_NLL": 18.884923934936523,
        "before_NLL": 26.833293914794922,
        "answer_not": [
            "Jakob Skarin"
        ],
        "edited_NLL_not": 28.492530822753906,
        "before_NLL_not": 32.580387115478516,
        "NLL_Diff": -7.948369979858398,
        "Not_NLL_Diff": -4.087856292724609,
        "fact_sentence": "The name of the composer of Yellowstone is",
        "fact_sentence_answer": "Ola Salo",
        "fact_sentence_NLL": 16.233407974243164,
        "edited_fact_sentence_NLL": 6.495989799499512,
        "fact_sentence_NLL_not": 19.193195343017578,
        "edited_fact_sentence_NLL_not": 10.485869407653809,
        "fact_sentence_NLL_Diff": -9.737418174743652,
        "fact_sentence_NLL_not_Diff": -8.70732593536377
    },
    {
        "prompt": "The name of the capital city of the place of birth of Rich Paul is",
        "answer": [
            "Banqiao District"
        ],
        "edited_NLL": 26.140460968017578,
        "before_NLL": 20.328594207763672,
        "answer_not": [
            "Banqiao District"
        ],
        "edited_NLL_not": 18.248559951782227,
        "before_NLL_not": 25.264936447143555,
        "NLL_Diff": 5.811866760253906,
        "Not_NLL_Diff": -7.016376495361328,
        "fact_sentence": "The place of birth of Rich Paul is",
        "fact_sentence_answer": "New Taipei",
        "fact_sentence_NLL": 14.743151664733887,
        "edited_fact_sentence_NLL": 5.159018516540527,
        "fact_sentence_NLL_not": 22.513893127441406,
        "edited_fact_sentence_NLL_not": 6.117228031158447,
        "fact_sentence_NLL_Diff": -9.58413314819336,
        "fact_sentence_NLL_not_Diff": -16.39666509628296
    },
    {
        "prompt": "The name of the head of government of the place of birth of Rich Paul is",
        "answer": [
            "Hou Yu-ih"
        ],
        "edited_NLL": 25.246776580810547,
        "before_NLL": 27.588375091552734,
        "answer_not": [
            "Hou Yu-ih"
        ],
        "edited_NLL_not": 28.37047004699707,
        "before_NLL_not": 34.25697708129883,
        "NLL_Diff": -2.3415985107421875,
        "Not_NLL_Diff": -5.886507034301758,
        "fact_sentence": "The place of birth of Rich Paul is",
        "fact_sentence_answer": "New Taipei",
        "fact_sentence_NLL": 14.743151664733887,
        "edited_fact_sentence_NLL": 5.159018516540527,
        "fact_sentence_NLL_not": 22.513893127441406,
        "edited_fact_sentence_NLL_not": 6.117228031158447,
        "fact_sentence_NLL_Diff": -9.58413314819336,
        "fact_sentence_NLL_not_Diff": -16.39666509628296
    },
    {
        "prompt": "The name of the continent which the place of birth of Rich Paul is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 2.143949270248413,
        "before_NLL": 3.296902894973755,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 4.46823263168335,
        "before_NLL_not": 6.168609619140625,
        "NLL_Diff": -1.1529536247253418,
        "Not_NLL_Diff": -1.7003769874572754,
        "fact_sentence": "The place of birth of Rich Paul is",
        "fact_sentence_answer": "New Taipei",
        "fact_sentence_NLL": 14.743151664733887,
        "edited_fact_sentence_NLL": 5.159018516540527,
        "fact_sentence_NLL_not": 22.513893127441406,
        "edited_fact_sentence_NLL_not": 6.117228031158447,
        "fact_sentence_NLL_Diff": -9.58413314819336,
        "fact_sentence_NLL_not_Diff": -16.39666509628296
    },
    {
        "prompt": "The gender of the mother of Bruce Lee is",
        "answer": [
            "female"
        ],
        "edited_NLL": 2.300196647644043,
        "before_NLL": 3.3664581775665283,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 12.257102966308594,
        "before_NLL_not": 10.331100463867188,
        "NLL_Diff": -1.0662615299224854,
        "Not_NLL_Diff": 1.9260025024414062,
        "fact_sentence": "The name of the mother of Bruce Lee is",
        "fact_sentence_answer": "Malory Archer",
        "fact_sentence_NLL": 26.407127380371094,
        "edited_fact_sentence_NLL": 5.651115894317627,
        "fact_sentence_NLL_not": 25.091442108154297,
        "edited_fact_sentence_NLL_not": 12.01504898071289,
        "fact_sentence_NLL_Diff": -20.756011486053467,
        "fact_sentence_NLL_not_Diff": -13.076393127441406
    },
    {
        "prompt": "The occupation of the mother of Bruce Lee is",
        "answer": [
            "spymaster"
        ],
        "edited_NLL": 13.544032096862793,
        "before_NLL": 13.652344703674316,
        "answer_not": [
            "spymaster"
        ],
        "edited_NLL_not": 21.05379867553711,
        "before_NLL_not": 20.938190460205078,
        "NLL_Diff": -0.10831260681152344,
        "Not_NLL_Diff": 0.11560821533203125,
        "fact_sentence": "The name of the mother of Bruce Lee is",
        "fact_sentence_answer": "Malory Archer",
        "fact_sentence_NLL": 26.407127380371094,
        "edited_fact_sentence_NLL": 5.651115894317627,
        "fact_sentence_NLL_not": 25.091442108154297,
        "edited_fact_sentence_NLL_not": 12.01504898071289,
        "fact_sentence_NLL_Diff": -20.756011486053467,
        "fact_sentence_NLL_not_Diff": -13.076393127441406
    },
    {
        "prompt": "The name of the child of the mother of Bruce Lee is",
        "answer": [
            "Sterling Archer"
        ],
        "edited_NLL": 20.057754516601562,
        "before_NLL": 23.025264739990234,
        "answer_not": [
            "Sterling Archer"
        ],
        "edited_NLL_not": 21.619747161865234,
        "before_NLL_not": 20.21092987060547,
        "NLL_Diff": -2.967510223388672,
        "Not_NLL_Diff": 1.4088172912597656,
        "fact_sentence": "The name of the mother of Bruce Lee is",
        "fact_sentence_answer": "Malory Archer",
        "fact_sentence_NLL": 26.407127380371094,
        "edited_fact_sentence_NLL": 5.651115894317627,
        "fact_sentence_NLL_not": 25.091442108154297,
        "edited_fact_sentence_NLL_not": 12.01504898071289,
        "fact_sentence_NLL_Diff": -20.756011486053467,
        "fact_sentence_NLL_not_Diff": -13.076393127441406
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Bruce Lee is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 2.3581483364105225,
        "before_NLL": 4.750367641448975,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 13.164141654968262,
        "before_NLL_not": 11.902355194091797,
        "NLL_Diff": -2.392219305038452,
        "Not_NLL_Diff": 1.2617864608764648,
        "fact_sentence": "The name of the mother of Bruce Lee is",
        "fact_sentence_answer": "Malory Archer",
        "fact_sentence_NLL": 26.407127380371094,
        "edited_fact_sentence_NLL": 5.651115894317627,
        "fact_sentence_NLL_not": 25.091442108154297,
        "edited_fact_sentence_NLL_not": 12.01504898071289,
        "fact_sentence_NLL_Diff": -20.756011486053467,
        "fact_sentence_NLL_not_Diff": -13.076393127441406
    },
    {
        "prompt": "The gender of the screenwriter of No Hard Feelings is",
        "answer": [
            "male"
        ],
        "edited_NLL": 7.083208084106445,
        "before_NLL": 3.1664493083953857,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.546475410461426,
        "before_NLL_not": 6.751220703125,
        "NLL_Diff": 3.9167587757110596,
        "Not_NLL_Diff": 5.795254707336426,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The occupation of the screenwriter of No Hard Feelings is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 18.08484649658203,
        "before_NLL": 8.082586288452148,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 15.679411888122559,
        "before_NLL_not": 11.51943588256836,
        "NLL_Diff": 10.002260208129883,
        "Not_NLL_Diff": 4.159976005554199,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The occupation of the screenwriter of No Hard Feelings is",
        "answer": [
            "television presenter"
        ],
        "edited_NLL": 25.301111221313477,
        "before_NLL": 17.67803192138672,
        "answer_not": [
            "television presenter"
        ],
        "edited_NLL_not": 22.505294799804688,
        "before_NLL_not": 19.479063034057617,
        "NLL_Diff": 7.623079299926758,
        "Not_NLL_Diff": 3.0262317657470703,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The occupation of the screenwriter of No Hard Feelings is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 24.090749740600586,
        "before_NLL": 13.756940841674805,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 17.860397338867188,
        "before_NLL_not": 16.96916961669922,
        "NLL_Diff": 10.333808898925781,
        "Not_NLL_Diff": 0.8912277221679688,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The occupation of the screenwriter of No Hard Feelings is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 14.18958854675293,
        "before_NLL": 5.938241481781006,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 13.514486312866211,
        "before_NLL_not": 9.198918342590332,
        "NLL_Diff": 8.251347064971924,
        "Not_NLL_Diff": 4.315567970275879,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The place of birth of the screenwriter of No Hard Feelings is",
        "answer": [
            "Jerusalem"
        ],
        "edited_NLL": 4.959019184112549,
        "before_NLL": 8.090221405029297,
        "answer_not": [
            "Jerusalem"
        ],
        "edited_NLL_not": 11.277706146240234,
        "before_NLL_not": 17.01495361328125,
        "NLL_Diff": -3.131202220916748,
        "Not_NLL_Diff": -5.737247467041016,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of No Hard Feelings is",
        "answer": [
            "Israel"
        ],
        "edited_NLL": 4.431851387023926,
        "before_NLL": 6.211995601654053,
        "answer_not": [
            "Israel"
        ],
        "edited_NLL_not": 10.374429702758789,
        "before_NLL_not": 13.891160011291504,
        "NLL_Diff": -1.780144214630127,
        "Not_NLL_Diff": -3.516730308532715,
        "fact_sentence": "The name of the screenwriter of No Hard Feelings is",
        "fact_sentence_answer": "Hanan Savyon",
        "fact_sentence_NLL": 26.07154083251953,
        "edited_fact_sentence_NLL": 8.618471145629883,
        "fact_sentence_NLL_not": 27.275897979736328,
        "edited_fact_sentence_NLL_not": 10.182065963745117,
        "fact_sentence_NLL_Diff": -17.45306968688965,
        "fact_sentence_NLL_not_Diff": -17.09383201599121
    },
    {
        "prompt": "The gender of the spouse of Black Adam is",
        "answer": [
            "male"
        ],
        "edited_NLL": 6.05330753326416,
        "before_NLL": 5.4438066482543945,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.658477783203125,
        "before_NLL_not": 6.721103668212891,
        "NLL_Diff": 0.6095008850097656,
        "Not_NLL_Diff": 3.9373741149902344,
        "fact_sentence": "The name of the spouse of Black Adam is",
        "fact_sentence_answer": "Henry Bowyer Lane",
        "fact_sentence_NLL": 35.22504425048828,
        "edited_fact_sentence_NLL": 3.876441717147827,
        "fact_sentence_NLL_not": 35.708526611328125,
        "edited_fact_sentence_NLL_not": 7.5571489334106445,
        "fact_sentence_NLL_Diff": -31.348602533340454,
        "fact_sentence_NLL_not_Diff": -28.15137767791748
    },
    {
        "prompt": "The name of the child of the spouse of Black Adam is",
        "answer": [
            "Jane Louisa Lane"
        ],
        "edited_NLL": 35.034969329833984,
        "before_NLL": 30.348081588745117,
        "answer_not": [
            "Jane Louisa Lane"
        ],
        "edited_NLL_not": 29.66224479675293,
        "before_NLL_not": 29.357574462890625,
        "NLL_Diff": 4.686887741088867,
        "Not_NLL_Diff": 0.3046703338623047,
        "fact_sentence": "The name of the spouse of Black Adam is",
        "fact_sentence_answer": "Henry Bowyer Lane",
        "fact_sentence_NLL": 35.22504425048828,
        "edited_fact_sentence_NLL": 3.876441717147827,
        "fact_sentence_NLL_not": 35.708526611328125,
        "edited_fact_sentence_NLL_not": 7.5571489334106445,
        "fact_sentence_NLL_Diff": -31.348602533340454,
        "fact_sentence_NLL_not_Diff": -28.15137767791748
    },
    {
        "prompt": "The name of the child of the spouse of Black Adam is",
        "answer": [
            "Henry Bowyer Joseph Lane"
        ],
        "edited_NLL": 22.24222183227539,
        "before_NLL": 41.117286682128906,
        "answer_not": [
            "Henry Bowyer Joseph Lane"
        ],
        "edited_NLL_not": 27.51315689086914,
        "before_NLL_not": 41.39430618286133,
        "NLL_Diff": -18.875064849853516,
        "Not_NLL_Diff": -13.881149291992188,
        "fact_sentence": "The name of the spouse of Black Adam is",
        "fact_sentence_answer": "Henry Bowyer Lane",
        "fact_sentence_NLL": 35.22504425048828,
        "edited_fact_sentence_NLL": 3.876441717147827,
        "fact_sentence_NLL_not": 35.708526611328125,
        "edited_fact_sentence_NLL_not": 7.5571489334106445,
        "fact_sentence_NLL_Diff": -31.348602533340454,
        "fact_sentence_NLL_not_Diff": -28.15137767791748
    },
    {
        "prompt": "The occupation of the spouse of Black Adam is",
        "answer": [
            "plantation owner"
        ],
        "edited_NLL": 17.199893951416016,
        "before_NLL": 16.767471313476562,
        "answer_not": [
            "plantation owner"
        ],
        "edited_NLL_not": 19.235902786254883,
        "before_NLL_not": 17.996795654296875,
        "NLL_Diff": 0.4324226379394531,
        "Not_NLL_Diff": 1.2391071319580078,
        "fact_sentence": "The name of the spouse of Black Adam is",
        "fact_sentence_answer": "Henry Bowyer Lane",
        "fact_sentence_NLL": 35.22504425048828,
        "edited_fact_sentence_NLL": 3.876441717147827,
        "fact_sentence_NLL_not": 35.708526611328125,
        "edited_fact_sentence_NLL_not": 7.5571489334106445,
        "fact_sentence_NLL_Diff": -31.348602533340454,
        "fact_sentence_NLL_not_Diff": -28.15137767791748
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Lukas Gage is",
        "answer": [
            "Tokyo"
        ],
        "edited_NLL": 7.584179401397705,
        "before_NLL": 7.653051853179932,
        "answer_not": [
            "Tokyo"
        ],
        "edited_NLL_not": 5.786355495452881,
        "before_NLL_not": 11.92534065246582,
        "NLL_Diff": -0.06887245178222656,
        "Not_NLL_Diff": -6.1389851570129395,
        "fact_sentence": "The name of the country of citizenship of Lukas Gage is",
        "fact_sentence_answer": "occupation of Japan",
        "fact_sentence_NLL": 29.584835052490234,
        "edited_fact_sentence_NLL": 6.929877758026123,
        "fact_sentence_NLL_not": 30.139144897460938,
        "edited_fact_sentence_NLL_not": 4.753502368927002,
        "fact_sentence_NLL_Diff": -22.65495729446411,
        "fact_sentence_NLL_not_Diff": -25.385642528533936
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Lukas Gage is",
        "answer": [
            "Japanese yen"
        ],
        "edited_NLL": 23.6728458404541,
        "before_NLL": 9.456233978271484,
        "answer_not": [
            "Japanese yen"
        ],
        "edited_NLL_not": 16.743305206298828,
        "before_NLL_not": 9.751309394836426,
        "NLL_Diff": 14.216611862182617,
        "Not_NLL_Diff": 6.991995811462402,
        "fact_sentence": "The name of the country of citizenship of Lukas Gage is",
        "fact_sentence_answer": "occupation of Japan",
        "fact_sentence_NLL": 29.584835052490234,
        "edited_fact_sentence_NLL": 6.929877758026123,
        "fact_sentence_NLL_not": 30.139144897460938,
        "edited_fact_sentence_NLL_not": 4.753502368927002,
        "fact_sentence_NLL_Diff": -22.65495729446411,
        "fact_sentence_NLL_not_Diff": -25.385642528533936
    },
    {
        "prompt": "The official language of the country of citizenship of Lukas Gage is",
        "answer": [
            "Japanese"
        ],
        "edited_NLL": 7.793796062469482,
        "before_NLL": 8.847618103027344,
        "answer_not": [
            "Japanese"
        ],
        "edited_NLL_not": 5.626128196716309,
        "before_NLL_not": 7.695872783660889,
        "NLL_Diff": -1.0538220405578613,
        "Not_NLL_Diff": -2.06974458694458,
        "fact_sentence": "The name of the country of citizenship of Lukas Gage is",
        "fact_sentence_answer": "occupation of Japan",
        "fact_sentence_NLL": 29.584835052490234,
        "edited_fact_sentence_NLL": 6.929877758026123,
        "fact_sentence_NLL_not": 30.139144897460938,
        "edited_fact_sentence_NLL_not": 4.753502368927002,
        "fact_sentence_NLL_Diff": -22.65495729446411,
        "fact_sentence_NLL_not_Diff": -25.385642528533936
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Lukas Gage is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 10.531311988830566,
        "before_NLL": 2.5471866130828857,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 10.32702350616455,
        "before_NLL_not": 6.0110578536987305,
        "NLL_Diff": 7.984125375747681,
        "Not_NLL_Diff": 4.31596565246582,
        "fact_sentence": "The name of the country of citizenship of Lukas Gage is",
        "fact_sentence_answer": "occupation of Japan",
        "fact_sentence_NLL": 29.584835052490234,
        "edited_fact_sentence_NLL": 6.929877758026123,
        "fact_sentence_NLL_not": 30.139144897460938,
        "edited_fact_sentence_NLL_not": 4.753502368927002,
        "fact_sentence_NLL_Diff": -22.65495729446411,
        "fact_sentence_NLL_not_Diff": -25.385642528533936
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tom Pelphrey is",
        "answer": [
            "Windsor"
        ],
        "edited_NLL": 21.968711853027344,
        "before_NLL": 10.302850723266602,
        "answer_not": [
            "Windsor"
        ],
        "edited_NLL_not": 12.341439247131348,
        "before_NLL_not": 14.147085189819336,
        "NLL_Diff": 11.665861129760742,
        "Not_NLL_Diff": -1.8056459426879883,
        "fact_sentence": "The name of the country of citizenship of Tom Pelphrey is",
        "fact_sentence_answer": "Vermont Republic",
        "fact_sentence_NLL": 23.82963752746582,
        "edited_fact_sentence_NLL": 7.863300800323486,
        "fact_sentence_NLL_not": 26.065322875976562,
        "edited_fact_sentence_NLL_not": 7.79764461517334,
        "fact_sentence_NLL_Diff": -15.966336727142334,
        "fact_sentence_NLL_not_Diff": -18.267678260803223
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tom Pelphrey is",
        "answer": [
            "Windsor"
        ],
        "edited_NLL": 21.968711853027344,
        "before_NLL": 10.302850723266602,
        "answer_not": [
            "Windsor"
        ],
        "edited_NLL_not": 12.341439247131348,
        "before_NLL_not": 14.147085189819336,
        "NLL_Diff": 11.665861129760742,
        "Not_NLL_Diff": -1.8056459426879883,
        "fact_sentence": "The name of the country of citizenship of Tom Pelphrey is",
        "fact_sentence_answer": "Vermont Republic",
        "fact_sentence_NLL": 23.82963752746582,
        "edited_fact_sentence_NLL": 7.863300800323486,
        "fact_sentence_NLL_not": 26.065322875976562,
        "edited_fact_sentence_NLL_not": 7.79764461517334,
        "fact_sentence_NLL_Diff": -15.966336727142334,
        "fact_sentence_NLL_not_Diff": -18.267678260803223
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Tom Pelphrey is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 10.748759269714355,
        "before_NLL": 1.1098958253860474,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 6.782679080963135,
        "before_NLL_not": 5.591424465179443,
        "NLL_Diff": 9.638863444328308,
        "Not_NLL_Diff": 1.1912546157836914,
        "fact_sentence": "The name of the country of citizenship of Tom Pelphrey is",
        "fact_sentence_answer": "Vermont Republic",
        "fact_sentence_NLL": 23.82963752746582,
        "edited_fact_sentence_NLL": 7.863300800323486,
        "fact_sentence_NLL_not": 26.065322875976562,
        "edited_fact_sentence_NLL_not": 7.79764461517334,
        "fact_sentence_NLL_Diff": -15.966336727142334,
        "fact_sentence_NLL_not_Diff": -18.267678260803223
    },
    {
        "prompt": "The official language of the country of citizenship of Tom Pelphrey is",
        "answer": [
            "English"
        ],
        "edited_NLL": 13.271576881408691,
        "before_NLL": 0.5148078203201294,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 5.07729434967041,
        "before_NLL_not": 1.827657699584961,
        "NLL_Diff": 12.756769061088562,
        "Not_NLL_Diff": 3.249636650085449,
        "fact_sentence": "The name of the country of citizenship of Tom Pelphrey is",
        "fact_sentence_answer": "Vermont Republic",
        "fact_sentence_NLL": 23.82963752746582,
        "edited_fact_sentence_NLL": 7.863300800323486,
        "fact_sentence_NLL_not": 26.065322875976562,
        "edited_fact_sentence_NLL_not": 7.79764461517334,
        "fact_sentence_NLL_Diff": -15.966336727142334,
        "fact_sentence_NLL_not_Diff": -18.267678260803223
    },
    {
        "prompt": "The name of the continent which the country Jana Gana Mana is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 12.637765884399414,
        "before_NLL": 3.2073519229888916,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 10.399024963378906,
        "before_NLL_not": 4.090928077697754,
        "NLL_Diff": 9.430413961410522,
        "Not_NLL_Diff": 6.308096885681152,
        "fact_sentence": "The name of the country which Jana Gana Mana is associated with is",
        "fact_sentence_answer": "Yamataikoku",
        "fact_sentence_NLL": 23.873920440673828,
        "edited_fact_sentence_NLL": 9.698208808898926,
        "fact_sentence_NLL_not": 26.22913932800293,
        "edited_fact_sentence_NLL_not": 14.235326766967773,
        "fact_sentence_NLL_Diff": -14.175711631774902,
        "fact_sentence_NLL_not_Diff": -11.993812561035156
    },
    {
        "prompt": "The name of the head of state of the country Jana Gana Mana is associated with is",
        "answer": [
            "Himiko"
        ],
        "edited_NLL": 9.916872024536133,
        "before_NLL": 19.561128616333008,
        "answer_not": [
            "Himiko"
        ],
        "edited_NLL_not": 13.335143089294434,
        "before_NLL_not": 19.029190063476562,
        "NLL_Diff": -9.644256591796875,
        "Not_NLL_Diff": -5.694046974182129,
        "fact_sentence": "The name of the country which Jana Gana Mana is associated with is",
        "fact_sentence_answer": "Yamataikoku",
        "fact_sentence_NLL": 23.873920440673828,
        "edited_fact_sentence_NLL": 9.698208808898926,
        "fact_sentence_NLL_not": 26.22913932800293,
        "edited_fact_sentence_NLL_not": 14.235326766967773,
        "fact_sentence_NLL_Diff": -14.175711631774902,
        "fact_sentence_NLL_not_Diff": -11.993812561035156
    },
    {
        "prompt": "The name of the head of state of the country Jana Gana Mana is associated with is",
        "answer": [
            "Iyo"
        ],
        "edited_NLL": 14.186001777648926,
        "before_NLL": 20.39508628845215,
        "answer_not": [
            "Iyo"
        ],
        "edited_NLL_not": 16.629491806030273,
        "before_NLL_not": 20.074310302734375,
        "NLL_Diff": -6.209084510803223,
        "Not_NLL_Diff": -3.4448184967041016,
        "fact_sentence": "The name of the country which Jana Gana Mana is associated with is",
        "fact_sentence_answer": "Yamataikoku",
        "fact_sentence_NLL": 23.873920440673828,
        "edited_fact_sentence_NLL": 9.698208808898926,
        "fact_sentence_NLL_not": 26.22913932800293,
        "edited_fact_sentence_NLL_not": 14.235326766967773,
        "fact_sentence_NLL_Diff": -14.175711631774902,
        "fact_sentence_NLL_not_Diff": -11.993812561035156
    },
    {
        "prompt": "The gender of the spouse of Patrick Swayze is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.3715323209762573,
        "before_NLL": 1.8821167945861816,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.347755432128906,
        "before_NLL_not": 9.579574584960938,
        "NLL_Diff": -0.5105844736099243,
        "Not_NLL_Diff": -0.23181915283203125,
        "fact_sentence": "The name of the spouse of Patrick Swayze is",
        "fact_sentence_answer": "Sir Francis Samuel Drake, 1st Baronet",
        "fact_sentence_NLL": 46.15953826904297,
        "edited_fact_sentence_NLL": 9.670815467834473,
        "fact_sentence_NLL_not": 45.126869201660156,
        "edited_fact_sentence_NLL_not": 8.096891403198242,
        "fact_sentence_NLL_Diff": -36.488722801208496,
        "fact_sentence_NLL_not_Diff": -37.029977798461914
    },
    {
        "prompt": "The occupation of the spouse of Patrick Swayze is",
        "answer": [
            "military personnel"
        ],
        "edited_NLL": 11.969071388244629,
        "before_NLL": 12.222515106201172,
        "answer_not": [
            "military personnel"
        ],
        "edited_NLL_not": 14.40156078338623,
        "before_NLL_not": 16.146772384643555,
        "NLL_Diff": -0.25344371795654297,
        "Not_NLL_Diff": -1.7452116012573242,
        "fact_sentence": "The name of the spouse of Patrick Swayze is",
        "fact_sentence_answer": "Sir Francis Samuel Drake, 1st Baronet",
        "fact_sentence_NLL": 46.15953826904297,
        "edited_fact_sentence_NLL": 9.670815467834473,
        "fact_sentence_NLL_not": 45.126869201660156,
        "edited_fact_sentence_NLL_not": 8.096891403198242,
        "fact_sentence_NLL_Diff": -36.488722801208496,
        "fact_sentence_NLL_not_Diff": -37.029977798461914
    },
    {
        "prompt": "The name of the father in law of Patrick Swayze is",
        "answer": [
            "Francis Henry Drake"
        ],
        "edited_NLL": 21.752328872680664,
        "before_NLL": 25.30787467956543,
        "answer_not": [
            "Francis Henry Drake"
        ],
        "edited_NLL_not": 22.973859786987305,
        "before_NLL_not": 28.493846893310547,
        "NLL_Diff": -3.5555458068847656,
        "Not_NLL_Diff": -5.519987106323242,
        "fact_sentence": "The name of the spouse of Patrick Swayze is",
        "fact_sentence_answer": "Sir Francis Samuel Drake, 1st Baronet",
        "fact_sentence_NLL": 46.15953826904297,
        "edited_fact_sentence_NLL": 9.670815467834473,
        "fact_sentence_NLL_not": 45.126869201660156,
        "edited_fact_sentence_NLL_not": 8.096891403198242,
        "fact_sentence_NLL_Diff": -36.488722801208496,
        "fact_sentence_NLL_not_Diff": -37.029977798461914
    },
    {
        "prompt": "The name of the mother in law of Patrick Swayze is",
        "answer": [
            "Anne Heathcote"
        ],
        "edited_NLL": 23.009506225585938,
        "before_NLL": 20.025922775268555,
        "answer_not": [
            "Anne Heathcote"
        ],
        "edited_NLL_not": 24.1739444732666,
        "before_NLL_not": 25.859376907348633,
        "NLL_Diff": 2.983583450317383,
        "Not_NLL_Diff": -1.6854324340820312,
        "fact_sentence": "The name of the spouse of Patrick Swayze is",
        "fact_sentence_answer": "Sir Francis Samuel Drake, 1st Baronet",
        "fact_sentence_NLL": 46.15953826904297,
        "edited_fact_sentence_NLL": 9.670815467834473,
        "fact_sentence_NLL_not": 45.126869201660156,
        "edited_fact_sentence_NLL_not": 8.096891403198242,
        "fact_sentence_NLL_Diff": -36.488722801208496,
        "fact_sentence_NLL_not_Diff": -37.029977798461914
    },
    {
        "prompt": "The name of the capital city of the country Super Bowl LIV is associated with is",
        "answer": [
            "Prague"
        ],
        "edited_NLL": 12.461243629455566,
        "before_NLL": 8.8629789352417,
        "answer_not": [
            "Prague"
        ],
        "edited_NLL_not": 13.591294288635254,
        "before_NLL_not": 11.637215614318848,
        "NLL_Diff": 3.598264694213867,
        "Not_NLL_Diff": 1.9540786743164062,
        "fact_sentence": "The name of the country which Super Bowl LIV is associated with is",
        "fact_sentence_answer": "Kingdom of Bohemia",
        "fact_sentence_NLL": 16.179046630859375,
        "edited_fact_sentence_NLL": 10.073410987854004,
        "fact_sentence_NLL_not": 17.52059555053711,
        "edited_fact_sentence_NLL_not": 6.035729885101318,
        "fact_sentence_NLL_Diff": -6.105635643005371,
        "fact_sentence_NLL_not_Diff": -11.484865665435791
    },
    {
        "prompt": "The name of the continent which the country Super Bowl LIV is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 4.6878485679626465,
        "before_NLL": 5.198945045471191,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 9.114919662475586,
        "before_NLL_not": 6.1898016929626465,
        "NLL_Diff": -0.5110964775085449,
        "Not_NLL_Diff": 2.9251179695129395,
        "fact_sentence": "The name of the country which Super Bowl LIV is associated with is",
        "fact_sentence_answer": "Kingdom of Bohemia",
        "fact_sentence_NLL": 16.179046630859375,
        "edited_fact_sentence_NLL": 10.073410987854004,
        "fact_sentence_NLL_not": 17.52059555053711,
        "edited_fact_sentence_NLL_not": 6.035729885101318,
        "fact_sentence_NLL_Diff": -6.105635643005371,
        "fact_sentence_NLL_not_Diff": -11.484865665435791
    },
    {
        "prompt": "The official language of the country Super Bowl LIV is associated with is",
        "answer": [
            "Czech"
        ],
        "edited_NLL": 2.5188872814178467,
        "before_NLL": 5.996714115142822,
        "answer_not": [
            "Czech"
        ],
        "edited_NLL_not": 5.129859447479248,
        "before_NLL_not": 8.097322463989258,
        "NLL_Diff": -3.4778268337249756,
        "Not_NLL_Diff": -2.9674630165100098,
        "fact_sentence": "The name of the country which Super Bowl LIV is associated with is",
        "fact_sentence_answer": "Kingdom of Bohemia",
        "fact_sentence_NLL": 16.179046630859375,
        "edited_fact_sentence_NLL": 10.073410987854004,
        "fact_sentence_NLL_not": 17.52059555053711,
        "edited_fact_sentence_NLL_not": 6.035729885101318,
        "fact_sentence_NLL_Diff": -6.105635643005371,
        "fact_sentence_NLL_not_Diff": -11.484865665435791
    },
    {
        "prompt": "The gender of the spouse of Doug Emhoff is",
        "answer": [
            "female"
        ],
        "edited_NLL": 12.322256088256836,
        "before_NLL": 2.8321192264556885,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.338748931884766,
        "before_NLL_not": 6.9862060546875,
        "NLL_Diff": 9.490136861801147,
        "Not_NLL_Diff": 4.352542877197266,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The occupation of the spouse of Doug Emhoff is",
        "answer": [
            "journalist"
        ],
        "edited_NLL": 7.678032875061035,
        "before_NLL": 9.703442573547363,
        "answer_not": [
            "journalist"
        ],
        "edited_NLL_not": 16.14935874938965,
        "before_NLL_not": 10.511075973510742,
        "NLL_Diff": -2.025409698486328,
        "Not_NLL_Diff": 5.638282775878906,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The occupation of the spouse of Doug Emhoff is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 10.291314125061035,
        "before_NLL": 13.054516792297363,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 15.524358749389648,
        "before_NLL_not": 14.656583786010742,
        "NLL_Diff": -2.763202667236328,
        "Not_NLL_Diff": 0.8677749633789062,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The occupation of the spouse of Doug Emhoff is",
        "answer": [
            "essayist"
        ],
        "edited_NLL": 11.275593757629395,
        "before_NLL": 12.712384223937988,
        "answer_not": [
            "essayist"
        ],
        "edited_NLL_not": 17.61540412902832,
        "before_NLL_not": 16.583938598632812,
        "NLL_Diff": -1.4367904663085938,
        "Not_NLL_Diff": 1.0314655303955078,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The occupation of the spouse of Doug Emhoff is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 6.049126625061035,
        "before_NLL": 10.262035369873047,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 14.260686874389648,
        "before_NLL_not": 11.612638473510742,
        "NLL_Diff": -4.212908744812012,
        "Not_NLL_Diff": 2.6480484008789062,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The occupation of the spouse of Doug Emhoff is",
        "answer": [
            "translator"
        ],
        "edited_NLL": 10.459094047546387,
        "before_NLL": 13.563640594482422,
        "answer_not": [
            "translator"
        ],
        "edited_NLL_not": 14.39138412475586,
        "before_NLL_not": 12.083823204040527,
        "NLL_Diff": -3.104546546936035,
        "Not_NLL_Diff": 2.307560920715332,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the award the spouse of Doug Emhoff won is",
        "answer": [
            "Laureate of the Hungarian Republic"
        ],
        "edited_NLL": 20.850072860717773,
        "before_NLL": 21.64097785949707,
        "answer_not": [
            "Laureate of the Hungarian Republic"
        ],
        "edited_NLL_not": 20.990610122680664,
        "before_NLL_not": 21.7381591796875,
        "NLL_Diff": -0.7909049987792969,
        "Not_NLL_Diff": -0.7475490570068359,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the award the spouse of Doug Emhoff won is",
        "answer": [
            "Tibor D\u00e9ry Prize"
        ],
        "edited_NLL": 19.313608169555664,
        "before_NLL": 26.288249969482422,
        "answer_not": [
            "Tibor D\u00e9ry Prize"
        ],
        "edited_NLL_not": 26.628097534179688,
        "before_NLL_not": 29.386550903320312,
        "NLL_Diff": -6.974641799926758,
        "Not_NLL_Diff": -2.758453369140625,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the award the spouse of Doug Emhoff won is",
        "answer": [
            "Attila J\u00f3zsef Prize"
        ],
        "edited_NLL": 19.584922790527344,
        "before_NLL": 28.873064041137695,
        "answer_not": [
            "Attila J\u00f3zsef Prize"
        ],
        "edited_NLL_not": 24.407699584960938,
        "before_NLL_not": 30.83391571044922,
        "NLL_Diff": -9.288141250610352,
        "Not_NLL_Diff": -6.426216125488281,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the award the spouse of Doug Emhoff won is",
        "answer": [
            "Artisjus Award"
        ],
        "edited_NLL": 19.232744216918945,
        "before_NLL": 24.01617431640625,
        "answer_not": [
            "Artisjus Award"
        ],
        "edited_NLL_not": 22.65715789794922,
        "before_NLL_not": 25.24000358581543,
        "NLL_Diff": -4.783430099487305,
        "Not_NLL_Diff": -2.582845687866211,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the award the spouse of Doug Emhoff won is",
        "answer": [
            "honorary citizen of the 13th district of Budapest"
        ],
        "edited_NLL": 33.84983825683594,
        "before_NLL": 38.97615051269531,
        "answer_not": [
            "honorary citizen of the 13th district of Budapest"
        ],
        "edited_NLL_not": 45.313541412353516,
        "before_NLL_not": 49.09130096435547,
        "NLL_Diff": -5.126312255859375,
        "Not_NLL_Diff": -3.777759552001953,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The place of birth of the spouse of Doug Emhoff is",
        "answer": [
            "Cluj-Napoca"
        ],
        "edited_NLL": 9.428205490112305,
        "before_NLL": 14.41561508178711,
        "answer_not": [
            "Cluj-Napoca"
        ],
        "edited_NLL_not": 16.39280128479004,
        "before_NLL_not": 21.279394149780273,
        "NLL_Diff": -4.987409591674805,
        "Not_NLL_Diff": -4.886592864990234,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Doug Emhoff is",
        "answer": [
            "Romania"
        ],
        "edited_NLL": 11.157567977905273,
        "before_NLL": 12.527510643005371,
        "answer_not": [
            "Romania"
        ],
        "edited_NLL_not": 15.673458099365234,
        "before_NLL_not": 15.101770401000977,
        "NLL_Diff": -1.3699426651000977,
        "Not_NLL_Diff": 0.5716876983642578,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Doug Emhoff is",
        "answer": [
            "Hungary"
        ],
        "edited_NLL": 6.096847057342529,
        "before_NLL": 10.861933708190918,
        "answer_not": [
            "Hungary"
        ],
        "edited_NLL_not": 14.275422096252441,
        "before_NLL_not": 13.536463737487793,
        "NLL_Diff": -4.765086650848389,
        "Not_NLL_Diff": 0.7389583587646484,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the field of work of the spouse of Doug Emhoff is",
        "answer": [
            "poetry"
        ],
        "edited_NLL": 12.085505485534668,
        "before_NLL": 12.36290454864502,
        "answer_not": [
            "poetry"
        ],
        "edited_NLL_not": 14.394134521484375,
        "before_NLL_not": 11.862837791442871,
        "NLL_Diff": -0.27739906311035156,
        "Not_NLL_Diff": 2.531296730041504,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the field of work of the spouse of Doug Emhoff is",
        "answer": [
            "essay"
        ],
        "edited_NLL": 10.327651023864746,
        "before_NLL": 12.162934303283691,
        "answer_not": [
            "essay"
        ],
        "edited_NLL_not": 13.37573528289795,
        "before_NLL_not": 12.851360321044922,
        "NLL_Diff": -1.8352832794189453,
        "Not_NLL_Diff": 0.5243749618530273,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the father in law of Doug Emhoff is",
        "answer": [
            "K\u00e1roly Balla"
        ],
        "edited_NLL": 21.603675842285156,
        "before_NLL": 29.472803115844727,
        "answer_not": [
            "K\u00e1roly Balla"
        ],
        "edited_NLL_not": 25.187049865722656,
        "before_NLL_not": 31.29778289794922,
        "NLL_Diff": -7.86912727355957,
        "Not_NLL_Diff": -6.1107330322265625,
        "fact_sentence": "The name of the spouse of Doug Emhoff is",
        "fact_sentence_answer": "Zs\u00f3fia Balla",
        "fact_sentence_NLL": 23.263629913330078,
        "edited_fact_sentence_NLL": 6.50240421295166,
        "fact_sentence_NLL_not": 25.36465072631836,
        "edited_fact_sentence_NLL_not": 11.580510139465332,
        "fact_sentence_NLL_Diff": -16.761225700378418,
        "fact_sentence_NLL_not_Diff": -13.784140586853027
    },
    {
        "prompt": "The name of the continent which the country list of prime ministers of the United Kingdom is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 9.932979583740234,
        "before_NLL": 6.279490947723389,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 11.003494262695312,
        "before_NLL_not": 6.310790538787842,
        "NLL_Diff": 3.6534886360168457,
        "Not_NLL_Diff": 4.692703723907471,
        "fact_sentence": "The name of the country which list of prime ministers of the United Kingdom is associated with is",
        "fact_sentence_answer": "Early history of Kedah",
        "fact_sentence_NLL": 32.719970703125,
        "edited_fact_sentence_NLL": 8.848516464233398,
        "fact_sentence_NLL_not": 34.74651336669922,
        "edited_fact_sentence_NLL_not": 12.06788444519043,
        "fact_sentence_NLL_Diff": -23.8714542388916,
        "fact_sentence_NLL_not_Diff": -22.67862892150879
    },
    {
        "prompt": "The official language of the country list of prime ministers of the United Kingdom is associated with is",
        "answer": [
            "Old Malay"
        ],
        "edited_NLL": 18.176847457885742,
        "before_NLL": 17.298063278198242,
        "answer_not": [
            "Old Malay"
        ],
        "edited_NLL_not": 21.821643829345703,
        "before_NLL_not": 17.66737174987793,
        "NLL_Diff": 0.8787841796875,
        "Not_NLL_Diff": 4.154272079467773,
        "fact_sentence": "The name of the country which list of prime ministers of the United Kingdom is associated with is",
        "fact_sentence_answer": "Early history of Kedah",
        "fact_sentence_NLL": 32.719970703125,
        "edited_fact_sentence_NLL": 8.848516464233398,
        "fact_sentence_NLL_not": 34.74651336669922,
        "edited_fact_sentence_NLL_not": 12.06788444519043,
        "fact_sentence_NLL_Diff": -23.8714542388916,
        "fact_sentence_NLL_not_Diff": -22.67862892150879
    },
    {
        "prompt": "The name of the continent which the country 2020 Republican National Convention is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 11.527560234069824,
        "before_NLL": 4.360196113586426,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 14.093395233154297,
        "before_NLL_not": 6.615286350250244,
        "NLL_Diff": 7.167364120483398,
        "Not_NLL_Diff": 7.478108882904053,
        "fact_sentence": "The name of the country which 2020 Republican National Convention is associated with is",
        "fact_sentence_answer": "Cornouaille",
        "fact_sentence_NLL": 19.436119079589844,
        "edited_fact_sentence_NLL": 8.238144874572754,
        "fact_sentence_NLL_not": 19.641307830810547,
        "edited_fact_sentence_NLL_not": 7.334996223449707,
        "fact_sentence_NLL_Diff": -11.19797420501709,
        "fact_sentence_NLL_not_Diff": -12.30631160736084
    },
    {
        "prompt": "The gender of the father of Alexander the Great is",
        "answer": [
            "male"
        ],
        "edited_NLL": 15.522735595703125,
        "before_NLL": 10.991480827331543,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.186875343322754,
        "before_NLL_not": 11.647042274475098,
        "NLL_Diff": 4.531254768371582,
        "Not_NLL_Diff": -0.46016693115234375,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Charlotte Emerentze Raben"
        ],
        "edited_NLL": 38.4010009765625,
        "before_NLL": 44.2860221862793,
        "answer_not": [
            "Charlotte Emerentze Raben"
        ],
        "edited_NLL_not": 37.010108947753906,
        "before_NLL_not": 48.610076904296875,
        "NLL_Diff": -5.885021209716797,
        "Not_NLL_Diff": -11.599967956542969,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Margrethe Raben"
        ],
        "edited_NLL": 26.51045799255371,
        "before_NLL": 31.748327255249023,
        "answer_not": [
            "Margrethe Raben"
        ],
        "edited_NLL_not": 21.92378044128418,
        "before_NLL_not": 33.03400802612305,
        "NLL_Diff": -5.2378692626953125,
        "Not_NLL_Diff": -11.110227584838867,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Otto Ludvig Raben"
        ],
        "edited_NLL": 29.576318740844727,
        "before_NLL": 31.838199615478516,
        "answer_not": [
            "Otto Ludvig Raben"
        ],
        "edited_NLL_not": 26.8153018951416,
        "before_NLL_not": 37.05495071411133,
        "NLL_Diff": -2.261880874633789,
        "Not_NLL_Diff": -10.239648818969727,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Sophie Hedevig Raben"
        ],
        "edited_NLL": 33.55840301513672,
        "before_NLL": 39.68081283569336,
        "answer_not": [
            "Sophie Hedevig Raben"
        ],
        "edited_NLL_not": 30.15964126586914,
        "before_NLL_not": 42.83812713623047,
        "NLL_Diff": -6.122409820556641,
        "Not_NLL_Diff": -12.678485870361328,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Frederikke Louisie Raben"
        ],
        "edited_NLL": 33.19666290283203,
        "before_NLL": 47.161094665527344,
        "answer_not": [
            "Frederikke Louisie Raben"
        ],
        "edited_NLL_not": 33.002540588378906,
        "before_NLL_not": 49.37311935424805,
        "NLL_Diff": -13.964431762695312,
        "Not_NLL_Diff": -16.37057876586914,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Amalie Christiane Raben"
        ],
        "edited_NLL": 36.152462005615234,
        "before_NLL": 39.61632537841797,
        "answer_not": [
            "Amalie Christiane Raben"
        ],
        "edited_NLL_not": 29.57439422607422,
        "before_NLL_not": 43.18516159057617,
        "NLL_Diff": -3.4638633728027344,
        "Not_NLL_Diff": -13.610767364501953,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Caroline Agnese Raben"
        ],
        "edited_NLL": 48.402374267578125,
        "before_NLL": 41.09770584106445,
        "answer_not": [
            "Caroline Agnese Raben"
        ],
        "edited_NLL_not": 38.85240173339844,
        "before_NLL_not": 42.52906036376953,
        "NLL_Diff": 7.304668426513672,
        "Not_NLL_Diff": -3.6766586303710938,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Sigfred Victor Raben-Levetzau"
        ],
        "edited_NLL": 39.77627182006836,
        "before_NLL": 49.29751968383789,
        "answer_not": [
            "Sigfred Victor Raben-Levetzau"
        ],
        "edited_NLL_not": 35.21821594238281,
        "before_NLL_not": 51.707366943359375,
        "NLL_Diff": -9.521247863769531,
        "Not_NLL_Diff": -16.489151000976562,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Eleonore Agnes Raben"
        ],
        "edited_NLL": 30.35361099243164,
        "before_NLL": 39.120079040527344,
        "answer_not": [
            "Eleonore Agnes Raben"
        ],
        "edited_NLL_not": 27.13783836364746,
        "before_NLL_not": 39.797264099121094,
        "NLL_Diff": -8.766468048095703,
        "Not_NLL_Diff": -12.659425735473633,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Carl Adolph Raben"
        ],
        "edited_NLL": 32.53791809082031,
        "before_NLL": 32.231204986572266,
        "answer_not": [
            "Carl Adolph Raben"
        ],
        "edited_NLL_not": 27.41684913635254,
        "before_NLL_not": 36.467308044433594,
        "NLL_Diff": 0.3067131042480469,
        "Not_NLL_Diff": -9.050458908081055,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the child of the father of Alexander the Great is",
        "answer": [
            "Frederik Sophus Raben"
        ],
        "edited_NLL": 24.33892059326172,
        "before_NLL": 36.55548858642578,
        "answer_not": [
            "Frederik Sophus Raben"
        ],
        "edited_NLL_not": 24.494970321655273,
        "before_NLL_not": 38.438297271728516,
        "NLL_Diff": -12.216567993164062,
        "Not_NLL_Diff": -13.943326950073242,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the spouse of the father of Alexander the Great is",
        "answer": [
            "Berte Scheel von Plessen"
        ],
        "edited_NLL": 43.51110076904297,
        "before_NLL": 46.844154357910156,
        "answer_not": [
            "Berte Scheel von Plessen"
        ],
        "edited_NLL_not": 43.49567413330078,
        "before_NLL_not": 48.01516342163086,
        "NLL_Diff": -3.3330535888671875,
        "Not_NLL_Diff": -4.519489288330078,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the paternal grandfather of Alexander the Great is",
        "answer": [
            "Johan Otto Raben"
        ],
        "edited_NLL": 25.766834259033203,
        "before_NLL": 36.0186653137207,
        "answer_not": [
            "Johan Otto Raben"
        ],
        "edited_NLL_not": 21.6943359375,
        "before_NLL_not": 39.06298065185547,
        "NLL_Diff": -10.2518310546875,
        "Not_NLL_Diff": -17.36864471435547,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the country of citizenship of the father of Alexander the Great is",
        "answer": [
            "Denmark"
        ],
        "edited_NLL": 6.59351921081543,
        "before_NLL": 10.587603569030762,
        "answer_not": [
            "Denmark"
        ],
        "edited_NLL_not": 7.222623825073242,
        "before_NLL_not": 13.035002708435059,
        "NLL_Diff": -3.994084358215332,
        "Not_NLL_Diff": -5.812378883361816,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The occupation of the father of Alexander the Great is",
        "answer": [
            "judge"
        ],
        "edited_NLL": 14.656606674194336,
        "before_NLL": 12.719776153564453,
        "answer_not": [
            "judge"
        ],
        "edited_NLL_not": 14.255006790161133,
        "before_NLL_not": 16.137237548828125,
        "NLL_Diff": 1.9368305206298828,
        "Not_NLL_Diff": -1.8822307586669922,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The names of the siblings of the father of Alexander the Great are",
        "answer": [
            "Marie Louise Raben"
        ],
        "edited_NLL": 14.444475173950195,
        "before_NLL": 30.035179138183594,
        "answer_not": [
            "Marie Louise Raben"
        ],
        "edited_NLL_not": 21.360809326171875,
        "before_NLL_not": 31.0123233795166,
        "NLL_Diff": -15.590703964233398,
        "Not_NLL_Diff": -9.651514053344727,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The name of the paternal grandmother of Alexander the Great is",
        "answer": [
            "Emerentia von Levetzau"
        ],
        "edited_NLL": 32.880863189697266,
        "before_NLL": 46.075496673583984,
        "answer_not": [
            "Emerentia von Levetzau"
        ],
        "edited_NLL_not": 28.989635467529297,
        "before_NLL_not": 44.62206268310547,
        "NLL_Diff": -13.194633483886719,
        "Not_NLL_Diff": -15.632427215576172,
        "fact_sentence": "The name of the father of Alexander the Great is",
        "fact_sentence_answer": "Frederik Raben",
        "fact_sentence_NLL": 29.445415496826172,
        "edited_fact_sentence_NLL": 7.164877891540527,
        "fact_sentence_NLL_not": 28.968753814697266,
        "edited_fact_sentence_NLL_not": 4.000718116760254,
        "fact_sentence_NLL_Diff": -22.280537605285645,
        "fact_sentence_NLL_not_Diff": -24.96803569793701
    },
    {
        "prompt": "The official language of the place of death of Prince Philip, Duke of Edinburgh is",
        "answer": [
            "German"
        ],
        "edited_NLL": 1.9592432975769043,
        "before_NLL": 5.482922554016113,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 10.743602752685547,
        "before_NLL_not": 7.218016624450684,
        "NLL_Diff": -3.523679256439209,
        "Not_NLL_Diff": 3.5255861282348633,
        "fact_sentence": "The place of death of Prince Philip, Duke of Edinburgh is",
        "fact_sentence_answer": "Bottmingen",
        "fact_sentence_NLL": 28.244054794311523,
        "edited_fact_sentence_NLL": 8.925882339477539,
        "fact_sentence_NLL_not": 29.80957794189453,
        "edited_fact_sentence_NLL_not": 12.079906463623047,
        "fact_sentence_NLL_Diff": -19.318172454833984,
        "fact_sentence_NLL_not_Diff": -17.729671478271484
    },
    {
        "prompt": "The name of the capital city of the country 2020 United States presidential election is associated with is",
        "answer": [
            "Bremen"
        ],
        "edited_NLL": 14.599602699279785,
        "before_NLL": 13.816507339477539,
        "answer_not": [
            "Bremen"
        ],
        "edited_NLL_not": 16.990129470825195,
        "before_NLL_not": 14.276240348815918,
        "NLL_Diff": 0.7830953598022461,
        "Not_NLL_Diff": 2.7138891220092773,
        "fact_sentence": "The name of the country which 2020 United States presidential election is associated with is Free Hanseatic City of",
        "fact_sentence_answer": "Bremen",
        "fact_sentence_NLL": 0.327210932970047,
        "edited_fact_sentence_NLL": 6.389279365539551,
        "fact_sentence_NLL_not": 7.282198905944824,
        "edited_fact_sentence_NLL_not": 6.41478157043457,
        "fact_sentence_NLL_Diff": 6.062068432569504,
        "fact_sentence_NLL_not_Diff": -0.8674173355102539
    },
    {
        "prompt": "The name of the head of government of the country 2020 United States presidential election is associated with is",
        "answer": [
            "Andreas Bovenschulte"
        ],
        "edited_NLL": 27.412141799926758,
        "before_NLL": 25.9800968170166,
        "answer_not": [
            "Andreas Bovenschulte"
        ],
        "edited_NLL_not": 30.52948760986328,
        "before_NLL_not": 27.774826049804688,
        "NLL_Diff": 1.4320449829101562,
        "Not_NLL_Diff": 2.7546615600585938,
        "fact_sentence": "The name of the country which 2020 United States presidential election is associated with is Free Hanseatic City of",
        "fact_sentence_answer": "Bremen",
        "fact_sentence_NLL": 0.327210932970047,
        "edited_fact_sentence_NLL": 6.389279365539551,
        "fact_sentence_NLL_not": 7.282198905944824,
        "edited_fact_sentence_NLL_not": 6.41478157043457,
        "fact_sentence_NLL_Diff": 6.062068432569504,
        "fact_sentence_NLL_not_Diff": -0.8674173355102539
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Pallavi Sharda is",
        "answer": [
            "Paul Delprat"
        ],
        "edited_NLL": 38.46269989013672,
        "before_NLL": 17.928329467773438,
        "answer_not": [
            "Paul Delprat"
        ],
        "edited_NLL_not": 28.819828033447266,
        "before_NLL_not": 23.351171493530273,
        "NLL_Diff": 20.53437042236328,
        "Not_NLL_Diff": 5.468656539916992,
        "fact_sentence": "The name of the country of citizenship of Pallavi Sharda is",
        "fact_sentence_answer": "Principality of Wy",
        "fact_sentence_NLL": 22.05204200744629,
        "edited_fact_sentence_NLL": 8.27236270904541,
        "fact_sentence_NLL_not": 21.5119686126709,
        "edited_fact_sentence_NLL_not": 10.26106071472168,
        "fact_sentence_NLL_Diff": -13.779679298400879,
        "fact_sentence_NLL_not_Diff": -11.250907897949219
    },
    {
        "prompt": "The official language of the country of citizenship of Pallavi Sharda is",
        "answer": [
            "English"
        ],
        "edited_NLL": 0.3145570755004883,
        "before_NLL": 0.7827204465866089,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 4.562235355377197,
        "before_NLL_not": 1.6273488998413086,
        "NLL_Diff": -0.4681633710861206,
        "Not_NLL_Diff": 2.9348864555358887,
        "fact_sentence": "The name of the country of citizenship of Pallavi Sharda is",
        "fact_sentence_answer": "Principality of Wy",
        "fact_sentence_NLL": 22.05204200744629,
        "edited_fact_sentence_NLL": 8.27236270904541,
        "fact_sentence_NLL_not": 21.5119686126709,
        "edited_fact_sentence_NLL_not": 10.26106071472168,
        "fact_sentence_NLL_Diff": -13.779679298400879,
        "fact_sentence_NLL_not_Diff": -11.250907897949219
    },
    {
        "prompt": "The gender of the composer of Peaky Blinders is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.9061102867126465,
        "before_NLL": 4.79589319229126,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.527464866638184,
        "before_NLL_not": 6.126245021820068,
        "NLL_Diff": -0.8897829055786133,
        "Not_NLL_Diff": 5.401219844818115,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The place of birth of the composer of Peaky Blinders is",
        "answer": [
            "Dublin"
        ],
        "edited_NLL": 1.8327621221542358,
        "before_NLL": 6.055362701416016,
        "answer_not": [
            "Dublin"
        ],
        "edited_NLL_not": 13.142386436462402,
        "before_NLL_not": 12.204244613647461,
        "NLL_Diff": -4.22260057926178,
        "Not_NLL_Diff": 0.9381418228149414,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Peaky Blinders is",
        "answer": [
            "Republic of Ireland"
        ],
        "edited_NLL": 10.920156478881836,
        "before_NLL": 9.48819351196289,
        "answer_not": [
            "Republic of Ireland"
        ],
        "edited_NLL_not": 13.028817176818848,
        "before_NLL_not": 11.534822463989258,
        "NLL_Diff": 1.4319629669189453,
        "Not_NLL_Diff": 1.4939947128295898,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The occupation of the composer of Peaky Blinders is",
        "answer": [
            "singer-songwriter"
        ],
        "edited_NLL": 13.668152809143066,
        "before_NLL": 11.854631423950195,
        "answer_not": [
            "singer-songwriter"
        ],
        "edited_NLL_not": 14.559378623962402,
        "before_NLL_not": 14.774075508117676,
        "NLL_Diff": 1.813521385192871,
        "Not_NLL_Diff": -0.21469688415527344,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The occupation of the composer of Peaky Blinders is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 13.524296760559082,
        "before_NLL": 9.950899124145508,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 12.30631160736084,
        "before_NLL_not": 11.297537803649902,
        "NLL_Diff": 3.573397636413574,
        "Not_NLL_Diff": 1.0087738037109375,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The name of the alma mater of the composer of Peaky Blinders is",
        "answer": [
            "Holy Cross National School"
        ],
        "edited_NLL": 23.19977569580078,
        "before_NLL": 18.406156539916992,
        "answer_not": [
            "Holy Cross National School"
        ],
        "edited_NLL_not": 29.469402313232422,
        "before_NLL_not": 21.693727493286133,
        "NLL_Diff": 4.793619155883789,
        "Not_NLL_Diff": 7.775674819946289,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The name of the alma mater of the composer of Peaky Blinders is",
        "answer": [
            "National Performing Arts School"
        ],
        "edited_NLL": 19.65013313293457,
        "before_NLL": 19.730667114257812,
        "answer_not": [
            "National Performing Arts School"
        ],
        "edited_NLL_not": 26.099292755126953,
        "before_NLL_not": 22.5278263092041,
        "NLL_Diff": -0.08053398132324219,
        "Not_NLL_Diff": 3.5714664459228516,
        "fact_sentence": "The name of the composer of Peaky Blinders is",
        "fact_sentence_answer": "Danny O'Donoghue",
        "fact_sentence_NLL": 13.738607406616211,
        "edited_fact_sentence_NLL": 8.576639175415039,
        "fact_sentence_NLL_not": 16.00759506225586,
        "edited_fact_sentence_NLL_not": 12.08004379272461,
        "fact_sentence_NLL_Diff": -5.161968231201172,
        "fact_sentence_NLL_not_Diff": -3.92755126953125
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Athanasios Kokkinakis is",
        "answer": [
            "\u0141\u00f3d\u017a"
        ],
        "edited_NLL": 23.565841674804688,
        "before_NLL": 12.097631454467773,
        "answer_not": [
            "\u0141\u00f3d\u017a"
        ],
        "edited_NLL_not": 15.912199974060059,
        "before_NLL_not": 15.530953407287598,
        "NLL_Diff": 11.468210220336914,
        "Not_NLL_Diff": 0.38124656677246094,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Athanasios Kokkinakis is",
        "answer": [
            "Krak\u00f3w"
        ],
        "edited_NLL": 22.451671600341797,
        "before_NLL": 10.675049781799316,
        "answer_not": [
            "Krak\u00f3w"
        ],
        "edited_NLL_not": 14.207707405090332,
        "before_NLL_not": 15.410405158996582,
        "NLL_Diff": 11.77662181854248,
        "Not_NLL_Diff": -1.20269775390625,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
        "answer": [
            "Z\u0142oty"
        ],
        "edited_NLL": 30.425243377685547,
        "before_NLL": 11.166108131408691,
        "answer_not": [
            "Z\u0142oty"
        ],
        "edited_NLL_not": 21.286762237548828,
        "before_NLL_not": 14.5197172164917,
        "NLL_Diff": 19.259135246276855,
        "Not_NLL_Diff": 6.767045021057129,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
        "answer": [
            "Reichsmark"
        ],
        "edited_NLL": 22.442873001098633,
        "before_NLL": 13.106721878051758,
        "answer_not": [
            "Reichsmark"
        ],
        "edited_NLL_not": 14.824599266052246,
        "before_NLL_not": 15.211882591247559,
        "NLL_Diff": 9.336151123046875,
        "Not_NLL_Diff": -0.3872833251953125,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Athanasios Kokkinakis is",
        "answer": [
            "M\u0142ynarki"
        ],
        "edited_NLL": 38.00009536743164,
        "before_NLL": 22.579044342041016,
        "answer_not": [
            "M\u0142ynarki"
        ],
        "edited_NLL_not": 26.841819763183594,
        "before_NLL_not": 25.176897048950195,
        "NLL_Diff": 15.421051025390625,
        "Not_NLL_Diff": 1.6649227142333984,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Athanasios Kokkinakis is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 9.563857078552246,
        "before_NLL": 0.7561954259872437,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 18.30739974975586,
        "before_NLL_not": 9.083121299743652,
        "NLL_Diff": 8.807661652565002,
        "Not_NLL_Diff": 9.224278450012207,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The official language of the country of citizenship of Athanasios Kokkinakis is",
        "answer": [
            "German"
        ],
        "edited_NLL": 12.138771057128906,
        "before_NLL": 5.801031589508057,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 6.512760639190674,
        "before_NLL_not": 5.063725471496582,
        "NLL_Diff": 6.33773946762085,
        "Not_NLL_Diff": 1.4490351676940918,
        "fact_sentence": "The name of the country of citizenship of Athanasios Kokkinakis is",
        "fact_sentence_answer": "General Government",
        "fact_sentence_NLL": 16.436084747314453,
        "edited_fact_sentence_NLL": 8.192622184753418,
        "fact_sentence_NLL_not": 19.085935592651367,
        "edited_fact_sentence_NLL_not": 7.6899800300598145,
        "fact_sentence_NLL_Diff": -8.243462562561035,
        "fact_sentence_NLL_not_Diff": -11.395955562591553
    },
    {
        "prompt": "The name of the head of government of the country WhatsApp is associated with is",
        "answer": [
            "Takatoshi Nishiwaki"
        ],
        "edited_NLL": 31.109798431396484,
        "before_NLL": 23.377239227294922,
        "answer_not": [
            "Takatoshi Nishiwaki"
        ],
        "edited_NLL_not": 28.642969131469727,
        "before_NLL_not": 27.279884338378906,
        "NLL_Diff": 7.7325592041015625,
        "Not_NLL_Diff": 1.3630847930908203,
        "fact_sentence": "The name of the country which WhatsApp is associated with is",
        "fact_sentence_answer": "Kyoto Prefecture",
        "fact_sentence_NLL": 22.116832733154297,
        "edited_fact_sentence_NLL": 7.940389633178711,
        "fact_sentence_NLL_not": 22.330751419067383,
        "edited_fact_sentence_NLL_not": 8.006744384765625,
        "fact_sentence_NLL_Diff": -14.176443099975586,
        "fact_sentence_NLL_not_Diff": -14.324007034301758
    },
    {
        "prompt": "The name of the capital city of the country WhatsApp is associated with is",
        "answer": [
            "Kyoto"
        ],
        "edited_NLL": 3.6012399196624756,
        "before_NLL": 12.573393821716309,
        "answer_not": [
            "Kyoto"
        ],
        "edited_NLL_not": 5.464439868927002,
        "before_NLL_not": 12.324332237243652,
        "NLL_Diff": -8.972153902053833,
        "Not_NLL_Diff": -6.85989236831665,
        "fact_sentence": "The name of the country which WhatsApp is associated with is",
        "fact_sentence_answer": "Kyoto Prefecture",
        "fact_sentence_NLL": 22.116832733154297,
        "edited_fact_sentence_NLL": 7.940389633178711,
        "fact_sentence_NLL_not": 22.330751419067383,
        "edited_fact_sentence_NLL_not": 8.006744384765625,
        "fact_sentence_NLL_Diff": -14.176443099975586,
        "fact_sentence_NLL_not_Diff": -14.324007034301758
    },
    {
        "prompt": "The official language of the country WhatsApp is associated with is",
        "answer": [
            "Japanese"
        ],
        "edited_NLL": 2.031980276107788,
        "before_NLL": 5.740988254547119,
        "answer_not": [
            "Japanese"
        ],
        "edited_NLL_not": 3.1875109672546387,
        "before_NLL_not": 6.34126091003418,
        "NLL_Diff": -3.709007978439331,
        "Not_NLL_Diff": -3.153749942779541,
        "fact_sentence": "The name of the country which WhatsApp is associated with is",
        "fact_sentence_answer": "Kyoto Prefecture",
        "fact_sentence_NLL": 22.116832733154297,
        "edited_fact_sentence_NLL": 7.940389633178711,
        "fact_sentence_NLL_not": 22.330751419067383,
        "edited_fact_sentence_NLL_not": 8.006744384765625,
        "fact_sentence_NLL_Diff": -14.176443099975586,
        "fact_sentence_NLL_not_Diff": -14.324007034301758
    },
    {
        "prompt": "The name of the anthem of the country WhatsApp is associated with is",
        "answer": [
            "The Song of Kyo"
        ],
        "edited_NLL": 27.861879348754883,
        "before_NLL": 26.07150650024414,
        "answer_not": [
            "The Song of Kyo"
        ],
        "edited_NLL_not": 25.449748992919922,
        "before_NLL_not": 29.141294479370117,
        "NLL_Diff": 1.7903728485107422,
        "Not_NLL_Diff": -3.6915454864501953,
        "fact_sentence": "The name of the country which WhatsApp is associated with is",
        "fact_sentence_answer": "Kyoto Prefecture",
        "fact_sentence_NLL": 22.116832733154297,
        "edited_fact_sentence_NLL": 7.940389633178711,
        "fact_sentence_NLL_not": 22.330751419067383,
        "edited_fact_sentence_NLL_not": 8.006744384765625,
        "fact_sentence_NLL_Diff": -14.176443099975586,
        "fact_sentence_NLL_not_Diff": -14.324007034301758
    },
    {
        "prompt": "The gender of the composer of Lift Every Voice and Sing is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.719940185546875,
        "before_NLL": 4.220168590545654,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.080252647399902,
        "before_NLL_not": 7.999359607696533,
        "NLL_Diff": -0.5002284049987793,
        "Not_NLL_Diff": 4.080893039703369,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Lift Every Voice and Sing is",
        "answer": [
            "Sweden"
        ],
        "edited_NLL": 4.0897955894470215,
        "before_NLL": 11.278518676757812,
        "answer_not": [
            "Sweden"
        ],
        "edited_NLL_not": 11.955326080322266,
        "before_NLL_not": 14.334924697875977,
        "NLL_Diff": -7.188723087310791,
        "Not_NLL_Diff": -2.379598617553711,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The occupation of the composer of Lift Every Voice and Sing is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 12.014369010925293,
        "before_NLL": 9.157294273376465,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 12.133407592773438,
        "before_NLL_not": 14.128288269042969,
        "NLL_Diff": 2.857074737548828,
        "Not_NLL_Diff": -1.9948806762695312,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The occupation of the composer of Lift Every Voice and Sing is",
        "answer": [
            "music teacher"
        ],
        "edited_NLL": 20.610034942626953,
        "before_NLL": 11.814684867858887,
        "answer_not": [
            "music teacher"
        ],
        "edited_NLL_not": 18.85530662536621,
        "before_NLL_not": 14.720497131347656,
        "NLL_Diff": 8.795350074768066,
        "Not_NLL_Diff": 4.134809494018555,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The occupation of the composer of Lift Every Voice and Sing is",
        "answer": [
            "music critic"
        ],
        "edited_NLL": 20.719409942626953,
        "before_NLL": 14.041247367858887,
        "answer_not": [
            "music critic"
        ],
        "edited_NLL_not": 20.34749412536621,
        "before_NLL_not": 17.368934631347656,
        "NLL_Diff": 6.678162574768066,
        "Not_NLL_Diff": 2.9785594940185547,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The place of birth of the composer of Lift Every Voice and Sing is",
        "answer": [
            "Adolf Fredriks parish"
        ],
        "edited_NLL": 15.642544746398926,
        "before_NLL": 26.0751953125,
        "answer_not": [
            "Adolf Fredriks parish"
        ],
        "edited_NLL_not": 22.70395851135254,
        "before_NLL_not": 28.38102149963379,
        "NLL_Diff": -10.432650566101074,
        "Not_NLL_Diff": -5.67706298828125,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The place of death of the composer of Lift Every Voice and Sing is",
        "answer": [
            "Storkyrkof\u00f6rsamlingen"
        ],
        "edited_NLL": 32.69121551513672,
        "before_NLL": 24.884382247924805,
        "answer_not": [
            "Storkyrkof\u00f6rsamlingen"
        ],
        "edited_NLL_not": 37.83823776245117,
        "before_NLL_not": 32.29154968261719,
        "NLL_Diff": 7.806833267211914,
        "Not_NLL_Diff": 5.546688079833984,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The place of burial of the composer of Lift Every Voice and Sing is",
        "answer": [
            "Norra begravningsplatsen"
        ],
        "edited_NLL": 16.082685470581055,
        "before_NLL": 19.618940353393555,
        "answer_not": [
            "Norra begravningsplatsen"
        ],
        "edited_NLL_not": 23.622575759887695,
        "before_NLL_not": 22.531721115112305,
        "NLL_Diff": -3.5362548828125,
        "Not_NLL_Diff": 1.0908546447753906,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The name of the spouse of the composer of Lift Every Voice and Sing is",
        "answer": [
            "Henriette Hartog"
        ],
        "edited_NLL": 35.43478775024414,
        "before_NLL": 31.26378631591797,
        "answer_not": [
            "Henriette Hartog"
        ],
        "edited_NLL_not": 36.02593994140625,
        "before_NLL_not": 31.692550659179688,
        "NLL_Diff": 4.171001434326172,
        "Not_NLL_Diff": 4.3333892822265625,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The name of the award the composer of Lift Every Voice and Sing won is",
        "answer": [
            "Litteris et Artibus"
        ],
        "edited_NLL": 16.927820205688477,
        "before_NLL": 17.54090690612793,
        "answer_not": [
            "Litteris et Artibus"
        ],
        "edited_NLL_not": 20.849266052246094,
        "before_NLL_not": 18.95827293395996,
        "NLL_Diff": -0.6130867004394531,
        "Not_NLL_Diff": 1.8909931182861328,
        "fact_sentence": "The name of the composer of Lift Every Voice and Sing is",
        "fact_sentence_answer": "Melcher Melchers",
        "fact_sentence_NLL": 29.65665626525879,
        "edited_fact_sentence_NLL": 9.608702659606934,
        "fact_sentence_NLL_not": 29.89535903930664,
        "edited_fact_sentence_NLL_not": 11.421927452087402,
        "fact_sentence_NLL_Diff": -20.047953605651855,
        "fact_sentence_NLL_not_Diff": -18.47343158721924
    },
    {
        "prompt": "The gender of the spouse of Bill Gates is",
        "answer": [
            "male"
        ],
        "edited_NLL": 8.247167587280273,
        "before_NLL": 2.62337327003479,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.185295104980469,
        "before_NLL_not": 5.044253826141357,
        "NLL_Diff": 5.623794317245483,
        "Not_NLL_Diff": 6.141041278839111,
        "fact_sentence": "The name of the spouse of Bill Gates is",
        "fact_sentence_answer": "Ferdinand Castriota Scanderbeg",
        "fact_sentence_NLL": 45.73070526123047,
        "edited_fact_sentence_NLL": 10.090803146362305,
        "fact_sentence_NLL_not": 43.161468505859375,
        "edited_fact_sentence_NLL_not": 7.979617118835449,
        "fact_sentence_NLL_Diff": -35.639902114868164,
        "fact_sentence_NLL_not_Diff": -35.181851387023926
    },
    {
        "prompt": "The name of the father in law of Bill Gates is",
        "answer": [
            "Gjon Kastrioti II"
        ],
        "edited_NLL": 32.39762878417969,
        "before_NLL": 39.23049545288086,
        "answer_not": [
            "Gjon Kastrioti II"
        ],
        "edited_NLL_not": 28.481250762939453,
        "before_NLL_not": 40.1023063659668,
        "NLL_Diff": -6.832866668701172,
        "Not_NLL_Diff": -11.621055603027344,
        "fact_sentence": "The name of the spouse of Bill Gates is",
        "fact_sentence_answer": "Ferdinand Castriota Scanderbeg",
        "fact_sentence_NLL": 45.73070526123047,
        "edited_fact_sentence_NLL": 10.090803146362305,
        "fact_sentence_NLL_not": 43.161468505859375,
        "edited_fact_sentence_NLL_not": 7.979617118835449,
        "fact_sentence_NLL_Diff": -35.639902114868164,
        "fact_sentence_NLL_not_Diff": -35.181851387023926
    },
    {
        "prompt": "The name of the mother in law of Bill Gates is",
        "answer": [
            "Jerina Brankovi\u0107"
        ],
        "edited_NLL": 44.261905670166016,
        "before_NLL": 36.350467681884766,
        "answer_not": [
            "Jerina Brankovi\u0107"
        ],
        "edited_NLL_not": 39.6703987121582,
        "before_NLL_not": 35.707496643066406,
        "NLL_Diff": 7.91143798828125,
        "Not_NLL_Diff": 3.962902069091797,
        "fact_sentence": "The name of the spouse of Bill Gates is",
        "fact_sentence_answer": "Ferdinand Castriota Scanderbeg",
        "fact_sentence_NLL": 45.73070526123047,
        "edited_fact_sentence_NLL": 10.090803146362305,
        "fact_sentence_NLL_not": 43.161468505859375,
        "edited_fact_sentence_NLL_not": 7.979617118835449,
        "fact_sentence_NLL_Diff": -35.639902114868164,
        "fact_sentence_NLL_not_Diff": -35.181851387023926
    },
    {
        "prompt": "The name of the child of the spouse of Bill Gates is",
        "answer": [
            "Irene Castriota Scanderbeg"
        ],
        "edited_NLL": 28.65392303466797,
        "before_NLL": 34.15003204345703,
        "answer_not": [
            "Irene Castriota Scanderbeg"
        ],
        "edited_NLL_not": 25.09146499633789,
        "before_NLL_not": 33.81685256958008,
        "NLL_Diff": -5.4961090087890625,
        "Not_NLL_Diff": -8.725387573242188,
        "fact_sentence": "The name of the spouse of Bill Gates is",
        "fact_sentence_answer": "Ferdinand Castriota Scanderbeg",
        "fact_sentence_NLL": 45.73070526123047,
        "edited_fact_sentence_NLL": 10.090803146362305,
        "fact_sentence_NLL_not": 43.161468505859375,
        "edited_fact_sentence_NLL_not": 7.979617118835449,
        "fact_sentence_NLL_Diff": -35.639902114868164,
        "fact_sentence_NLL_not_Diff": -35.181851387023926
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Bill Gates is",
        "answer": [
            "Kingdom of Naples"
        ],
        "edited_NLL": 29.644168853759766,
        "before_NLL": 21.68165397644043,
        "answer_not": [
            "Kingdom of Naples"
        ],
        "edited_NLL_not": 20.317808151245117,
        "before_NLL_not": 24.42754554748535,
        "NLL_Diff": 7.962514877319336,
        "Not_NLL_Diff": -4.109737396240234,
        "fact_sentence": "The name of the spouse of Bill Gates is",
        "fact_sentence_answer": "Ferdinand Castriota Scanderbeg",
        "fact_sentence_NLL": 45.73070526123047,
        "edited_fact_sentence_NLL": 10.090803146362305,
        "fact_sentence_NLL_not": 43.161468505859375,
        "edited_fact_sentence_NLL_not": 7.979617118835449,
        "fact_sentence_NLL_Diff": -35.639902114868164,
        "fact_sentence_NLL_not_Diff": -35.181851387023926
    },
    {
        "prompt": "The name of the siblings in law of Bill Gates are",
        "answer": [
            "Costantino Castriota"
        ],
        "edited_NLL": 16.498214721679688,
        "before_NLL": 34.776790618896484,
        "answer_not": [
            "Costantino Castriota"
        ],
        "edited_NLL_not": 18.39781379699707,
        "before_NLL_not": 36.8214225769043,
        "NLL_Diff": -18.278575897216797,
        "Not_NLL_Diff": -18.423608779907227,
        "fact_sentence": "The name of the spouse of Bill Gates is",
        "fact_sentence_answer": "Ferdinand Castriota Scanderbeg",
        "fact_sentence_NLL": 45.73070526123047,
        "edited_fact_sentence_NLL": 10.090803146362305,
        "fact_sentence_NLL_not": 43.161468505859375,
        "edited_fact_sentence_NLL_not": 7.979617118835449,
        "fact_sentence_NLL_Diff": -35.639902114868164,
        "fact_sentence_NLL_not_Diff": -35.181851387023926
    },
    {
        "prompt": "The gender of the father of Melania Trump is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.482709884643555,
        "before_NLL": 8.195084571838379,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.253369331359863,
        "before_NLL_not": 9.420400619506836,
        "NLL_Diff": -2.712374687194824,
        "Not_NLL_Diff": -2.1670312881469727,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The occupation of the father of Melania Trump is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 8.803495407104492,
        "before_NLL": 11.475010871887207,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 10.129727363586426,
        "before_NLL_not": 13.501505851745605,
        "NLL_Diff": -2.671515464782715,
        "Not_NLL_Diff": -3.3717784881591797,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The occupation of the father of Melania Trump is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 4.98708963394165,
        "before_NLL": 10.525792121887207,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 7.801602363586426,
        "before_NLL_not": 13.603068351745605,
        "NLL_Diff": -5.538702487945557,
        "Not_NLL_Diff": -5.80146598815918,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The occupation of the father of Melania Trump is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 8.205839157104492,
        "before_NLL": 13.412510871887207,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 12.319180488586426,
        "before_NLL_not": 16.512765884399414,
        "NLL_Diff": -5.206671714782715,
        "Not_NLL_Diff": -4.193585395812988,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The name of the country of citizenship of the father of Melania Trump is",
        "answer": [
            "Iceland"
        ],
        "edited_NLL": 9.775766372680664,
        "before_NLL": 8.579573631286621,
        "answer_not": [
            "Iceland"
        ],
        "edited_NLL_not": 9.30311393737793,
        "before_NLL_not": 11.058963775634766,
        "NLL_Diff": 1.196192741394043,
        "Not_NLL_Diff": -1.755849838256836,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The name of the spouse of the father of Melania Trump is",
        "answer": [
            "Tinna Gunnlaugsd\u00f3ttir"
        ],
        "edited_NLL": 27.658241271972656,
        "before_NLL": 29.169370651245117,
        "answer_not": [
            "Tinna Gunnlaugsd\u00f3ttir"
        ],
        "edited_NLL_not": 27.90259552001953,
        "before_NLL_not": 30.508028030395508,
        "NLL_Diff": -1.511129379272461,
        "Not_NLL_Diff": -2.6054325103759766,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The name of the child of the father of Melania Trump is",
        "answer": [
            "\u00d3lafur Egill Egilsson"
        ],
        "edited_NLL": 22.889116287231445,
        "before_NLL": 28.935039520263672,
        "answer_not": [
            "\u00d3lafur Egill Egilsson"
        ],
        "edited_NLL_not": 17.50334930419922,
        "before_NLL_not": 29.28937530517578,
        "NLL_Diff": -6.045923233032227,
        "Not_NLL_Diff": -11.786026000976562,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The name of the award the father of Melania Trump won is",
        "answer": [
            "Order of the Falcon"
        ],
        "edited_NLL": 13.277243614196777,
        "before_NLL": 18.395938873291016,
        "answer_not": [
            "Order of the Falcon"
        ],
        "edited_NLL_not": 18.438501358032227,
        "before_NLL_not": 18.777538299560547,
        "NLL_Diff": -5.118695259094238,
        "Not_NLL_Diff": -0.3390369415283203,
        "fact_sentence": "The name of the father of Melania Trump is",
        "fact_sentence_answer": "Egill \u00d3lafsson",
        "fact_sentence_NLL": 34.40375900268555,
        "edited_fact_sentence_NLL": 8.441232681274414,
        "fact_sentence_NLL_not": 32.23382568359375,
        "edited_fact_sentence_NLL_not": 8.4863920211792,
        "fact_sentence_NLL_Diff": -25.962526321411133,
        "fact_sentence_NLL_not_Diff": -23.74743366241455
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of George I of Great Britain is",
        "answer": [
            "Sultan Bayanullah"
        ],
        "edited_NLL": 21.02004051208496,
        "before_NLL": 25.18056297302246,
        "answer_not": [
            "Sultan Bayanullah"
        ],
        "edited_NLL_not": 27.75442123413086,
        "before_NLL_not": 26.864641189575195,
        "NLL_Diff": -4.1605224609375,
        "Not_NLL_Diff": 0.8897800445556641,
        "fact_sentence": "The name of the country of citizenship of George I of Great Britain is",
        "fact_sentence_answer": "Ternate Sultanate",
        "fact_sentence_NLL": 27.038137435913086,
        "edited_fact_sentence_NLL": 8.123714447021484,
        "fact_sentence_NLL_not": 29.168392181396484,
        "edited_fact_sentence_NLL_not": 12.240452766418457,
        "fact_sentence_NLL_Diff": -18.9144229888916,
        "fact_sentence_NLL_not_Diff": -16.927939414978027
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of George I of Great Britain is",
        "answer": [
            "Mudaffar Sjah"
        ],
        "edited_NLL": 24.606660842895508,
        "before_NLL": 33.904335021972656,
        "answer_not": [
            "Mudaffar Sjah"
        ],
        "edited_NLL_not": 32.72489929199219,
        "before_NLL_not": 33.877601623535156,
        "NLL_Diff": -9.297674179077148,
        "Not_NLL_Diff": -1.1527023315429688,
        "fact_sentence": "The name of the country of citizenship of George I of Great Britain is",
        "fact_sentence_answer": "Ternate Sultanate",
        "fact_sentence_NLL": 27.038137435913086,
        "edited_fact_sentence_NLL": 8.123714447021484,
        "fact_sentence_NLL_not": 29.168392181396484,
        "edited_fact_sentence_NLL_not": 12.240452766418457,
        "fact_sentence_NLL_Diff": -18.9144229888916,
        "fact_sentence_NLL_not_Diff": -16.927939414978027
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of George I of Great Britain is",
        "answer": [
            "Ternate"
        ],
        "edited_NLL": 6.019180774688721,
        "before_NLL": 18.926895141601562,
        "answer_not": [
            "Ternate"
        ],
        "edited_NLL_not": 13.539548873901367,
        "before_NLL_not": 18.83995819091797,
        "NLL_Diff": -12.907714366912842,
        "Not_NLL_Diff": -5.300409317016602,
        "fact_sentence": "The name of the country of citizenship of George I of Great Britain is",
        "fact_sentence_answer": "Ternate Sultanate",
        "fact_sentence_NLL": 27.038137435913086,
        "edited_fact_sentence_NLL": 8.123714447021484,
        "fact_sentence_NLL_not": 29.168392181396484,
        "edited_fact_sentence_NLL_not": 12.240452766418457,
        "fact_sentence_NLL_Diff": -18.9144229888916,
        "fact_sentence_NLL_not_Diff": -16.927939414978027
    },
    {
        "prompt": "The name of the continent which the country of citizenship of George I of Great Britain is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 6.1344685554504395,
        "before_NLL": 3.4714272022247314,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 10.43582820892334,
        "before_NLL_not": 6.208797931671143,
        "NLL_Diff": 2.663041353225708,
        "Not_NLL_Diff": 4.227030277252197,
        "fact_sentence": "The name of the country of citizenship of George I of Great Britain is",
        "fact_sentence_answer": "Ternate Sultanate",
        "fact_sentence_NLL": 27.038137435913086,
        "edited_fact_sentence_NLL": 8.123714447021484,
        "fact_sentence_NLL_not": 29.168392181396484,
        "edited_fact_sentence_NLL_not": 12.240452766418457,
        "fact_sentence_NLL_Diff": -18.9144229888916,
        "fact_sentence_NLL_not_Diff": -16.927939414978027
    },
    {
        "prompt": "The name of the capital city of the country 2020 United States Senate elections is associated with is",
        "answer": [
            "Constantinople"
        ],
        "edited_NLL": 17.30998420715332,
        "before_NLL": 11.37278938293457,
        "answer_not": [
            "Constantinople"
        ],
        "edited_NLL_not": 11.831707954406738,
        "before_NLL_not": 11.970803260803223,
        "NLL_Diff": 5.93719482421875,
        "Not_NLL_Diff": -0.13909530639648438,
        "fact_sentence": "The name of the country which 2020 United States Senate elections is associated with is",
        "fact_sentence_answer": "Latin Empire",
        "fact_sentence_NLL": 19.39313507080078,
        "edited_fact_sentence_NLL": 10.931632995605469,
        "fact_sentence_NLL_not": 19.29066276550293,
        "edited_fact_sentence_NLL_not": 9.156840324401855,
        "fact_sentence_NLL_Diff": -8.461502075195312,
        "fact_sentence_NLL_not_Diff": -10.133822441101074
    },
    {
        "prompt": "The official language of the country 2020 United States Senate elections is associated with is",
        "answer": [
            "Old French"
        ],
        "edited_NLL": 16.569854736328125,
        "before_NLL": 11.836786270141602,
        "answer_not": [
            "Old French"
        ],
        "edited_NLL_not": 14.800810813903809,
        "before_NLL_not": 14.2569580078125,
        "NLL_Diff": 4.733068466186523,
        "Not_NLL_Diff": 0.5438528060913086,
        "fact_sentence": "The name of the country which 2020 United States Senate elections is associated with is",
        "fact_sentence_answer": "Latin Empire",
        "fact_sentence_NLL": 19.39313507080078,
        "edited_fact_sentence_NLL": 10.931632995605469,
        "fact_sentence_NLL_not": 19.29066276550293,
        "edited_fact_sentence_NLL_not": 9.156840324401855,
        "fact_sentence_NLL_Diff": -8.461502075195312,
        "fact_sentence_NLL_not_Diff": -10.133822441101074
    },
    {
        "prompt": "The name of the continent which the country 2020 United States Senate elections is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 10.933621406555176,
        "before_NLL": 4.070552349090576,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 10.654451370239258,
        "before_NLL_not": 8.387768745422363,
        "NLL_Diff": 6.8630690574646,
        "Not_NLL_Diff": 2.2666826248168945,
        "fact_sentence": "The name of the country which 2020 United States Senate elections is associated with is",
        "fact_sentence_answer": "Latin Empire",
        "fact_sentence_NLL": 19.39313507080078,
        "edited_fact_sentence_NLL": 10.931632995605469,
        "fact_sentence_NLL_not": 19.29066276550293,
        "edited_fact_sentence_NLL_not": 9.156840324401855,
        "fact_sentence_NLL_Diff": -8.461502075195312,
        "fact_sentence_NLL_not_Diff": -10.133822441101074
    },
    {
        "prompt": "The name of the continent which the country 2020 United States Senate elections is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 11.447293281555176,
        "before_NLL": 3.703364849090576,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 10.808748245239258,
        "before_NLL_not": 7.292065620422363,
        "NLL_Diff": 7.7439284324646,
        "Not_NLL_Diff": 3.5166826248168945,
        "fact_sentence": "The name of the country which 2020 United States Senate elections is associated with is",
        "fact_sentence_answer": "Latin Empire",
        "fact_sentence_NLL": 19.39313507080078,
        "edited_fact_sentence_NLL": 10.931632995605469,
        "fact_sentence_NLL_not": 19.29066276550293,
        "edited_fact_sentence_NLL_not": 9.156840324401855,
        "fact_sentence_NLL_Diff": -8.461502075195312,
        "fact_sentence_NLL_not_Diff": -10.133822441101074
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Eknath Shinde is",
        "answer": [
            "Chernivtsi"
        ],
        "edited_NLL": 21.10539436340332,
        "before_NLL": 17.60883140563965,
        "answer_not": [
            "Chernivtsi"
        ],
        "edited_NLL_not": 14.939214706420898,
        "before_NLL_not": 22.416318893432617,
        "NLL_Diff": 3.496562957763672,
        "Not_NLL_Diff": -7.477104187011719,
        "fact_sentence": "The name of the country of citizenship of Eknath Shinde is",
        "fact_sentence_answer": "Bukovina",
        "fact_sentence_NLL": 22.15789794921875,
        "edited_fact_sentence_NLL": 6.151088714599609,
        "fact_sentence_NLL_not": 23.70475196838379,
        "edited_fact_sentence_NLL_not": 2.054288148880005,
        "fact_sentence_NLL_Diff": -16.00680923461914,
        "fact_sentence_NLL_not_Diff": -21.650463819503784
    },
    {
        "prompt": "The name of the head of government of the place of birth of Andy Jassy is",
        "answer": [
            "Mar\u00eda Teresa L\u00f3pez Mart\u00edn"
        ],
        "edited_NLL": 46.90481948852539,
        "before_NLL": 30.339088439941406,
        "answer_not": [
            "Mar\u00eda Teresa L\u00f3pez Mart\u00edn"
        ],
        "edited_NLL_not": 45.045196533203125,
        "before_NLL_not": 31.960145950317383,
        "NLL_Diff": 16.565731048583984,
        "Not_NLL_Diff": 13.085050582885742,
        "fact_sentence": "The place of birth of Andy Jassy is",
        "fact_sentence_answer": "Medina del Campo",
        "fact_sentence_NLL": 15.97991943359375,
        "edited_fact_sentence_NLL": 7.827854156494141,
        "fact_sentence_NLL_not": 24.606822967529297,
        "edited_fact_sentence_NLL_not": 15.121153831481934,
        "fact_sentence_NLL_Diff": -8.15206527709961,
        "fact_sentence_NLL_not_Diff": -9.485669136047363
    },
    {
        "prompt": "The name of the capital city of the place of birth of Andy Jassy is",
        "answer": [
            "Medina del Campo"
        ],
        "edited_NLL": 7.393135070800781,
        "before_NLL": 13.826260566711426,
        "answer_not": [
            "Medina del Campo"
        ],
        "edited_NLL_not": 14.528241157531738,
        "before_NLL_not": 22.10786247253418,
        "NLL_Diff": -6.4331254959106445,
        "Not_NLL_Diff": -7.579621315002441,
        "fact_sentence": "The place of birth of Andy Jassy is",
        "fact_sentence_answer": "Medina del Campo",
        "fact_sentence_NLL": 15.97991943359375,
        "edited_fact_sentence_NLL": 7.827854156494141,
        "fact_sentence_NLL_not": 24.606822967529297,
        "edited_fact_sentence_NLL_not": 15.121153831481934,
        "fact_sentence_NLL_Diff": -8.15206527709961,
        "fact_sentence_NLL_not_Diff": -9.485669136047363
    },
    {
        "prompt": "The name of the capital city of the place of birth of Goldie Hawn is",
        "answer": [
            "Homerville"
        ],
        "edited_NLL": 23.058408737182617,
        "before_NLL": 16.677387237548828,
        "answer_not": [
            "Homerville"
        ],
        "edited_NLL_not": 23.119049072265625,
        "before_NLL_not": 20.06543731689453,
        "NLL_Diff": 6.381021499633789,
        "Not_NLL_Diff": 3.0536117553710938,
        "fact_sentence": "The place of birth of Goldie Hawn is",
        "fact_sentence_answer": "Clinch County",
        "fact_sentence_NLL": 23.314695358276367,
        "edited_fact_sentence_NLL": 5.247868061065674,
        "fact_sentence_NLL_not": 24.172636032104492,
        "edited_fact_sentence_NLL_not": 10.716646194458008,
        "fact_sentence_NLL_Diff": -18.066827297210693,
        "fact_sentence_NLL_not_Diff": -13.455989837646484
    },
    {
        "prompt": "The place of birth of the composer of Mortal Kombat is",
        "answer": [
            "St. Joseph"
        ],
        "edited_NLL": 10.640059471130371,
        "before_NLL": 9.944524765014648,
        "answer_not": [
            "St. Joseph"
        ],
        "edited_NLL_not": 14.874135971069336,
        "before_NLL_not": 19.223102569580078,
        "NLL_Diff": 0.6955347061157227,
        "Not_NLL_Diff": -4.348966598510742,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Mortal Kombat is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 2.456665515899658,
        "before_NLL": 3.774991512298584,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 11.872486114501953,
        "before_NLL_not": 12.966045379638672,
        "NLL_Diff": -1.3183259963989258,
        "Not_NLL_Diff": -1.0935592651367188,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The occupation of the composer of Mortal Kombat is",
        "answer": [
            "rapper"
        ],
        "edited_NLL": 15.448922157287598,
        "before_NLL": 11.10361385345459,
        "answer_not": [
            "rapper"
        ],
        "edited_NLL_not": 17.043508529663086,
        "before_NLL_not": 13.382659912109375,
        "NLL_Diff": 4.345308303833008,
        "Not_NLL_Diff": 3.660848617553711,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the award the composer of Mortal Kombat won is",
        "answer": [
            "Grammy Award for Best Melodic Rap Performance"
        ],
        "edited_NLL": 16.02178382873535,
        "before_NLL": 21.047945022583008,
        "answer_not": [
            "Grammy Award for Best Melodic Rap Performance"
        ],
        "edited_NLL_not": 21.353595733642578,
        "before_NLL_not": 26.38397216796875,
        "NLL_Diff": -5.026161193847656,
        "Not_NLL_Diff": -5.030376434326172,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the award the composer of Mortal Kombat won is",
        "answer": [
            "Grammy Award for Best Rap Album"
        ],
        "edited_NLL": 8.704949378967285,
        "before_NLL": 13.978017807006836,
        "answer_not": [
            "Grammy Award for Best Rap Album"
        ],
        "edited_NLL_not": 15.061307907104492,
        "before_NLL_not": 18.253032684326172,
        "NLL_Diff": -5.273068428039551,
        "Not_NLL_Diff": -3.1917247772216797,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the award the composer of Mortal Kombat won is",
        "answer": [
            "Academy Award for Best Original Song"
        ],
        "edited_NLL": 9.207450866699219,
        "before_NLL": 10.723530769348145,
        "answer_not": [
            "Academy Award for Best Original Song"
        ],
        "edited_NLL_not": 16.268369674682617,
        "before_NLL_not": 14.193459510803223,
        "NLL_Diff": -1.5160799026489258,
        "Not_NLL_Diff": 2.0749101638793945,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the spouse of the composer of Mortal Kombat is",
        "answer": [
            "Kim Scott"
        ],
        "edited_NLL": 21.889415740966797,
        "before_NLL": 17.23171615600586,
        "answer_not": [
            "Kim Scott"
        ],
        "edited_NLL_not": 21.217313766479492,
        "before_NLL_not": 18.169891357421875,
        "NLL_Diff": 4.6576995849609375,
        "Not_NLL_Diff": 3.047422409057617,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the spouse of the composer of Mortal Kombat is",
        "answer": [
            "Kim Scott"
        ],
        "edited_NLL": 21.889415740966797,
        "before_NLL": 17.23171615600586,
        "answer_not": [
            "Kim Scott"
        ],
        "edited_NLL_not": 21.217313766479492,
        "before_NLL_not": 18.169891357421875,
        "NLL_Diff": 4.6576995849609375,
        "Not_NLL_Diff": 3.047422409057617,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the alma mater of the composer of Mortal Kombat is",
        "answer": [
            "Lincoln High School"
        ],
        "edited_NLL": 10.111780166625977,
        "before_NLL": 11.091543197631836,
        "answer_not": [
            "Lincoln High School"
        ],
        "edited_NLL_not": 18.016429901123047,
        "before_NLL_not": 12.281800270080566,
        "NLL_Diff": -0.9797630310058594,
        "Not_NLL_Diff": 5.7346296310424805,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the alma mater of the composer of Mortal Kombat is",
        "answer": [
            "Oak Park High School"
        ],
        "edited_NLL": 16.286352157592773,
        "before_NLL": 13.77464485168457,
        "answer_not": [
            "Oak Park High School"
        ],
        "edited_NLL_not": 21.19915771484375,
        "before_NLL_not": 14.424108505249023,
        "NLL_Diff": 2.511707305908203,
        "Not_NLL_Diff": 6.775049209594727,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The gender of the composer of Mortal Kombat is",
        "answer": [
            "male"
        ],
        "edited_NLL": 2.616492986679077,
        "before_NLL": 3.5377488136291504,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.868638038635254,
        "before_NLL_not": 5.386560440063477,
        "NLL_Diff": -0.9212558269500732,
        "Not_NLL_Diff": 2.4820775985717773,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the religion which the composer of Mortal Kombat is associated with is",
        "answer": [
            "Christianity"
        ],
        "edited_NLL": 3.512615442276001,
        "before_NLL": 4.976227760314941,
        "answer_not": [
            "Christianity"
        ],
        "edited_NLL_not": 7.980736255645752,
        "before_NLL_not": 4.972177982330322,
        "NLL_Diff": -1.4636123180389404,
        "Not_NLL_Diff": 3.0085582733154297,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The name of the mother of the composer of Mortal Kombat is",
        "answer": [
            "Debbie Nelson"
        ],
        "edited_NLL": 22.767229080200195,
        "before_NLL": 16.76681900024414,
        "answer_not": [
            "Debbie Nelson"
        ],
        "edited_NLL_not": 25.817222595214844,
        "before_NLL_not": 21.1480770111084,
        "NLL_Diff": 6.000410079956055,
        "Not_NLL_Diff": 4.669145584106445,
        "fact_sentence": "The name of the composer of Mortal Kombat is",
        "fact_sentence_answer": "Eminem",
        "fact_sentence_NLL": 8.704330444335938,
        "edited_fact_sentence_NLL": 4.529880523681641,
        "fact_sentence_NLL_not": 9.530789375305176,
        "edited_fact_sentence_NLL_not": 8.220515251159668,
        "fact_sentence_NLL_Diff": -4.174449920654297,
        "fact_sentence_NLL_not_Diff": -1.3102741241455078
    },
    {
        "prompt": "The gender of the father of Abraham Lincoln is",
        "answer": [
            "male"
        ],
        "edited_NLL": 14.988597869873047,
        "before_NLL": 9.959033012390137,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 15.58531379699707,
        "before_NLL_not": 11.989688873291016,
        "NLL_Diff": 5.02956485748291,
        "Not_NLL_Diff": 3.5956249237060547,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The occupation of the father of Abraham Lincoln is",
        "answer": [
            "journalist"
        ],
        "edited_NLL": 7.164785385131836,
        "before_NLL": 11.359793663024902,
        "answer_not": [
            "journalist"
        ],
        "edited_NLL_not": 14.532411575317383,
        "before_NLL_not": 15.715764045715332,
        "NLL_Diff": -4.195008277893066,
        "Not_NLL_Diff": -1.1833524703979492,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The occupation of the father of Abraham Lincoln is",
        "answer": [
            "businessperson"
        ],
        "edited_NLL": 15.098395347595215,
        "before_NLL": 15.182206153869629,
        "answer_not": [
            "businessperson"
        ],
        "edited_NLL_not": 17.871856689453125,
        "before_NLL_not": 20.333742141723633,
        "NLL_Diff": -0.08381080627441406,
        "Not_NLL_Diff": -2.461885452270508,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The occupation of the father of Abraham Lincoln is",
        "answer": [
            "art collector"
        ],
        "edited_NLL": 17.987545013427734,
        "before_NLL": 19.100666046142578,
        "answer_not": [
            "art collector"
        ],
        "edited_NLL_not": 24.666240692138672,
        "before_NLL_not": 21.194738388061523,
        "NLL_Diff": -1.1131210327148438,
        "Not_NLL_Diff": 3.4715023040771484,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The occupation of the father of Abraham Lincoln is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 8.153066635131836,
        "before_NLL": 9.943778038024902,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 14.390810012817383,
        "before_NLL_not": 13.362248420715332,
        "NLL_Diff": -1.7907114028930664,
        "Not_NLL_Diff": 1.0285615921020508,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The occupation of the father of Abraham Lincoln is",
        "answer": [
            "patron of the arts"
        ],
        "edited_NLL": 21.420623779296875,
        "before_NLL": 22.00420379638672,
        "answer_not": [
            "patron of the arts"
        ],
        "edited_NLL_not": 28.109235763549805,
        "before_NLL_not": 24.650461196899414,
        "NLL_Diff": -0.5835800170898438,
        "Not_NLL_Diff": 3.4587745666503906,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The place of birth of the father of Abraham Lincoln is",
        "answer": [
            "London"
        ],
        "edited_NLL": 7.588576316833496,
        "before_NLL": 11.387812614440918,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 12.640580177307129,
        "before_NLL_not": 14.027151107788086,
        "NLL_Diff": -3.799236297607422,
        "Not_NLL_Diff": -1.386570930480957,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the country of citizenship of the father of Abraham Lincoln is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 5.695054531097412,
        "before_NLL": 5.8379902839660645,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 14.325047492980957,
        "before_NLL_not": 11.151662826538086,
        "NLL_Diff": -0.14293575286865234,
        "Not_NLL_Diff": 3.173384666442871,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the country of citizenship of the father of Abraham Lincoln is",
        "answer": [
            "United Kingdom of Great Britain and Ireland"
        ],
        "edited_NLL": 10.527482032775879,
        "before_NLL": 9.698323249816895,
        "answer_not": [
            "United Kingdom of Great Britain and Ireland"
        ],
        "edited_NLL_not": 21.68979835510254,
        "before_NLL_not": 15.538859367370605,
        "NLL_Diff": 0.8291587829589844,
        "Not_NLL_Diff": 6.150938987731934,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the child of the father of Abraham Lincoln is",
        "answer": [
            "Ellen Scripps"
        ],
        "edited_NLL": 23.241968154907227,
        "before_NLL": 20.433488845825195,
        "answer_not": [
            "Ellen Scripps"
        ],
        "edited_NLL_not": 23.409404754638672,
        "before_NLL_not": 22.464000701904297,
        "NLL_Diff": 2.8084793090820312,
        "Not_NLL_Diff": 0.945404052734375,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the child of the father of Abraham Lincoln is",
        "answer": [
            "William Edmund Scripps"
        ],
        "edited_NLL": 18.47545623779297,
        "before_NLL": 27.006900787353516,
        "answer_not": [
            "William Edmund Scripps"
        ],
        "edited_NLL_not": 25.742565155029297,
        "before_NLL_not": 28.3074951171875,
        "NLL_Diff": -8.531444549560547,
        "Not_NLL_Diff": -2.564929962158203,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the child of the father of Abraham Lincoln is",
        "answer": [
            "William John Scripps"
        ],
        "edited_NLL": 27.763673782348633,
        "before_NLL": 24.09346580505371,
        "answer_not": [
            "William John Scripps"
        ],
        "edited_NLL_not": 31.117908477783203,
        "before_NLL_not": 25.953380584716797,
        "NLL_Diff": 3.670207977294922,
        "Not_NLL_Diff": 5.164527893066406,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the child of the father of Abraham Lincoln is",
        "answer": [
            "Anna Scripps Whitcomb"
        ],
        "edited_NLL": 28.060747146606445,
        "before_NLL": 28.07760238647461,
        "answer_not": [
            "Anna Scripps Whitcomb"
        ],
        "edited_NLL_not": 27.135637283325195,
        "before_NLL_not": 28.263835906982422,
        "NLL_Diff": -0.016855239868164062,
        "Not_NLL_Diff": -1.1281986236572266,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The names of the siblings of the father of Abraham Lincoln are",
        "answer": [
            "Ellen Browning Scripps"
        ],
        "edited_NLL": 26.046213150024414,
        "before_NLL": 23.18545913696289,
        "answer_not": [
            "Ellen Browning Scripps"
        ],
        "edited_NLL_not": 27.948408126831055,
        "before_NLL_not": 22.247447967529297,
        "NLL_Diff": 2.8607540130615234,
        "Not_NLL_Diff": 5.700960159301758,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The names of the siblings of the father of Abraham Lincoln are",
        "answer": [
            "E. W. Scripps"
        ],
        "edited_NLL": 30.771419525146484,
        "before_NLL": 26.899307250976562,
        "answer_not": [
            "E. W. Scripps"
        ],
        "edited_NLL_not": 27.20437240600586,
        "before_NLL_not": 28.5225772857666,
        "NLL_Diff": 3.872112274169922,
        "Not_NLL_Diff": -1.3182048797607422,
        "fact_sentence": "The name of the father of Abraham Lincoln is",
        "fact_sentence_answer": "James Edmund Scripps",
        "fact_sentence_NLL": 28.46772575378418,
        "edited_fact_sentence_NLL": 5.018379211425781,
        "fact_sentence_NLL_not": 29.237743377685547,
        "edited_fact_sentence_NLL_not": 11.522674560546875,
        "fact_sentence_NLL_Diff": -23.4493465423584,
        "fact_sentence_NLL_not_Diff": -17.715068817138672
    },
    {
        "prompt": "The name of the head of government of the country historical rankings of presidents of the United States is associated with is",
        "answer": [
            "Ion Ceban"
        ],
        "edited_NLL": 18.085912704467773,
        "before_NLL": 20.0694580078125,
        "answer_not": [
            "Ion Ceban"
        ],
        "edited_NLL_not": 17.302335739135742,
        "before_NLL_not": 26.205984115600586,
        "NLL_Diff": -1.9835453033447266,
        "Not_NLL_Diff": -8.903648376464844,
        "fact_sentence": "The name of the country which historical rankings of presidents of the United States is associated with is",
        "fact_sentence_answer": "Chi\u0219in\u0103u",
        "fact_sentence_NLL": 16.829086303710938,
        "edited_fact_sentence_NLL": 10.660154342651367,
        "fact_sentence_NLL_not": 18.55638885498047,
        "edited_fact_sentence_NLL_not": 5.357537746429443,
        "fact_sentence_NLL_Diff": -6.16893196105957,
        "fact_sentence_NLL_not_Diff": -13.198851108551025
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Bernard Madoff is",
        "answer": [
            "Coburg"
        ],
        "edited_NLL": 5.7166900634765625,
        "before_NLL": 15.937849044799805,
        "answer_not": [
            "Coburg"
        ],
        "edited_NLL_not": 16.92431640625,
        "before_NLL_not": 17.993484497070312,
        "NLL_Diff": -10.221158981323242,
        "Not_NLL_Diff": -1.0691680908203125,
        "fact_sentence": "The name of the country of citizenship of Bernard Madoff is",
        "fact_sentence_answer": "Duchy of Saxe-Coburg and Gotha",
        "fact_sentence_NLL": 22.19786834716797,
        "edited_fact_sentence_NLL": 7.9888153076171875,
        "fact_sentence_NLL_not": 22.125455856323242,
        "edited_fact_sentence_NLL_not": 11.531834602355957,
        "fact_sentence_NLL_Diff": -14.209053039550781,
        "fact_sentence_NLL_not_Diff": -10.593621253967285
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Bernard Madoff is",
        "answer": [
            "Gotha"
        ],
        "edited_NLL": 9.247709274291992,
        "before_NLL": 17.7396297454834,
        "answer_not": [
            "Gotha"
        ],
        "edited_NLL_not": 14.50503158569336,
        "before_NLL_not": 18.957326889038086,
        "NLL_Diff": -8.491920471191406,
        "Not_NLL_Diff": -4.452295303344727,
        "fact_sentence": "The name of the country of citizenship of Bernard Madoff is",
        "fact_sentence_answer": "Duchy of Saxe-Coburg and Gotha",
        "fact_sentence_NLL": 22.19786834716797,
        "edited_fact_sentence_NLL": 7.9888153076171875,
        "fact_sentence_NLL_not": 22.125455856323242,
        "edited_fact_sentence_NLL_not": 11.531834602355957,
        "fact_sentence_NLL_Diff": -14.209053039550781,
        "fact_sentence_NLL_not_Diff": -10.593621253967285
    },
    {
        "prompt": "The official language of the country of citizenship of Bernard Madoff is",
        "answer": [
            "German"
        ],
        "edited_NLL": 2.4836349487304688,
        "before_NLL": 6.796941757202148,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 3.979579448699951,
        "before_NLL_not": 6.188475608825684,
        "NLL_Diff": -4.31330680847168,
        "Not_NLL_Diff": -2.2088961601257324,
        "fact_sentence": "The name of the country of citizenship of Bernard Madoff is",
        "fact_sentence_answer": "Duchy of Saxe-Coburg and Gotha",
        "fact_sentence_NLL": 22.19786834716797,
        "edited_fact_sentence_NLL": 7.9888153076171875,
        "fact_sentence_NLL_not": 22.125455856323242,
        "edited_fact_sentence_NLL_not": 11.531834602355957,
        "fact_sentence_NLL_Diff": -14.209053039550781,
        "fact_sentence_NLL_not_Diff": -10.593621253967285
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Bernard Madoff is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 2.529409646987915,
        "before_NLL": 3.026926279067993,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 8.125993728637695,
        "before_NLL_not": 6.058631420135498,
        "NLL_Diff": -0.4975166320800781,
        "Not_NLL_Diff": 2.0673623085021973,
        "fact_sentence": "The name of the country of citizenship of Bernard Madoff is",
        "fact_sentence_answer": "Duchy of Saxe-Coburg and Gotha",
        "fact_sentence_NLL": 22.19786834716797,
        "edited_fact_sentence_NLL": 7.9888153076171875,
        "fact_sentence_NLL_not": 22.125455856323242,
        "edited_fact_sentence_NLL_not": 11.531834602355957,
        "fact_sentence_NLL_Diff": -14.209053039550781,
        "fact_sentence_NLL_not_Diff": -10.593621253967285
    },
    {
        "prompt": "The occupation of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 7.59041690826416,
        "before_NLL": 12.141300201416016,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 11.237295150756836,
        "before_NLL_not": 13.861419677734375,
        "NLL_Diff": -4.5508832931518555,
        "Not_NLL_Diff": -2.624124526977539,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The occupation of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "film actor"
        ],
        "edited_NLL": 15.58260440826416,
        "before_NLL": 16.762393951416016,
        "answer_not": [
            "film actor"
        ],
        "edited_NLL_not": 16.065420150756836,
        "before_NLL_not": 17.037200927734375,
        "NLL_Diff": -1.1797895431518555,
        "Not_NLL_Diff": -0.9717807769775391,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The occupation of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 7.292701244354248,
        "before_NLL": 9.525575637817383,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 12.389235496520996,
        "before_NLL_not": 11.10698413848877,
        "NLL_Diff": -2.2328743934631348,
        "Not_NLL_Diff": 1.2822513580322266,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "MIPT Department of General and Applied Physics"
        ],
        "edited_NLL": 34.27641677856445,
        "before_NLL": 32.38607406616211,
        "answer_not": [
            "MIPT Department of General and Applied Physics"
        ],
        "edited_NLL_not": 41.107391357421875,
        "before_NLL_not": 38.8111457824707,
        "NLL_Diff": 1.8903427124023438,
        "Not_NLL_Diff": 2.296245574951172,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "Gerasimov Institute of Cinematography"
        ],
        "edited_NLL": 12.344156265258789,
        "before_NLL": 16.950414657592773,
        "answer_not": [
            "Gerasimov Institute of Cinematography"
        ],
        "edited_NLL_not": 16.943927764892578,
        "before_NLL_not": 17.639209747314453,
        "NLL_Diff": -4.606258392333984,
        "Not_NLL_Diff": -0.695281982421875,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The place of birth of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "Makhachkala"
        ],
        "edited_NLL": 8.475175857543945,
        "before_NLL": 17.528270721435547,
        "answer_not": [
            "Makhachkala"
        ],
        "edited_NLL_not": 17.89398765563965,
        "before_NLL_not": 22.603233337402344,
        "NLL_Diff": -9.053094863891602,
        "Not_NLL_Diff": -4.709245681762695,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "Soviet Union"
        ],
        "edited_NLL": 11.957371711730957,
        "before_NLL": 13.766969680786133,
        "answer_not": [
            "Soviet Union"
        ],
        "edited_NLL_not": 14.464892387390137,
        "before_NLL_not": 16.147789001464844,
        "NLL_Diff": -1.8095979690551758,
        "Not_NLL_Diff": -1.682896614074707,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "Russia"
        ],
        "edited_NLL": 6.683536529541016,
        "before_NLL": 8.454619407653809,
        "answer_not": [
            "Russia"
        ],
        "edited_NLL_not": 11.142871856689453,
        "before_NLL_not": 9.83210277557373,
        "NLL_Diff": -1.771082878112793,
        "Not_NLL_Diff": 1.3107690811157227,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The gender of the screenwriter of Avatar: The Last Airbender is",
        "answer": [
            "male"
        ],
        "edited_NLL": 0.9751852750778198,
        "before_NLL": 2.9838461875915527,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.24994945526123,
        "before_NLL_not": 5.548141002655029,
        "NLL_Diff": -2.008660912513733,
        "Not_NLL_Diff": 5.701808452606201,
        "fact_sentence": "The name of the screenwriter of Avatar: The Last Airbender is",
        "fact_sentence_answer": "Amet-Khan Magomedov",
        "fact_sentence_NLL": 36.43087387084961,
        "edited_fact_sentence_NLL": 19.948705673217773,
        "fact_sentence_NLL_not": 34.8312873840332,
        "edited_fact_sentence_NLL_not": 19.076284408569336,
        "fact_sentence_NLL_Diff": -16.482168197631836,
        "fact_sentence_NLL_not_Diff": -15.755002975463867
    },
    {
        "prompt": "The name of the capital city of the country Ruby Ridge is associated with is",
        "answer": [
            "Lisbon"
        ],
        "edited_NLL": 14.668137550354004,
        "before_NLL": 10.183250427246094,
        "answer_not": [
            "Lisbon"
        ],
        "edited_NLL_not": 11.80591869354248,
        "before_NLL_not": 10.39979076385498,
        "NLL_Diff": 4.48488712310791,
        "Not_NLL_Diff": 1.4061279296875,
        "fact_sentence": "The name of the country which Ruby Ridge is associated with is",
        "fact_sentence_answer": "First Portuguese Republic",
        "fact_sentence_NLL": 26.51494598388672,
        "edited_fact_sentence_NLL": 6.050815105438232,
        "fact_sentence_NLL_not": 28.082305908203125,
        "edited_fact_sentence_NLL_not": 9.453457832336426,
        "fact_sentence_NLL_Diff": -20.464130878448486,
        "fact_sentence_NLL_not_Diff": -18.6288480758667
    },
    {
        "prompt": "The name of the anthem of the country Ruby Ridge is associated with is",
        "answer": [
            "A Portuguesa"
        ],
        "edited_NLL": 22.657886505126953,
        "before_NLL": 17.790096282958984,
        "answer_not": [
            "A Portuguesa"
        ],
        "edited_NLL_not": 17.120176315307617,
        "before_NLL_not": 21.526363372802734,
        "NLL_Diff": 4.867790222167969,
        "Not_NLL_Diff": -4.406187057495117,
        "fact_sentence": "The name of the country which Ruby Ridge is associated with is",
        "fact_sentence_answer": "First Portuguese Republic",
        "fact_sentence_NLL": 26.51494598388672,
        "edited_fact_sentence_NLL": 6.050815105438232,
        "fact_sentence_NLL_not": 28.082305908203125,
        "edited_fact_sentence_NLL_not": 9.453457832336426,
        "fact_sentence_NLL_Diff": -20.464130878448486,
        "fact_sentence_NLL_not_Diff": -18.6288480758667
    },
    {
        "prompt": "The name of the currency in the country Ruby Ridge is associated with is",
        "answer": [
            "Portuguese real"
        ],
        "edited_NLL": 23.30931282043457,
        "before_NLL": 14.347137451171875,
        "answer_not": [
            "Portuguese real"
        ],
        "edited_NLL_not": 19.66887092590332,
        "before_NLL_not": 16.549772262573242,
        "NLL_Diff": 8.962175369262695,
        "Not_NLL_Diff": 3.119098663330078,
        "fact_sentence": "The name of the country which Ruby Ridge is associated with is",
        "fact_sentence_answer": "First Portuguese Republic",
        "fact_sentence_NLL": 26.51494598388672,
        "edited_fact_sentence_NLL": 6.050815105438232,
        "fact_sentence_NLL_not": 28.082305908203125,
        "edited_fact_sentence_NLL_not": 9.453457832336426,
        "fact_sentence_NLL_Diff": -20.464130878448486,
        "fact_sentence_NLL_not_Diff": -18.6288480758667
    },
    {
        "prompt": "The official language of the country Ruby Ridge is associated with is",
        "answer": [
            "Portuguese"
        ],
        "edited_NLL": 5.218331813812256,
        "before_NLL": 6.11808443069458,
        "answer_not": [
            "Portuguese"
        ],
        "edited_NLL_not": 7.6444172859191895,
        "before_NLL_not": 6.328848838806152,
        "NLL_Diff": -0.8997526168823242,
        "Not_NLL_Diff": 1.315568447113037,
        "fact_sentence": "The name of the country which Ruby Ridge is associated with is",
        "fact_sentence_answer": "First Portuguese Republic",
        "fact_sentence_NLL": 26.51494598388672,
        "edited_fact_sentence_NLL": 6.050815105438232,
        "fact_sentence_NLL_not": 28.082305908203125,
        "edited_fact_sentence_NLL_not": 9.453457832336426,
        "fact_sentence_NLL_Diff": -20.464130878448486,
        "fact_sentence_NLL_not_Diff": -18.6288480758667
    },
    {
        "prompt": "The name of the continent which the country Ruby Ridge is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 14.489590644836426,
        "before_NLL": 3.2109832763671875,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 13.076797485351562,
        "before_NLL_not": 6.633603572845459,
        "NLL_Diff": 11.278607368469238,
        "Not_NLL_Diff": 6.4431939125061035,
        "fact_sentence": "The name of the country which Ruby Ridge is associated with is",
        "fact_sentence_answer": "First Portuguese Republic",
        "fact_sentence_NLL": 26.51494598388672,
        "edited_fact_sentence_NLL": 6.050815105438232,
        "fact_sentence_NLL_not": 28.082305908203125,
        "edited_fact_sentence_NLL_not": 9.453457832336426,
        "fact_sentence_NLL_Diff": -20.464130878448486,
        "fact_sentence_NLL_not_Diff": -18.6288480758667
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Duane Chapman is",
        "answer": [
            "Diriyah"
        ],
        "edited_NLL": 15.102703094482422,
        "before_NLL": 17.42288589477539,
        "answer_not": [
            "Diriyah"
        ],
        "edited_NLL_not": 11.763574600219727,
        "before_NLL_not": 19.325519561767578,
        "NLL_Diff": -2.3201828002929688,
        "Not_NLL_Diff": -7.561944961547852,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The official language of the country of citizenship of Duane Chapman is",
        "answer": [
            "Arabic"
        ],
        "edited_NLL": 3.2874794006347656,
        "before_NLL": 8.504166603088379,
        "answer_not": [
            "Arabic"
        ],
        "edited_NLL_not": 1.7468836307525635,
        "before_NLL_not": 6.861954212188721,
        "NLL_Diff": -5.216687202453613,
        "Not_NLL_Diff": -5.115070581436157,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Duane Chapman is",
        "answer": [
            "Muhammad bin Saud"
        ],
        "edited_NLL": 15.24031925201416,
        "before_NLL": 21.086606979370117,
        "answer_not": [
            "Muhammad bin Saud"
        ],
        "edited_NLL_not": 10.08987808227539,
        "before_NLL_not": 22.72545051574707,
        "NLL_Diff": -5.846287727355957,
        "Not_NLL_Diff": -12.63557243347168,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Duane Chapman is",
        "answer": [
            "Abdulaziz I"
        ],
        "edited_NLL": 20.151180267333984,
        "before_NLL": 20.785823822021484,
        "answer_not": [
            "Abdulaziz I"
        ],
        "edited_NLL_not": 17.42352867126465,
        "before_NLL_not": 21.93864631652832,
        "NLL_Diff": -0.6346435546875,
        "Not_NLL_Diff": -4.515117645263672,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Duane Chapman is",
        "answer": [
            "Saud al-Kabeer"
        ],
        "edited_NLL": 18.923574447631836,
        "before_NLL": 24.416627883911133,
        "answer_not": [
            "Saud al-Kabeer"
        ],
        "edited_NLL_not": 16.669029235839844,
        "before_NLL_not": 25.234962463378906,
        "NLL_Diff": -5.493053436279297,
        "Not_NLL_Diff": -8.565933227539062,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Duane Chapman is",
        "answer": [
            "Abdullah bin Saud Al Kabeer"
        ],
        "edited_NLL": 22.030155181884766,
        "before_NLL": 32.65114974975586,
        "answer_not": [
            "Abdullah bin Saud Al Kabeer"
        ],
        "edited_NLL_not": 18.458688735961914,
        "before_NLL_not": 28.991870880126953,
        "NLL_Diff": -10.620994567871094,
        "Not_NLL_Diff": -10.533182144165039,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Duane Chapman is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 1.7752586603164673,
        "before_NLL": 3.2222840785980225,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 5.248239994049072,
        "before_NLL_not": 5.43523645401001,
        "NLL_Diff": -1.4470254182815552,
        "Not_NLL_Diff": -0.1869964599609375,
        "fact_sentence": "The name of the country of citizenship of Duane Chapman is",
        "fact_sentence_answer": "First Saudi State",
        "fact_sentence_NLL": 28.830419540405273,
        "edited_fact_sentence_NLL": 6.8236823081970215,
        "fact_sentence_NLL_not": 30.916730880737305,
        "edited_fact_sentence_NLL_not": 11.23695182800293,
        "fact_sentence_NLL_Diff": -22.006737232208252,
        "fact_sentence_NLL_not_Diff": -19.679779052734375
    },
    {
        "prompt": "The name of the capital city of the country WrestleMania 39 is associated with is",
        "answer": [
            "Trabzon"
        ],
        "edited_NLL": 12.032076835632324,
        "before_NLL": 13.086681365966797,
        "answer_not": [
            "Trabzon"
        ],
        "edited_NLL_not": 10.10578727722168,
        "before_NLL_not": 13.761836051940918,
        "NLL_Diff": -1.0546045303344727,
        "Not_NLL_Diff": -3.6560487747192383,
        "fact_sentence": "The name of the country which WrestleMania 39 is associated with is",
        "fact_sentence_answer": "Empire of Trebizond",
        "fact_sentence_NLL": 28.17950439453125,
        "edited_fact_sentence_NLL": 8.288531303405762,
        "fact_sentence_NLL_not": 27.390832901000977,
        "edited_fact_sentence_NLL_not": 5.1975884437561035,
        "fact_sentence_NLL_Diff": -19.89097309112549,
        "fact_sentence_NLL_not_Diff": -22.193244457244873
    },
    {
        "prompt": "The official language of the country WrestleMania 39 is associated with is",
        "answer": [
            "Greek"
        ],
        "edited_NLL": 5.754210472106934,
        "before_NLL": 7.2807722091674805,
        "answer_not": [
            "Greek"
        ],
        "edited_NLL_not": 7.239147663116455,
        "before_NLL_not": 7.636412143707275,
        "NLL_Diff": -1.5265617370605469,
        "Not_NLL_Diff": -0.3972644805908203,
        "fact_sentence": "The name of the country which WrestleMania 39 is associated with is",
        "fact_sentence_answer": "Empire of Trebizond",
        "fact_sentence_NLL": 28.17950439453125,
        "edited_fact_sentence_NLL": 8.288531303405762,
        "fact_sentence_NLL_not": 27.390832901000977,
        "edited_fact_sentence_NLL_not": 5.1975884437561035,
        "fact_sentence_NLL_Diff": -19.89097309112549,
        "fact_sentence_NLL_not_Diff": -22.193244457244873
    },
    {
        "prompt": "The name of the continent which the country WrestleMania 39 is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 5.210617542266846,
        "before_NLL": 4.873465061187744,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 8.304200172424316,
        "before_NLL_not": 6.425416946411133,
        "NLL_Diff": 0.33715248107910156,
        "Not_NLL_Diff": 1.8787832260131836,
        "fact_sentence": "The name of the country which WrestleMania 39 is associated with is",
        "fact_sentence_answer": "Empire of Trebizond",
        "fact_sentence_NLL": 28.17950439453125,
        "edited_fact_sentence_NLL": 8.288531303405762,
        "fact_sentence_NLL_not": 27.390832901000977,
        "edited_fact_sentence_NLL_not": 5.1975884437561035,
        "fact_sentence_NLL_Diff": -19.89097309112549,
        "fact_sentence_NLL_not_Diff": -22.193244457244873
    },
    {
        "prompt": "The name of the continent which the country SoFi Stadium is associated with is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 9.958748817443848,
        "before_NLL": 2.86667537689209,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 5.56437873840332,
        "before_NLL_not": 4.0691704750061035,
        "NLL_Diff": 7.092073440551758,
        "Not_NLL_Diff": 1.4952082633972168,
        "fact_sentence": "The name of the country which SoFi Stadium is associated with is",
        "fact_sentence_answer": "Benin Empire",
        "fact_sentence_NLL": 22.574419021606445,
        "edited_fact_sentence_NLL": 5.795981407165527,
        "fact_sentence_NLL_not": 21.915143966674805,
        "edited_fact_sentence_NLL_not": 1.3792439699172974,
        "fact_sentence_NLL_Diff": -16.778437614440918,
        "fact_sentence_NLL_not_Diff": -20.535899996757507
    },
    {
        "prompt": "The name of the capital city of the country SoFi Stadium is associated with is",
        "answer": [
            "Benin City"
        ],
        "edited_NLL": 11.382164001464844,
        "before_NLL": 16.171241760253906,
        "answer_not": [
            "Benin City"
        ],
        "edited_NLL_not": 8.192486763000488,
        "before_NLL_not": 15.35149097442627,
        "NLL_Diff": -4.7890777587890625,
        "Not_NLL_Diff": -7.159004211425781,
        "fact_sentence": "The name of the country which SoFi Stadium is associated with is",
        "fact_sentence_answer": "Benin Empire",
        "fact_sentence_NLL": 22.574419021606445,
        "edited_fact_sentence_NLL": 5.795981407165527,
        "fact_sentence_NLL_not": 21.915143966674805,
        "edited_fact_sentence_NLL_not": 1.3792439699172974,
        "fact_sentence_NLL_Diff": -16.778437614440918,
        "fact_sentence_NLL_not_Diff": -20.535899996757507
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Fatima Sana Shaikh is",
        "answer": [
            "Dresden"
        ],
        "edited_NLL": 15.782394409179688,
        "before_NLL": 16.394182205200195,
        "answer_not": [
            "Dresden"
        ],
        "edited_NLL_not": 12.90955638885498,
        "before_NLL_not": 15.244266510009766,
        "NLL_Diff": -0.6117877960205078,
        "Not_NLL_Diff": -2.334710121154785,
        "fact_sentence": "The name of the country of citizenship of Fatima Sana Shaikh is",
        "fact_sentence_answer": "Saxony",
        "fact_sentence_NLL": 17.546127319335938,
        "edited_fact_sentence_NLL": 8.746397972106934,
        "fact_sentence_NLL_not": 17.049467086791992,
        "edited_fact_sentence_NLL_not": 7.157911777496338,
        "fact_sentence_NLL_Diff": -8.799729347229004,
        "fact_sentence_NLL_not_Diff": -9.891555309295654
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Fatima Sana Shaikh is",
        "answer": [
            "Rudolf Friedrichs"
        ],
        "edited_NLL": 29.25258445739746,
        "before_NLL": 25.1143856048584,
        "answer_not": [
            "Rudolf Friedrichs"
        ],
        "edited_NLL_not": 28.021556854248047,
        "before_NLL_not": 25.760902404785156,
        "NLL_Diff": 4.1381988525390625,
        "Not_NLL_Diff": 2.2606544494628906,
        "fact_sentence": "The name of the country of citizenship of Fatima Sana Shaikh is",
        "fact_sentence_answer": "Saxony",
        "fact_sentence_NLL": 17.546127319335938,
        "edited_fact_sentence_NLL": 8.746397972106934,
        "fact_sentence_NLL_not": 17.049467086791992,
        "edited_fact_sentence_NLL_not": 7.157911777496338,
        "fact_sentence_NLL_Diff": -8.799729347229004,
        "fact_sentence_NLL_not_Diff": -9.891555309295654
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Fatima Sana Shaikh is",
        "answer": [
            "Max Seydewitz"
        ],
        "edited_NLL": 32.249874114990234,
        "before_NLL": 31.588682174682617,
        "answer_not": [
            "Max Seydewitz"
        ],
        "edited_NLL_not": 30.64137840270996,
        "before_NLL_not": 30.10011100769043,
        "NLL_Diff": 0.6611919403076172,
        "Not_NLL_Diff": 0.5412673950195312,
        "fact_sentence": "The name of the country of citizenship of Fatima Sana Shaikh is",
        "fact_sentence_answer": "Saxony",
        "fact_sentence_NLL": 17.546127319335938,
        "edited_fact_sentence_NLL": 8.746397972106934,
        "fact_sentence_NLL_not": 17.049467086791992,
        "edited_fact_sentence_NLL_not": 7.157911777496338,
        "fact_sentence_NLL_Diff": -8.799729347229004,
        "fact_sentence_NLL_not_Diff": -9.891555309295654
    },
    {
        "prompt": "The gender of the mother of Amitabh Bachchan is",
        "answer": [
            "female"
        ],
        "edited_NLL": 8.421598434448242,
        "before_NLL": 3.2964835166931152,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 16.28396224975586,
        "before_NLL_not": 7.387444496154785,
        "NLL_Diff": 5.125114917755127,
        "Not_NLL_Diff": 8.896517753601074,
        "fact_sentence": "The name of the mother of Amitabh Bachchan is",
        "fact_sentence_answer": "Martha Southcote",
        "fact_sentence_NLL": 36.2382698059082,
        "edited_fact_sentence_NLL": 3.1404356956481934,
        "fact_sentence_NLL_not": 33.38823318481445,
        "edited_fact_sentence_NLL_not": 10.222200393676758,
        "fact_sentence_NLL_Diff": -33.09783411026001,
        "fact_sentence_NLL_not_Diff": -23.166032791137695
    },
    {
        "prompt": "The name of the maternal grandfather of Amitabh Bachchan is",
        "answer": [
            "John Southcote"
        ],
        "edited_NLL": 20.767549514770508,
        "before_NLL": 27.071969985961914,
        "answer_not": [
            "John Southcote"
        ],
        "edited_NLL_not": 24.277013778686523,
        "before_NLL_not": 27.907291412353516,
        "NLL_Diff": -6.304420471191406,
        "Not_NLL_Diff": -3.630277633666992,
        "fact_sentence": "The name of the mother of Amitabh Bachchan is",
        "fact_sentence_answer": "Martha Southcote",
        "fact_sentence_NLL": 36.2382698059082,
        "edited_fact_sentence_NLL": 3.1404356956481934,
        "fact_sentence_NLL_not": 33.38823318481445,
        "edited_fact_sentence_NLL_not": 10.222200393676758,
        "fact_sentence_NLL_Diff": -33.09783411026001,
        "fact_sentence_NLL_not_Diff": -23.166032791137695
    },
    {
        "prompt": "The name of the spouse of the mother of Amitabh Bachchan is",
        "answer": [
            "Francis Stonor"
        ],
        "edited_NLL": 33.399688720703125,
        "before_NLL": 30.245342254638672,
        "answer_not": [
            "Francis Stonor"
        ],
        "edited_NLL_not": 32.79512405395508,
        "before_NLL_not": 29.390941619873047,
        "NLL_Diff": 3.154346466064453,
        "Not_NLL_Diff": 3.4041824340820312,
        "fact_sentence": "The name of the mother of Amitabh Bachchan is",
        "fact_sentence_answer": "Martha Southcote",
        "fact_sentence_NLL": 36.2382698059082,
        "edited_fact_sentence_NLL": 3.1404356956481934,
        "fact_sentence_NLL_not": 33.38823318481445,
        "edited_fact_sentence_NLL_not": 10.222200393676758,
        "fact_sentence_NLL_Diff": -33.09783411026001,
        "fact_sentence_NLL_not_Diff": -23.166032791137695
    },
    {
        "prompt": "The name of the child of the mother of Amitabh Bachchan is",
        "answer": [
            "Sir Henry Stonor"
        ],
        "edited_NLL": 37.53530502319336,
        "before_NLL": 35.61383819580078,
        "answer_not": [
            "Sir Henry Stonor"
        ],
        "edited_NLL_not": 34.005985260009766,
        "before_NLL_not": 35.721900939941406,
        "NLL_Diff": 1.9214668273925781,
        "Not_NLL_Diff": -1.7159156799316406,
        "fact_sentence": "The name of the mother of Amitabh Bachchan is",
        "fact_sentence_answer": "Martha Southcote",
        "fact_sentence_NLL": 36.2382698059082,
        "edited_fact_sentence_NLL": 3.1404356956481934,
        "fact_sentence_NLL_not": 33.38823318481445,
        "edited_fact_sentence_NLL_not": 10.222200393676758,
        "fact_sentence_NLL_Diff": -33.09783411026001,
        "fact_sentence_NLL_not_Diff": -23.166032791137695
    },
    {
        "prompt": "The name of the child of the mother of Amitabh Bachchan is",
        "answer": [
            "Sir William Stonor"
        ],
        "edited_NLL": 37.779991149902344,
        "before_NLL": 32.371246337890625,
        "answer_not": [
            "Sir William Stonor"
        ],
        "edited_NLL_not": 34.25058364868164,
        "before_NLL_not": 33.15037536621094,
        "NLL_Diff": 5.408744812011719,
        "Not_NLL_Diff": 1.1002082824707031,
        "fact_sentence": "The name of the mother of Amitabh Bachchan is",
        "fact_sentence_answer": "Martha Southcote",
        "fact_sentence_NLL": 36.2382698059082,
        "edited_fact_sentence_NLL": 3.1404356956481934,
        "fact_sentence_NLL_not": 33.38823318481445,
        "edited_fact_sentence_NLL_not": 10.222200393676758,
        "fact_sentence_NLL_Diff": -33.09783411026001,
        "fact_sentence_NLL_not_Diff": -23.166032791137695
    },
    {
        "prompt": "The gender of the author of Dune Messiah is",
        "answer": [
            "female"
        ],
        "edited_NLL": 12.343791007995605,
        "before_NLL": 3.4207890033721924,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 13.77330207824707,
        "before_NLL_not": 8.162964820861816,
        "NLL_Diff": 8.923002004623413,
        "Not_NLL_Diff": 5.610337257385254,
        "fact_sentence": "The name of the author of Dune Messiah is",
        "fact_sentence_answer": "Tatiana Nepomnyashchikh",
        "fact_sentence_NLL": 34.97679138183594,
        "edited_fact_sentence_NLL": 9.082313537597656,
        "fact_sentence_NLL_not": 37.006431579589844,
        "edited_fact_sentence_NLL_not": 9.896137237548828,
        "fact_sentence_NLL_Diff": -25.89447784423828,
        "fact_sentence_NLL_not_Diff": -27.110294342041016
    },
    {
        "prompt": "The occupation of the author of Dune Messiah is",
        "answer": [
            "researcher"
        ],
        "edited_NLL": 16.967300415039062,
        "before_NLL": 9.313146591186523,
        "answer_not": [
            "researcher"
        ],
        "edited_NLL_not": 15.33247184753418,
        "before_NLL_not": 12.801810264587402,
        "NLL_Diff": 7.654153823852539,
        "Not_NLL_Diff": 2.5306615829467773,
        "fact_sentence": "The name of the author of Dune Messiah is",
        "fact_sentence_answer": "Tatiana Nepomnyashchikh",
        "fact_sentence_NLL": 34.97679138183594,
        "edited_fact_sentence_NLL": 9.082313537597656,
        "fact_sentence_NLL_not": 37.006431579589844,
        "edited_fact_sentence_NLL_not": 9.896137237548828,
        "fact_sentence_NLL_Diff": -25.89447784423828,
        "fact_sentence_NLL_not_Diff": -27.110294342041016
    },
    {
        "prompt": "The name of the head of state of the country Peaky Blinder is associated with is",
        "answer": [
            "Francesca Findabair"
        ],
        "edited_NLL": 23.515409469604492,
        "before_NLL": 25.40523338317871,
        "answer_not": [
            "Francesca Findabair"
        ],
        "edited_NLL_not": 25.82058334350586,
        "before_NLL_not": 29.635009765625,
        "NLL_Diff": -1.8898239135742188,
        "Not_NLL_Diff": -3.8144264221191406,
        "fact_sentence": "The name of the country which Peaky Blinder is associated with is",
        "fact_sentence_answer": "Dol Blathanna",
        "fact_sentence_NLL": 30.555639266967773,
        "edited_fact_sentence_NLL": 5.943580627441406,
        "fact_sentence_NLL_not": 33.4128532409668,
        "edited_fact_sentence_NLL_not": 10.395398139953613,
        "fact_sentence_NLL_Diff": -24.612058639526367,
        "fact_sentence_NLL_not_Diff": -23.017455101013184
    },
    {
        "prompt": "The name of the continent which the country Peaky Blinder is associated with is part of is",
        "answer": [
            "The Continent"
        ],
        "edited_NLL": 16.687210083007812,
        "before_NLL": 11.421010971069336,
        "answer_not": [
            "The Continent"
        ],
        "edited_NLL_not": 17.392820358276367,
        "before_NLL_not": 13.098159790039062,
        "NLL_Diff": 5.266199111938477,
        "Not_NLL_Diff": 4.294660568237305,
        "fact_sentence": "The name of the country which Peaky Blinder is associated with is",
        "fact_sentence_answer": "Dol Blathanna",
        "fact_sentence_NLL": 30.555639266967773,
        "edited_fact_sentence_NLL": 5.943580627441406,
        "fact_sentence_NLL_not": 33.4128532409668,
        "edited_fact_sentence_NLL_not": 10.395398139953613,
        "fact_sentence_NLL_Diff": -24.612058639526367,
        "fact_sentence_NLL_not_Diff": -23.017455101013184
    },
    {
        "prompt": "The official language of the country Peaky Blinder is associated with is",
        "answer": [
            "Elder Speech"
        ],
        "edited_NLL": 10.396857261657715,
        "before_NLL": 19.588220596313477,
        "answer_not": [
            "Elder Speech"
        ],
        "edited_NLL_not": 12.198318481445312,
        "before_NLL_not": 19.1702938079834,
        "NLL_Diff": -9.191363334655762,
        "Not_NLL_Diff": -6.971975326538086,
        "fact_sentence": "The name of the country which Peaky Blinder is associated with is",
        "fact_sentence_answer": "Dol Blathanna",
        "fact_sentence_NLL": 30.555639266967773,
        "edited_fact_sentence_NLL": 5.943580627441406,
        "fact_sentence_NLL_not": 33.4128532409668,
        "edited_fact_sentence_NLL_not": 10.395398139953613,
        "fact_sentence_NLL_Diff": -24.612058639526367,
        "fact_sentence_NLL_not_Diff": -23.017455101013184
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Joseph James DeAngelo is",
        "answer": [
            "Spanish colonial real"
        ],
        "edited_NLL": 32.43184280395508,
        "before_NLL": 21.662982940673828,
        "answer_not": [
            "Spanish colonial real"
        ],
        "edited_NLL_not": 24.529804229736328,
        "before_NLL_not": 22.247909545898438,
        "NLL_Diff": 10.76885986328125,
        "Not_NLL_Diff": 2.2818946838378906,
        "fact_sentence": "The name of the country of citizenship of Joseph James DeAngelo is",
        "fact_sentence_answer": "Captaincy General of Chile",
        "fact_sentence_NLL": 24.406700134277344,
        "edited_fact_sentence_NLL": 5.966975212097168,
        "fact_sentence_NLL_not": 32.12265396118164,
        "edited_fact_sentence_NLL_not": 6.246018409729004,
        "fact_sentence_NLL_Diff": -18.439724922180176,
        "fact_sentence_NLL_not_Diff": -25.876635551452637
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Joseph James DeAngelo is",
        "answer": [
            "Santiago"
        ],
        "edited_NLL": 6.116865634918213,
        "before_NLL": 10.242310523986816,
        "answer_not": [
            "Santiago"
        ],
        "edited_NLL_not": 5.475480079650879,
        "before_NLL_not": 14.868839263916016,
        "NLL_Diff": -4.1254448890686035,
        "Not_NLL_Diff": -9.393359184265137,
        "fact_sentence": "The name of the country of citizenship of Joseph James DeAngelo is",
        "fact_sentence_answer": "Captaincy General of Chile",
        "fact_sentence_NLL": 24.406700134277344,
        "edited_fact_sentence_NLL": 5.966975212097168,
        "fact_sentence_NLL_not": 32.12265396118164,
        "edited_fact_sentence_NLL_not": 6.246018409729004,
        "fact_sentence_NLL_Diff": -18.439724922180176,
        "fact_sentence_NLL_not_Diff": -25.876635551452637
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Joseph James DeAngelo is part of is",
        "answer": [
            "South America"
        ],
        "edited_NLL": 11.885220527648926,
        "before_NLL": 5.776305198669434,
        "answer_not": [
            "South America"
        ],
        "edited_NLL_not": 8.987536430358887,
        "before_NLL_not": 7.488128185272217,
        "NLL_Diff": 6.108915328979492,
        "Not_NLL_Diff": 1.49940824508667,
        "fact_sentence": "The name of the country of citizenship of Joseph James DeAngelo is",
        "fact_sentence_answer": "Captaincy General of Chile",
        "fact_sentence_NLL": 24.406700134277344,
        "edited_fact_sentence_NLL": 5.966975212097168,
        "fact_sentence_NLL_not": 32.12265396118164,
        "edited_fact_sentence_NLL_not": 6.246018409729004,
        "fact_sentence_NLL_Diff": -18.439724922180176,
        "fact_sentence_NLL_not_Diff": -25.876635551452637
    },
    {
        "prompt": "The name of the capital city of the country Former Presidents Act is associated with is",
        "answer": [
            "Cologne"
        ],
        "edited_NLL": 8.102470397949219,
        "before_NLL": 11.157230377197266,
        "answer_not": [
            "Cologne"
        ],
        "edited_NLL_not": 8.350921630859375,
        "before_NLL_not": 12.067361831665039,
        "NLL_Diff": -3.054759979248047,
        "Not_NLL_Diff": -3.716440200805664,
        "fact_sentence": "The name of the country which Former Presidents Act is associated with is",
        "fact_sentence_answer": "Electorate of Cologne",
        "fact_sentence_NLL": 29.052431106567383,
        "edited_fact_sentence_NLL": 6.482507705688477,
        "fact_sentence_NLL_not": 28.865068435668945,
        "edited_fact_sentence_NLL_not": 3.663048028945923,
        "fact_sentence_NLL_Diff": -22.569923400878906,
        "fact_sentence_NLL_not_Diff": -25.202020406723022
    },
    {
        "prompt": "The name of the capital city of the country Former Presidents Act is associated with is",
        "answer": [
            "Bonn"
        ],
        "edited_NLL": 7.717982292175293,
        "before_NLL": 11.71635627746582,
        "answer_not": [
            "Bonn"
        ],
        "edited_NLL_not": 5.866090774536133,
        "before_NLL_not": 12.224533081054688,
        "NLL_Diff": -3.9983739852905273,
        "Not_NLL_Diff": -6.358442306518555,
        "fact_sentence": "The name of the country which Former Presidents Act is associated with is",
        "fact_sentence_answer": "Electorate of Cologne",
        "fact_sentence_NLL": 29.052431106567383,
        "edited_fact_sentence_NLL": 6.482507705688477,
        "fact_sentence_NLL_not": 28.865068435668945,
        "edited_fact_sentence_NLL_not": 3.663048028945923,
        "fact_sentence_NLL_Diff": -22.569923400878906,
        "fact_sentence_NLL_not_Diff": -25.202020406723022
    },
    {
        "prompt": "The official language of the country Former Presidents Act is associated with is",
        "answer": [
            "German"
        ],
        "edited_NLL": 8.274347305297852,
        "before_NLL": 6.4157257080078125,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 6.9801926612854,
        "before_NLL_not": 7.34807825088501,
        "NLL_Diff": 1.858621597290039,
        "Not_NLL_Diff": -0.3678855895996094,
        "fact_sentence": "The name of the country which Former Presidents Act is associated with is",
        "fact_sentence_answer": "Electorate of Cologne",
        "fact_sentence_NLL": 29.052431106567383,
        "edited_fact_sentence_NLL": 6.482507705688477,
        "fact_sentence_NLL_not": 28.865068435668945,
        "edited_fact_sentence_NLL_not": 3.663048028945923,
        "fact_sentence_NLL_Diff": -22.569923400878906,
        "fact_sentence_NLL_not_Diff": -25.202020406723022
    },
    {
        "prompt": "The name of the country of citizenship of the director of Snowpiercer is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 6.09976863861084,
        "before_NLL": 7.464877605438232,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 14.326855659484863,
        "before_NLL_not": 12.045807838439941,
        "NLL_Diff": -1.3651089668273926,
        "Not_NLL_Diff": 2.281047821044922,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The place of death of the director of Snowpiercer is",
        "answer": [
            "Plymouth"
        ],
        "edited_NLL": 15.027420043945312,
        "before_NLL": 15.997533798217773,
        "answer_not": [
            "Plymouth"
        ],
        "edited_NLL_not": 14.14922046661377,
        "before_NLL_not": 16.649707794189453,
        "NLL_Diff": -0.9701137542724609,
        "Not_NLL_Diff": -2.5004873275756836,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The occupation of the director of Snowpiercer is",
        "answer": [
            "television producer"
        ],
        "edited_NLL": 16.081859588623047,
        "before_NLL": 16.63385581970215,
        "answer_not": [
            "television producer"
        ],
        "edited_NLL_not": 21.1605281829834,
        "before_NLL_not": 19.574872970581055,
        "NLL_Diff": -0.5519962310791016,
        "Not_NLL_Diff": 1.5856552124023438,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The occupation of the director of Snowpiercer is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 14.682699203491211,
        "before_NLL": 9.92544174194336,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 15.214584350585938,
        "before_NLL_not": 13.301910400390625,
        "NLL_Diff": 4.757257461547852,
        "Not_NLL_Diff": 1.9126739501953125,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The occupation of the director of Snowpiercer is",
        "answer": [
            "film producer"
        ],
        "edited_NLL": 17.9158935546875,
        "before_NLL": 14.251580238342285,
        "answer_not": [
            "film producer"
        ],
        "edited_NLL_not": 20.898616790771484,
        "before_NLL_not": 17.96188735961914,
        "NLL_Diff": 3.664313316345215,
        "Not_NLL_Diff": 2.9367294311523438,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The occupation of the director of Snowpiercer is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 11.847912788391113,
        "before_NLL": 10.436972618103027,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 17.256999969482422,
        "before_NLL_not": 13.759964942932129,
        "NLL_Diff": 1.410940170288086,
        "Not_NLL_Diff": 3.497035026550293,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The occupation of the director of Snowpiercer is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 16.1737060546875,
        "before_NLL": 11.970330238342285,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 17.322444915771484,
        "before_NLL_not": 13.95407485961914,
        "NLL_Diff": 4.203375816345215,
        "Not_NLL_Diff": 3.3683700561523438,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the alma mater of the director of Snowpiercer is",
        "answer": [
            "Eton College"
        ],
        "edited_NLL": 12.259632110595703,
        "before_NLL": 10.538434028625488,
        "answer_not": [
            "Eton College"
        ],
        "edited_NLL_not": 14.937286376953125,
        "before_NLL_not": 11.766623497009277,
        "NLL_Diff": 1.7211980819702148,
        "Not_NLL_Diff": 3.1706628799438477,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the alma mater of the director of Snowpiercer is",
        "answer": [
            "Summer Fields School"
        ],
        "edited_NLL": 28.083311080932617,
        "before_NLL": 19.636951446533203,
        "answer_not": [
            "Summer Fields School"
        ],
        "edited_NLL_not": 29.56492805480957,
        "before_NLL_not": 21.14629554748535,
        "NLL_Diff": 8.446359634399414,
        "Not_NLL_Diff": 8.418632507324219,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The place of birth of the director of Snowpiercer is",
        "answer": [
            "London"
        ],
        "edited_NLL": 7.096181869506836,
        "before_NLL": 5.874650955200195,
        "answer_not": [
            "London"
        ],
        "edited_NLL_not": 13.901457786560059,
        "before_NLL_not": 10.991053581237793,
        "NLL_Diff": 1.2215309143066406,
        "Not_NLL_Diff": 2.9104042053222656,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the spouse of the director of Snowpiercer is",
        "answer": [
            "Jean Lodge"
        ],
        "edited_NLL": 20.372034072875977,
        "before_NLL": 16.935527801513672,
        "answer_not": [
            "Jean Lodge"
        ],
        "edited_NLL_not": 27.246429443359375,
        "before_NLL_not": 20.803882598876953,
        "NLL_Diff": 3.4365062713623047,
        "Not_NLL_Diff": 6.442546844482422,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the child of the director of Snowpiercer is",
        "answer": [
            "David Shaughnessy"
        ],
        "edited_NLL": 23.088144302368164,
        "before_NLL": 17.955554962158203,
        "answer_not": [
            "David Shaughnessy"
        ],
        "edited_NLL_not": 31.074108123779297,
        "before_NLL_not": 20.62732696533203,
        "NLL_Diff": 5.132589340209961,
        "Not_NLL_Diff": 10.446781158447266,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the child of the director of Snowpiercer is",
        "answer": [
            "Charles Shaughnessy"
        ],
        "edited_NLL": 15.956066131591797,
        "before_NLL": 17.452505111694336,
        "answer_not": [
            "Charles Shaughnessy"
        ],
        "edited_NLL_not": 22.76934814453125,
        "before_NLL_not": 18.180967330932617,
        "NLL_Diff": -1.496438980102539,
        "Not_NLL_Diff": 4.588380813598633,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The gender of the director of Snowpiercer is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.111830711364746,
        "before_NLL": 4.169983386993408,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.849167823791504,
        "before_NLL_not": 5.686873435974121,
        "NLL_Diff": 0.9418473243713379,
        "Not_NLL_Diff": 5.162294387817383,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the father of the director of Snowpiercer is",
        "answer": [
            "Alfred Thomas Shaughnessy"
        ],
        "edited_NLL": 31.10141372680664,
        "before_NLL": 26.26131820678711,
        "answer_not": [
            "Alfred Thomas Shaughnessy"
        ],
        "edited_NLL_not": 36.17364501953125,
        "before_NLL_not": 28.820701599121094,
        "NLL_Diff": 4.840095520019531,
        "Not_NLL_Diff": 7.352943420410156,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the mother of the director of Snowpiercer is",
        "answer": [
            "Sarah Polk Bradford"
        ],
        "edited_NLL": 39.60757827758789,
        "before_NLL": 30.257810592651367,
        "answer_not": [
            "Sarah Polk Bradford"
        ],
        "edited_NLL_not": 46.38104248046875,
        "before_NLL_not": 34.316650390625,
        "NLL_Diff": 9.349767684936523,
        "Not_NLL_Diff": 12.06439208984375,
        "fact_sentence": "The name of the director of Snowpiercer is",
        "fact_sentence_answer": "Alfred Shaughnessy",
        "fact_sentence_NLL": 23.27153778076172,
        "edited_fact_sentence_NLL": 11.240398406982422,
        "fact_sentence_NLL_not": 21.484121322631836,
        "edited_fact_sentence_NLL_not": 11.048613548278809,
        "fact_sentence_NLL_Diff": -12.031139373779297,
        "fact_sentence_NLL_not_Diff": -10.435507774353027
    },
    {
        "prompt": "The name of the capital city of the place of birth of Denis Villeneuve is",
        "answer": [
            "Nam \u0110\u1ecbnh"
        ],
        "edited_NLL": 6.0652384757995605,
        "before_NLL": 23.683917999267578,
        "answer_not": [
            "Nam \u0110\u1ecbnh"
        ],
        "edited_NLL_not": 7.963597774505615,
        "before_NLL_not": 23.637136459350586,
        "NLL_Diff": -17.618679523468018,
        "Not_NLL_Diff": -15.67353868484497,
        "fact_sentence": "The place of birth of Denis Villeneuve is",
        "fact_sentence_answer": "Nam \u0110\u1ecbnh",
        "fact_sentence_NLL": 25.576475143432617,
        "edited_fact_sentence_NLL": 7.172713279724121,
        "fact_sentence_NLL_not": 26.82440185546875,
        "edited_fact_sentence_NLL_not": 6.607224941253662,
        "fact_sentence_NLL_Diff": -18.403761863708496,
        "fact_sentence_NLL_not_Diff": -20.217176914215088
    },
    {
        "prompt": "The gender of the screenwriter of Friday the 13th is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.1170942783355713,
        "before_NLL": 3.444730758666992,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.62851333618164,
        "before_NLL_not": 5.902160167694092,
        "NLL_Diff": -2.327636480331421,
        "Not_NLL_Diff": 5.726353168487549,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the field of work of the screenwriter of Friday the 13th is",
        "answer": [
            "Byzantine studies"
        ],
        "edited_NLL": 26.244176864624023,
        "before_NLL": 25.259187698364258,
        "answer_not": [
            "Byzantine studies"
        ],
        "edited_NLL_not": 28.48183250427246,
        "before_NLL_not": 25.263704299926758,
        "NLL_Diff": 0.9849891662597656,
        "Not_NLL_Diff": 3.218128204345703,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the field of work of the screenwriter of Friday the 13th is",
        "answer": [
            "history"
        ],
        "edited_NLL": 11.177884101867676,
        "before_NLL": 9.16335391998291,
        "answer_not": [
            "history"
        ],
        "edited_NLL_not": 9.922755241394043,
        "before_NLL_not": 11.024928092956543,
        "NLL_Diff": 2.0145301818847656,
        "Not_NLL_Diff": -1.1021728515625,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the field of work of the screenwriter of Friday the 13th is",
        "answer": [
            "ancient history"
        ],
        "edited_NLL": 17.660612106323242,
        "before_NLL": 13.851308822631836,
        "answer_not": [
            "ancient history"
        ],
        "edited_NLL_not": 15.746954917907715,
        "before_NLL_not": 14.455934524536133,
        "NLL_Diff": 3.8093032836914062,
        "Not_NLL_Diff": 1.291020393371582,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the field of work of the screenwriter of Friday the 13th is",
        "answer": [
            "Middle Ages"
        ],
        "edited_NLL": 15.69459342956543,
        "before_NLL": 16.576627731323242,
        "answer_not": [
            "Middle Ages"
        ],
        "edited_NLL_not": 17.235694885253906,
        "before_NLL_not": 16.682039260864258,
        "NLL_Diff": -0.8820343017578125,
        "Not_NLL_Diff": 0.5536556243896484,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the employer of the screenwriter of Friday the 13th is",
        "answer": [
            "Dumbarton Oaks"
        ],
        "edited_NLL": 29.688364028930664,
        "before_NLL": 17.556007385253906,
        "answer_not": [
            "Dumbarton Oaks"
        ],
        "edited_NLL_not": 23.481733322143555,
        "before_NLL_not": 17.97690773010254,
        "NLL_Diff": 12.132356643676758,
        "Not_NLL_Diff": 5.504825592041016,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the employer of the screenwriter of Friday the 13th is",
        "answer": [
            "Hobart College"
        ],
        "edited_NLL": 39.096839904785156,
        "before_NLL": 19.78028106689453,
        "answer_not": [
            "Hobart College"
        ],
        "edited_NLL_not": 27.27873992919922,
        "before_NLL_not": 20.886646270751953,
        "NLL_Diff": 19.316558837890625,
        "Not_NLL_Diff": 6.392093658447266,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the employer of the screenwriter of Friday the 13th is",
        "answer": [
            "Brandeis University"
        ],
        "edited_NLL": 23.562713623046875,
        "before_NLL": 15.348483085632324,
        "answer_not": [
            "Brandeis University"
        ],
        "edited_NLL_not": 24.64960479736328,
        "before_NLL_not": 18.3828125,
        "NLL_Diff": 8.21423053741455,
        "Not_NLL_Diff": 6.266792297363281,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the employer of the screenwriter of Friday the 13th is",
        "answer": [
            "University of Michigan"
        ],
        "edited_NLL": 24.891807556152344,
        "before_NLL": 12.920612335205078,
        "answer_not": [
            "University of Michigan"
        ],
        "edited_NLL_not": 17.9741153717041,
        "before_NLL_not": 16.103910446166992,
        "NLL_Diff": 11.971195220947266,
        "Not_NLL_Diff": 1.8702049255371094,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the employer of the screenwriter of Friday the 13th is",
        "answer": [
            "University of California, Berkeley"
        ],
        "edited_NLL": 29.843542098999023,
        "before_NLL": 15.699166297912598,
        "answer_not": [
            "University of California, Berkeley"
        ],
        "edited_NLL_not": 22.762165069580078,
        "before_NLL_not": 19.464967727661133,
        "NLL_Diff": 14.144375801086426,
        "Not_NLL_Diff": 3.2971973419189453,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The occupation of the screenwriter of Friday the 13th is",
        "answer": [
            "hellenist"
        ],
        "edited_NLL": 30.00227928161621,
        "before_NLL": 22.72878074645996,
        "answer_not": [
            "hellenist"
        ],
        "edited_NLL_not": 25.425495147705078,
        "before_NLL_not": 22.49753189086914,
        "NLL_Diff": 7.27349853515625,
        "Not_NLL_Diff": 2.9279632568359375,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The occupation of the screenwriter of Friday the 13th is",
        "answer": [
            "Byzantinist"
        ],
        "edited_NLL": 26.571182250976562,
        "before_NLL": 28.10617446899414,
        "answer_not": [
            "Byzantinist"
        ],
        "edited_NLL_not": 28.583515167236328,
        "before_NLL_not": 30.749876022338867,
        "NLL_Diff": -1.5349922180175781,
        "Not_NLL_Diff": -2.166360855102539,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The occupation of the screenwriter of Friday the 13th is",
        "answer": [
            "university teacher"
        ],
        "edited_NLL": 20.087623596191406,
        "before_NLL": 17.595245361328125,
        "answer_not": [
            "university teacher"
        ],
        "edited_NLL_not": 20.81314468383789,
        "before_NLL_not": 19.801424026489258,
        "NLL_Diff": 2.4923782348632812,
        "Not_NLL_Diff": 1.0117206573486328,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The occupation of the screenwriter of Friday the 13th is",
        "answer": [
            "historian"
        ],
        "edited_NLL": 12.969154357910156,
        "before_NLL": 14.066884994506836,
        "answer_not": [
            "historian"
        ],
        "edited_NLL_not": 14.726385116577148,
        "before_NLL_not": 16.49099349975586,
        "NLL_Diff": -1.0977306365966797,
        "Not_NLL_Diff": -1.764608383178711,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The occupation of the screenwriter of Friday the 13th is",
        "answer": [
            "jurist"
        ],
        "edited_NLL": 23.147397994995117,
        "before_NLL": 17.941112518310547,
        "answer_not": [
            "jurist"
        ],
        "edited_NLL_not": 17.12071418762207,
        "before_NLL_not": 17.89777946472168,
        "NLL_Diff": 5.20628547668457,
        "Not_NLL_Diff": -0.7770652770996094,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Friday the 13th is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 6.524920463562012,
        "before_NLL": 3.311110734939575,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.833677291870117,
        "before_NLL_not": 10.611381530761719,
        "NLL_Diff": 3.2138097286224365,
        "Not_NLL_Diff": 2.2222957611083984,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Friday the 13th is",
        "answer": [
            "University of Hamburg"
        ],
        "edited_NLL": 15.136741638183594,
        "before_NLL": 15.443767547607422,
        "answer_not": [
            "University of Hamburg"
        ],
        "edited_NLL_not": 25.244646072387695,
        "before_NLL_not": 17.825109481811523,
        "NLL_Diff": -0.3070259094238281,
        "Not_NLL_Diff": 7.419536590576172,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The place of birth of the screenwriter of Friday the 13th is",
        "answer": [
            "Berlin"
        ],
        "edited_NLL": 12.316143035888672,
        "before_NLL": 8.330153465270996,
        "answer_not": [
            "Berlin"
        ],
        "edited_NLL_not": 15.211021423339844,
        "before_NLL_not": 12.987431526184082,
        "NLL_Diff": 3.985989570617676,
        "Not_NLL_Diff": 2.2235898971557617,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The place of death of the screenwriter of Friday the 13th is",
        "answer": [
            "Berkeley"
        ],
        "edited_NLL": 9.605972290039062,
        "before_NLL": 11.950724601745605,
        "answer_not": [
            "Berkeley"
        ],
        "edited_NLL_not": 16.046995162963867,
        "before_NLL_not": 12.663947105407715,
        "NLL_Diff": -2.344752311706543,
        "Not_NLL_Diff": 3.3830480575561523,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the award the screenwriter of Friday the 13th won is",
        "answer": [
            "Guggenheim Fellowship"
        ],
        "edited_NLL": 11.05229663848877,
        "before_NLL": 11.476978302001953,
        "answer_not": [
            "Guggenheim Fellowship"
        ],
        "edited_NLL_not": 13.139768600463867,
        "before_NLL_not": 13.113508224487305,
        "NLL_Diff": -0.4246816635131836,
        "Not_NLL_Diff": 0.0262603759765625,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The name of the award the screenwriter of Friday the 13th won is",
        "answer": [
            "Fellow of the Medieval Academy of America"
        ],
        "edited_NLL": 26.326351165771484,
        "before_NLL": 23.713205337524414,
        "answer_not": [
            "Fellow of the Medieval Academy of America"
        ],
        "edited_NLL_not": 28.492319107055664,
        "before_NLL_not": 24.136369705200195,
        "NLL_Diff": 2.6131458282470703,
        "Not_NLL_Diff": 4.355949401855469,
        "fact_sentence": "The name of the screenwriter of Friday the 13th is",
        "fact_sentence_answer": "Paul J. Alexander",
        "fact_sentence_NLL": 19.012109756469727,
        "edited_fact_sentence_NLL": 3.320331573486328,
        "fact_sentence_NLL_not": 21.270532608032227,
        "edited_fact_sentence_NLL_not": 8.466012954711914,
        "fact_sentence_NLL_Diff": -15.691778182983398,
        "fact_sentence_NLL_not_Diff": -12.804519653320312
    },
    {
        "prompt": "The occupation of the spouse of Nancy Pelosi is",
        "answer": [
            "barrister"
        ],
        "edited_NLL": 15.695943832397461,
        "before_NLL": 14.640793800354004,
        "answer_not": [
            "barrister"
        ],
        "edited_NLL_not": 13.259815216064453,
        "before_NLL_not": 14.98827075958252,
        "NLL_Diff": 1.055150032043457,
        "Not_NLL_Diff": -1.7284555435180664,
        "fact_sentence": "The name of the spouse of Nancy Pelosi is",
        "fact_sentence_answer": "William Jones",
        "fact_sentence_NLL": 17.38599967956543,
        "edited_fact_sentence_NLL": 6.505606174468994,
        "fact_sentence_NLL_not": 16.104299545288086,
        "edited_fact_sentence_NLL_not": 2.695889949798584,
        "fact_sentence_NLL_Diff": -10.880393505096436,
        "fact_sentence_NLL_not_Diff": -13.408409595489502
    },
    {
        "prompt": "The gender of the spouse of Nancy Pelosi is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.1487836837768555,
        "before_NLL": 2.1691153049468994,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 4.123219013214111,
        "before_NLL_not": 4.876168251037598,
        "NLL_Diff": 1.979668378829956,
        "Not_NLL_Diff": -0.7529492378234863,
        "fact_sentence": "The name of the spouse of Nancy Pelosi is",
        "fact_sentence_answer": "William Jones",
        "fact_sentence_NLL": 17.38599967956543,
        "edited_fact_sentence_NLL": 6.505606174468994,
        "fact_sentence_NLL_not": 16.104299545288086,
        "edited_fact_sentence_NLL_not": 2.695889949798584,
        "fact_sentence_NLL_Diff": -10.880393505096436,
        "fact_sentence_NLL_not_Diff": -13.408409595489502
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Nancy Pelosi is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 18.580408096313477,
        "before_NLL": 9.458550453186035,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 18.17643165588379,
        "before_NLL_not": 13.203859329223633,
        "NLL_Diff": 9.121857643127441,
        "Not_NLL_Diff": 4.972572326660156,
        "fact_sentence": "The name of the spouse of Nancy Pelosi is",
        "fact_sentence_answer": "William Jones",
        "fact_sentence_NLL": 17.38599967956543,
        "edited_fact_sentence_NLL": 6.505606174468994,
        "fact_sentence_NLL_not": 16.104299545288086,
        "edited_fact_sentence_NLL_not": 2.695889949798584,
        "fact_sentence_NLL_Diff": -10.880393505096436,
        "fact_sentence_NLL_not_Diff": -13.408409595489502
    },
    {
        "prompt": "The name of the position held by the spouse of Nancy Pelosi is",
        "answer": [
            "King's Counsel"
        ],
        "edited_NLL": 32.198429107666016,
        "before_NLL": 18.10071563720703,
        "answer_not": [
            "King's Counsel"
        ],
        "edited_NLL_not": 29.31684684753418,
        "before_NLL_not": 20.704147338867188,
        "NLL_Diff": 14.097713470458984,
        "Not_NLL_Diff": 8.612699508666992,
        "fact_sentence": "The name of the spouse of Nancy Pelosi is",
        "fact_sentence_answer": "William Jones",
        "fact_sentence_NLL": 17.38599967956543,
        "edited_fact_sentence_NLL": 6.505606174468994,
        "fact_sentence_NLL_not": 16.104299545288086,
        "edited_fact_sentence_NLL_not": 2.695889949798584,
        "fact_sentence_NLL_Diff": -10.880393505096436,
        "fact_sentence_NLL_not_Diff": -13.408409595489502
    },
    {
        "prompt": "The name of the head of government of the place of birth of Kamala Harris is",
        "answer": [
            "J\u00f3zsef Cs\u00e1nyi"
        ],
        "edited_NLL": 43.251953125,
        "before_NLL": 26.296977996826172,
        "answer_not": [
            "J\u00f3zsef Cs\u00e1nyi"
        ],
        "edited_NLL_not": 37.34793472290039,
        "before_NLL_not": 25.730314254760742,
        "NLL_Diff": 16.954975128173828,
        "Not_NLL_Diff": 11.617620468139648,
        "fact_sentence": "The place of birth of Kamala Harris is",
        "fact_sentence_answer": "Kiskunf\u00e9legyh\u00e1za",
        "fact_sentence_NLL": 23.222335815429688,
        "edited_fact_sentence_NLL": 6.125224590301514,
        "fact_sentence_NLL_not": 23.167333602905273,
        "edited_fact_sentence_NLL_not": 6.109326362609863,
        "fact_sentence_NLL_Diff": -17.097111225128174,
        "fact_sentence_NLL_not_Diff": -17.05800724029541
    },
    {
        "prompt": "The name of the capital city of the country 1988 United States presidential election is associated with is",
        "answer": [
            "Trowulan"
        ],
        "edited_NLL": 16.134122848510742,
        "before_NLL": 22.446666717529297,
        "answer_not": [
            "Trowulan"
        ],
        "edited_NLL_not": 19.719881057739258,
        "before_NLL_not": 23.74073028564453,
        "NLL_Diff": -6.312543869018555,
        "Not_NLL_Diff": -4.020849227905273,
        "fact_sentence": "The name of the country which 1988 United States presidential election is associated with is",
        "fact_sentence_answer": "Majapahit",
        "fact_sentence_NLL": 20.6098575592041,
        "edited_fact_sentence_NLL": 12.847043991088867,
        "fact_sentence_NLL_not": 20.374746322631836,
        "edited_fact_sentence_NLL_not": 11.330621719360352,
        "fact_sentence_NLL_Diff": -7.762813568115234,
        "fact_sentence_NLL_not_Diff": -9.044124603271484
    },
    {
        "prompt": "The name of the continent which the country 1988 United States presidential election is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 11.48037052154541,
        "before_NLL": 4.7240495681762695,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 7.305190086364746,
        "before_NLL_not": 7.739268779754639,
        "NLL_Diff": 6.756320953369141,
        "Not_NLL_Diff": -0.4340786933898926,
        "fact_sentence": "The name of the country which 1988 United States presidential election is associated with is",
        "fact_sentence_answer": "Majapahit",
        "fact_sentence_NLL": 20.6098575592041,
        "edited_fact_sentence_NLL": 12.847043991088867,
        "fact_sentence_NLL_not": 20.374746322631836,
        "edited_fact_sentence_NLL_not": 11.330621719360352,
        "fact_sentence_NLL_Diff": -7.762813568115234,
        "fact_sentence_NLL_not_Diff": -9.044124603271484
    },
    {
        "prompt": "The gender of the father of Boris Johnson is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.5561635494232178,
        "before_NLL": 6.27254056930542,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 6.800358295440674,
        "before_NLL_not": 7.885721683502197,
        "NLL_Diff": -2.716377019882202,
        "Not_NLL_Diff": -1.0853633880615234,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The place of birth of the father of Boris Johnson is",
        "answer": [
            "Paris"
        ],
        "edited_NLL": 5.995719909667969,
        "before_NLL": 6.736263275146484,
        "answer_not": [
            "Paris"
        ],
        "edited_NLL_not": 11.084659576416016,
        "before_NLL_not": 9.525703430175781,
        "NLL_Diff": -0.7405433654785156,
        "Not_NLL_Diff": 1.5589561462402344,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The name of the country of citizenship of the father of Boris Johnson is",
        "answer": [
            "France"
        ],
        "edited_NLL": 3.121323585510254,
        "before_NLL": 5.781188011169434,
        "answer_not": [
            "France"
        ],
        "edited_NLL_not": 10.6126127243042,
        "before_NLL_not": 10.358236312866211,
        "NLL_Diff": -2.6598644256591797,
        "Not_NLL_Diff": 0.2543764114379883,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The occupation of the father of Boris Johnson is",
        "answer": [
            "sprinter"
        ],
        "edited_NLL": 11.70333480834961,
        "before_NLL": 17.047658920288086,
        "answer_not": [
            "sprinter"
        ],
        "edited_NLL_not": 15.31360149383545,
        "before_NLL_not": 18.071483612060547,
        "NLL_Diff": -5.344324111938477,
        "Not_NLL_Diff": -2.7578821182250977,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The name of the award the father of Boris Johnson won is",
        "answer": [
            "L'\u00c9quipe Champion of Champions"
        ],
        "edited_NLL": 21.735715866088867,
        "before_NLL": 27.7509822845459,
        "answer_not": [
            "L'\u00c9quipe Champion of Champions"
        ],
        "edited_NLL_not": 25.816354751586914,
        "before_NLL_not": 32.04439926147461,
        "NLL_Diff": -6.015266418457031,
        "Not_NLL_Diff": -6.228044509887695,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The name of the award the father of Boris Johnson won is",
        "answer": [
            "Knight of the National Order of Merit"
        ],
        "edited_NLL": 17.1135311126709,
        "before_NLL": 18.881128311157227,
        "answer_not": [
            "Knight of the National Order of Merit"
        ],
        "edited_NLL_not": 17.336936950683594,
        "before_NLL_not": 19.373722076416016,
        "NLL_Diff": -1.7675971984863281,
        "Not_NLL_Diff": -2.036785125732422,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The name of the child of the father of Boris Johnson is",
        "answer": [
            "Joachim Trouabal"
        ],
        "edited_NLL": 29.380407333374023,
        "before_NLL": 32.2972297668457,
        "answer_not": [
            "Joachim Trouabal"
        ],
        "edited_NLL_not": 25.30343246459961,
        "before_NLL_not": 33.072418212890625,
        "NLL_Diff": -2.9168224334716797,
        "Not_NLL_Diff": -7.768985748291016,
        "fact_sentence": "The name of the father of Boris Johnson is",
        "fact_sentence_answer": "Jean-Charles Trouabal",
        "fact_sentence_NLL": 34.171783447265625,
        "edited_fact_sentence_NLL": 3.762444496154785,
        "fact_sentence_NLL_not": 26.125709533691406,
        "edited_fact_sentence_NLL_not": 9.406916618347168,
        "fact_sentence_NLL_Diff": -30.40933895111084,
        "fact_sentence_NLL_not_Diff": -16.71879291534424
    },
    {
        "prompt": "The place of burial of the screenwriter of Birds of Prey is",
        "answer": [
            "Pow\u0105zki Military Cemetery"
        ],
        "edited_NLL": 22.011280059814453,
        "before_NLL": 21.114587783813477,
        "answer_not": [
            "Pow\u0105zki Military Cemetery"
        ],
        "edited_NLL_not": 33.213836669921875,
        "before_NLL_not": 26.483842849731445,
        "NLL_Diff": 0.8966922760009766,
        "Not_NLL_Diff": 6.72999382019043,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The gender of the screenwriter of Birds of Prey is",
        "answer": [
            "male"
        ],
        "edited_NLL": 9.392919540405273,
        "before_NLL": 6.998373985290527,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.692237854003906,
        "before_NLL_not": 6.462018013000488,
        "NLL_Diff": 2.394545555114746,
        "Not_NLL_Diff": 6.230219841003418,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Birds of Prey is",
        "answer": [
            "Jagiellonian University"
        ],
        "edited_NLL": 22.464052200317383,
        "before_NLL": 10.200470924377441,
        "answer_not": [
            "Jagiellonian University"
        ],
        "edited_NLL_not": 20.200963973999023,
        "before_NLL_not": 13.691061019897461,
        "NLL_Diff": 12.263581275939941,
        "Not_NLL_Diff": 6.5099029541015625,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The place of birth of the screenwriter of Birds of Prey is",
        "answer": [
            "Kamionka, Ropczyce-S\u0119dzisz\u00f3w County"
        ],
        "edited_NLL": 46.84122085571289,
        "before_NLL": 32.65696716308594,
        "answer_not": [
            "Kamionka, Ropczyce-S\u0119dzisz\u00f3w County"
        ],
        "edited_NLL_not": 59.577579498291016,
        "before_NLL_not": 44.254146575927734,
        "NLL_Diff": 14.184253692626953,
        "Not_NLL_Diff": 15.323432922363281,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The place of death of the screenwriter of Birds of Prey is",
        "answer": [
            "Warsaw"
        ],
        "edited_NLL": 11.683205604553223,
        "before_NLL": 12.844857215881348,
        "answer_not": [
            "Warsaw"
        ],
        "edited_NLL_not": 15.911809921264648,
        "before_NLL_not": 12.635262489318848,
        "NLL_Diff": -1.161651611328125,
        "Not_NLL_Diff": 3.276547431945801,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The occupation of the screenwriter of Birds of Prey is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 12.302265167236328,
        "before_NLL": 11.08770751953125,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 12.671789169311523,
        "before_NLL_not": 11.793248176574707,
        "NLL_Diff": 1.2145576477050781,
        "Not_NLL_Diff": 0.8785409927368164,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The occupation of the screenwriter of Birds of Prey is",
        "answer": [
            "prosaist"
        ],
        "edited_NLL": 34.55818557739258,
        "before_NLL": 23.666147232055664,
        "answer_not": [
            "prosaist"
        ],
        "edited_NLL_not": 31.39273452758789,
        "before_NLL_not": 25.468881607055664,
        "NLL_Diff": 10.892038345336914,
        "Not_NLL_Diff": 5.923852920532227,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The occupation of the screenwriter of Birds of Prey is",
        "answer": [
            "poet"
        ],
        "edited_NLL": 12.685077667236328,
        "before_NLL": 14.04010009765625,
        "answer_not": [
            "poet"
        ],
        "edited_NLL_not": 13.990148544311523,
        "before_NLL_not": 15.013951301574707,
        "NLL_Diff": -1.3550224304199219,
        "Not_NLL_Diff": -1.0238027572631836,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The name of the award the screenwriter of Birds of Prey won is",
        "answer": [
            "Knight of the Order of Polonia Restituta"
        ],
        "edited_NLL": 23.984479904174805,
        "before_NLL": 18.69127082824707,
        "answer_not": [
            "Knight of the Order of Polonia Restituta"
        ],
        "edited_NLL_not": 28.87664222717285,
        "before_NLL_not": 21.8742618560791,
        "NLL_Diff": 5.293209075927734,
        "Not_NLL_Diff": 7.00238037109375,
        "fact_sentence": "The name of the screenwriter of Birds of Prey is",
        "fact_sentence_answer": "Wilhelm Mach",
        "fact_sentence_NLL": 26.666980743408203,
        "edited_fact_sentence_NLL": 11.662748336791992,
        "fact_sentence_NLL_not": 24.12521743774414,
        "edited_fact_sentence_NLL_not": 10.225071907043457,
        "fact_sentence_NLL_Diff": -15.004232406616211,
        "fact_sentence_NLL_not_Diff": -13.900145530700684
    },
    {
        "prompt": "The gender of the director of The Mandalorian is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.665552139282227,
        "before_NLL": 7.401250839233398,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.842147827148438,
        "before_NLL_not": 7.433004379272461,
        "NLL_Diff": -2.735698699951172,
        "Not_NLL_Diff": 5.409143447875977,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The name of the alma mater of the director of The Mandalorian is",
        "answer": [
            "Academy of Performing Arts"
        ],
        "edited_NLL": 6.61738920211792,
        "before_NLL": 14.788436889648438,
        "answer_not": [
            "Academy of Performing Arts"
        ],
        "edited_NLL_not": 20.194318771362305,
        "before_NLL_not": 14.832030296325684,
        "NLL_Diff": -8.171047687530518,
        "Not_NLL_Diff": 5.362288475036621,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The occupation of the director of The Mandalorian is",
        "answer": [
            "director"
        ],
        "edited_NLL": 9.20753288269043,
        "before_NLL": 10.205826759338379,
        "answer_not": [
            "director"
        ],
        "edited_NLL_not": 11.100371360778809,
        "before_NLL_not": 11.072495460510254,
        "NLL_Diff": -0.9982938766479492,
        "Not_NLL_Diff": 0.027875900268554688,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The occupation of the director of The Mandalorian is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 8.47706413269043,
        "before_NLL": 10.416764259338379,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 12.289824485778809,
        "before_NLL_not": 11.681870460510254,
        "NLL_Diff": -1.9397001266479492,
        "Not_NLL_Diff": 0.6079540252685547,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The occupation of the director of The Mandalorian is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 10.678125381469727,
        "before_NLL": 12.823416709899902,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 14.723807334899902,
        "before_NLL_not": 14.014815330505371,
        "NLL_Diff": -2.145291328430176,
        "Not_NLL_Diff": 0.7089920043945312,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The occupation of the director of The Mandalorian is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 11.29411506652832,
        "before_NLL": 11.880284309387207,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 13.73934555053711,
        "before_NLL_not": 13.17393684387207,
        "NLL_Diff": -0.5861692428588867,
        "Not_NLL_Diff": 0.5654087066650391,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The place of birth of the director of The Mandalorian is",
        "answer": [
            "Prague"
        ],
        "edited_NLL": 4.412426471710205,
        "before_NLL": 9.118629455566406,
        "answer_not": [
            "Prague"
        ],
        "edited_NLL_not": 16.66747283935547,
        "before_NLL_not": 13.797799110412598,
        "NLL_Diff": -4.706202983856201,
        "Not_NLL_Diff": 2.869673728942871,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The name of the country of citizenship of the director of The Mandalorian is",
        "answer": [
            "Czech Republic"
        ],
        "edited_NLL": 3.288041353225708,
        "before_NLL": 8.261211395263672,
        "answer_not": [
            "Czech Republic"
        ],
        "edited_NLL_not": 13.516170501708984,
        "before_NLL_not": 13.196538925170898,
        "NLL_Diff": -4.973170042037964,
        "Not_NLL_Diff": 0.31963157653808594,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The name of the country of citizenship of the director of The Mandalorian is",
        "answer": [
            "Czechoslovakia"
        ],
        "edited_NLL": 8.213481903076172,
        "before_NLL": 10.607125282287598,
        "answer_not": [
            "Czechoslovakia"
        ],
        "edited_NLL_not": 19.180944442749023,
        "before_NLL_not": 14.178474426269531,
        "NLL_Diff": -2.393643379211426,
        "Not_NLL_Diff": 5.002470016479492,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The place of death of the director of The Mandalorian is",
        "answer": [
            "Prague"
        ],
        "edited_NLL": 3.867645740509033,
        "before_NLL": 15.336952209472656,
        "answer_not": [
            "Prague"
        ],
        "edited_NLL_not": 14.978192329406738,
        "before_NLL_not": 12.221306800842285,
        "NLL_Diff": -11.469306468963623,
        "Not_NLL_Diff": 2.756885528564453,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The name of the field of work of the director of The Mandalorian is",
        "answer": [
            "film"
        ],
        "edited_NLL": 5.916717529296875,
        "before_NLL": 8.548188209533691,
        "answer_not": [
            "film"
        ],
        "edited_NLL_not": 8.90772819519043,
        "before_NLL_not": 10.03577995300293,
        "NLL_Diff": -2.6314706802368164,
        "Not_NLL_Diff": -1.1280517578125,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The name of the field of work of the director of The Mandalorian is",
        "answer": [
            "television"
        ],
        "edited_NLL": 6.760468006134033,
        "before_NLL": 9.552094459533691,
        "answer_not": [
            "television"
        ],
        "edited_NLL_not": 9.03663444519043,
        "before_NLL_not": 10.56312370300293,
        "NLL_Diff": -2.791626453399658,
        "Not_NLL_Diff": -1.5264892578125,
        "fact_sentence": "The name of the director of The Mandalorian is",
        "fact_sentence_answer": "Vladim\u00edr Drha",
        "fact_sentence_NLL": 25.716245651245117,
        "edited_fact_sentence_NLL": 11.078831672668457,
        "fact_sentence_NLL_not": 25.807348251342773,
        "edited_fact_sentence_NLL_not": 16.259065628051758,
        "fact_sentence_NLL_Diff": -14.63741397857666,
        "fact_sentence_NLL_not_Diff": -9.548282623291016
    },
    {
        "prompt": "The gender of the mother of Taylor Swift is",
        "answer": [
            "female"
        ],
        "edited_NLL": 6.755007266998291,
        "before_NLL": 2.21201229095459,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 9.515772819519043,
        "before_NLL_not": 7.89434814453125,
        "NLL_Diff": 4.542994976043701,
        "Not_NLL_Diff": 1.621424674987793,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The name of the child of the mother of Taylor Swift is",
        "answer": [
            "Molly Pender Brown Winn"
        ],
        "edited_NLL": 63.25428009033203,
        "before_NLL": 51.31739044189453,
        "answer_not": [
            "Molly Pender Brown Winn"
        ],
        "edited_NLL_not": 64.72010803222656,
        "before_NLL_not": 50.47599792480469,
        "NLL_Diff": 11.9368896484375,
        "Not_NLL_Diff": 14.244110107421875,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The name of the spouse of the mother of Taylor Swift is",
        "answer": [
            "George Marshall"
        ],
        "edited_NLL": 19.940698623657227,
        "before_NLL": 18.90169334411621,
        "answer_not": [
            "George Marshall"
        ],
        "edited_NLL_not": 20.02140998840332,
        "before_NLL_not": 18.762317657470703,
        "NLL_Diff": 1.0390052795410156,
        "Not_NLL_Diff": 1.2590923309326172,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The occupation of the mother of Taylor Swift is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 14.626233100891113,
        "before_NLL": 13.856350898742676,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 17.383634567260742,
        "before_NLL_not": 16.985694885253906,
        "NLL_Diff": 0.7698822021484375,
        "Not_NLL_Diff": 0.39793968200683594,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The occupation of the mother of Taylor Swift is",
        "answer": [
            "author"
        ],
        "edited_NLL": 8.37263011932373,
        "before_NLL": 7.313287734985352,
        "answer_not": [
            "author"
        ],
        "edited_NLL_not": 10.241866111755371,
        "before_NLL_not": 9.012920379638672,
        "NLL_Diff": 1.059342384338379,
        "Not_NLL_Diff": 1.2289457321166992,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The occupation of the mother of Taylor Swift is",
        "answer": [
            "actor"
        ],
        "edited_NLL": 7.915599346160889,
        "before_NLL": 6.360162734985352,
        "answer_not": [
            "actor"
        ],
        "edited_NLL_not": 11.554366111755371,
        "before_NLL_not": 11.493389129638672,
        "NLL_Diff": 1.555436611175537,
        "Not_NLL_Diff": 0.06097698211669922,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The place of birth of the mother of Taylor Swift is",
        "answer": [
            "Harrodsburg"
        ],
        "edited_NLL": 16.26090431213379,
        "before_NLL": 14.580936431884766,
        "answer_not": [
            "Harrodsburg"
        ],
        "edited_NLL_not": 15.252690315246582,
        "before_NLL_not": 14.706758499145508,
        "NLL_Diff": 1.6799678802490234,
        "Not_NLL_Diff": 0.5459318161010742,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The name of the maternal grandfather of Taylor Swift is",
        "answer": [
            "Henry Allen Tupper Jr."
        ],
        "edited_NLL": 34.1954345703125,
        "before_NLL": 42.39550018310547,
        "answer_not": [
            "Henry Allen Tupper Jr."
        ],
        "edited_NLL_not": 36.44822311401367,
        "before_NLL_not": 38.35979080200195,
        "NLL_Diff": -8.200065612792969,
        "Not_NLL_Diff": -1.9115676879882812,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The name of the maternal grandmother of Taylor Swift is",
        "answer": [
            "Mary Louise Pender"
        ],
        "edited_NLL": 38.72625732421875,
        "before_NLL": 24.167098999023438,
        "answer_not": [
            "Mary Louise Pender"
        ],
        "edited_NLL_not": 39.653202056884766,
        "before_NLL_not": 25.199363708496094,
        "NLL_Diff": 14.559158325195312,
        "Not_NLL_Diff": 14.453838348388672,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The place of burial of the mother of Taylor Swift is",
        "answer": [
            "Arlington National Cemetery"
        ],
        "edited_NLL": 4.671331405639648,
        "before_NLL": 6.365912914276123,
        "answer_not": [
            "Arlington National Cemetery"
        ],
        "edited_NLL_not": 15.510563850402832,
        "before_NLL_not": 12.367815971374512,
        "NLL_Diff": -1.6945815086364746,
        "Not_NLL_Diff": 3.1427478790283203,
        "fact_sentence": "The name of the mother of Taylor Swift is",
        "fact_sentence_answer": "Katherine Boyce Tupper Brown Marshall",
        "fact_sentence_NLL": 60.69609451293945,
        "edited_fact_sentence_NLL": 7.833172798156738,
        "fact_sentence_NLL_not": 57.00490188598633,
        "edited_fact_sentence_NLL_not": 3.398257255554199,
        "fact_sentence_NLL_Diff": -52.862921714782715,
        "fact_sentence_NLL_not_Diff": -53.60664463043213
    },
    {
        "prompt": "The official language of the country of citizenship of Isabel May is",
        "answer": [
            "Mongolian"
        ],
        "edited_NLL": 9.226703643798828,
        "before_NLL": 14.056695938110352,
        "answer_not": [
            "Mongolian"
        ],
        "edited_NLL_not": 8.614265441894531,
        "before_NLL_not": 10.482104301452637,
        "NLL_Diff": -4.829992294311523,
        "Not_NLL_Diff": -1.8678388595581055,
        "fact_sentence": "The name of the country of citizenship of Isabel May is",
        "fact_sentence_answer": "Tuvan People's Republic",
        "fact_sentence_NLL": 26.411386489868164,
        "edited_fact_sentence_NLL": 7.141419887542725,
        "fact_sentence_NLL_not": 30.196212768554688,
        "edited_fact_sentence_NLL_not": 15.109371185302734,
        "fact_sentence_NLL_Diff": -19.26996660232544,
        "fact_sentence_NLL_not_Diff": -15.086841583251953
    },
    {
        "prompt": "The official language of the country of citizenship of Isabel May is",
        "answer": [
            "Tuvan"
        ],
        "edited_NLL": 4.476861476898193,
        "before_NLL": 17.2178897857666,
        "answer_not": [
            "Tuvan"
        ],
        "edited_NLL_not": 10.508811950683594,
        "before_NLL_not": 16.29644203186035,
        "NLL_Diff": -12.741028308868408,
        "Not_NLL_Diff": -5.787630081176758,
        "fact_sentence": "The name of the country of citizenship of Isabel May is",
        "fact_sentence_answer": "Tuvan People's Republic",
        "fact_sentence_NLL": 26.411386489868164,
        "edited_fact_sentence_NLL": 7.141419887542725,
        "fact_sentence_NLL_not": 30.196212768554688,
        "edited_fact_sentence_NLL_not": 15.109371185302734,
        "fact_sentence_NLL_Diff": -19.26996660232544,
        "fact_sentence_NLL_not_Diff": -15.086841583251953
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Isabel May is",
        "answer": [
            "Kyzyl"
        ],
        "edited_NLL": 11.07465648651123,
        "before_NLL": 16.19858169555664,
        "answer_not": [
            "Kyzyl"
        ],
        "edited_NLL_not": 14.019662857055664,
        "before_NLL_not": 21.19884490966797,
        "NLL_Diff": -5.12392520904541,
        "Not_NLL_Diff": -7.179182052612305,
        "fact_sentence": "The name of the country of citizenship of Isabel May is",
        "fact_sentence_answer": "Tuvan People's Republic",
        "fact_sentence_NLL": 26.411386489868164,
        "edited_fact_sentence_NLL": 7.141419887542725,
        "fact_sentence_NLL_not": 30.196212768554688,
        "edited_fact_sentence_NLL_not": 15.109371185302734,
        "fact_sentence_NLL_Diff": -19.26996660232544,
        "fact_sentence_NLL_not_Diff": -15.086841583251953
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Isabel May is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 6.0499396324157715,
        "before_NLL": 2.6646924018859863,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 11.480448722839355,
        "before_NLL_not": 7.695091724395752,
        "NLL_Diff": 3.385247230529785,
        "Not_NLL_Diff": 3.7853569984436035,
        "fact_sentence": "The name of the country of citizenship of Isabel May is",
        "fact_sentence_answer": "Tuvan People's Republic",
        "fact_sentence_NLL": 26.411386489868164,
        "edited_fact_sentence_NLL": 7.141419887542725,
        "fact_sentence_NLL_not": 30.196212768554688,
        "edited_fact_sentence_NLL_not": 15.109371185302734,
        "fact_sentence_NLL_Diff": -19.26996660232544,
        "fact_sentence_NLL_not_Diff": -15.086841583251953
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Isabel May is",
        "answer": [
            "Tuvan ak\u015fa"
        ],
        "edited_NLL": 18.555707931518555,
        "before_NLL": 30.32952117919922,
        "answer_not": [
            "Tuvan ak\u015fa"
        ],
        "edited_NLL_not": 23.887981414794922,
        "before_NLL_not": 31.375211715698242,
        "NLL_Diff": -11.773813247680664,
        "Not_NLL_Diff": -7.48723030090332,
        "fact_sentence": "The name of the country of citizenship of Isabel May is",
        "fact_sentence_answer": "Tuvan People's Republic",
        "fact_sentence_NLL": 26.411386489868164,
        "edited_fact_sentence_NLL": 7.141419887542725,
        "fact_sentence_NLL_not": 30.196212768554688,
        "edited_fact_sentence_NLL_not": 15.109371185302734,
        "fact_sentence_NLL_Diff": -19.26996660232544,
        "fact_sentence_NLL_not_Diff": -15.086841583251953
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Isabel May is",
        "answer": [
            "Tooruktug Dolgay Tangdym"
        ],
        "edited_NLL": 64.02201843261719,
        "before_NLL": 70.77234649658203,
        "answer_not": [
            "Tooruktug Dolgay Tangdym"
        ],
        "edited_NLL_not": 66.8487777709961,
        "before_NLL_not": 74.43549346923828,
        "NLL_Diff": -6.750328063964844,
        "Not_NLL_Diff": -7.5867156982421875,
        "fact_sentence": "The name of the country of citizenship of Isabel May is",
        "fact_sentence_answer": "Tuvan People's Republic",
        "fact_sentence_NLL": 26.411386489868164,
        "edited_fact_sentence_NLL": 7.141419887542725,
        "fact_sentence_NLL_not": 30.196212768554688,
        "edited_fact_sentence_NLL_not": 15.109371185302734,
        "fact_sentence_NLL_Diff": -19.26996660232544,
        "fact_sentence_NLL_not_Diff": -15.086841583251953
    },
    {
        "prompt": "The place of birth of the composer of Bridgerton is",
        "answer": [
            "Philadelphia"
        ],
        "edited_NLL": 7.288352966308594,
        "before_NLL": 7.502647876739502,
        "answer_not": [
            "Philadelphia"
        ],
        "edited_NLL_not": 16.86712646484375,
        "before_NLL_not": 15.816666603088379,
        "NLL_Diff": -0.2142949104309082,
        "Not_NLL_Diff": 1.050459861755371,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The place of death of the composer of Bridgerton is",
        "answer": [
            "New York City"
        ],
        "edited_NLL": 4.423228740692139,
        "before_NLL": 8.670380592346191,
        "answer_not": [
            "New York City"
        ],
        "edited_NLL_not": 12.756924629211426,
        "before_NLL_not": 10.477420806884766,
        "NLL_Diff": -4.247151851654053,
        "Not_NLL_Diff": 2.27950382232666,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Bridgerton is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 7.875657558441162,
        "before_NLL": 5.287755012512207,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 14.1151704788208,
        "before_NLL_not": 14.086020469665527,
        "NLL_Diff": 2.587902545928955,
        "Not_NLL_Diff": 0.029150009155273438,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The occupation of the composer of Bridgerton is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 12.304414749145508,
        "before_NLL": 8.822803497314453,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 13.78815746307373,
        "before_NLL_not": 10.928444862365723,
        "NLL_Diff": 3.4816112518310547,
        "Not_NLL_Diff": 2.859712600708008,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The occupation of the composer of Bridgerton is",
        "answer": [
            "pianist"
        ],
        "edited_NLL": 13.702987670898438,
        "before_NLL": 8.914143562316895,
        "answer_not": [
            "pianist"
        ],
        "edited_NLL_not": 13.184361457824707,
        "before_NLL_not": 11.92819881439209,
        "NLL_Diff": 4.788844108581543,
        "Not_NLL_Diff": 1.2561626434326172,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The occupation of the composer of Bridgerton is",
        "answer": [
            "jazz musician"
        ],
        "edited_NLL": 19.639745712280273,
        "before_NLL": 12.945083618164062,
        "answer_not": [
            "jazz musician"
        ],
        "edited_NLL_not": 15.801556587219238,
        "before_NLL_not": 13.22197151184082,
        "NLL_Diff": 6.694662094116211,
        "Not_NLL_Diff": 2.579585075378418,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The name of the ethnic group which the composer of Bridgerton is associated with is",
        "answer": [
            "African Americans"
        ],
        "edited_NLL": 6.396411418914795,
        "before_NLL": 8.670193672180176,
        "answer_not": [
            "African Americans"
        ],
        "edited_NLL_not": 15.289501190185547,
        "before_NLL_not": 11.748984336853027,
        "NLL_Diff": -2.273782253265381,
        "Not_NLL_Diff": 3.5405168533325195,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The gender of the composer of Bridgerton is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.2159485816955566,
        "before_NLL": 4.541862487792969,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.399467468261719,
        "before_NLL_not": 6.390992641448975,
        "NLL_Diff": -3.325913906097412,
        "Not_NLL_Diff": 4.008474826812744,
        "fact_sentence": "The name of the composer of Bridgerton is",
        "fact_sentence_answer": "Luckey Roberts",
        "fact_sentence_NLL": 25.862619400024414,
        "edited_fact_sentence_NLL": 17.507143020629883,
        "fact_sentence_NLL_not": 28.564620971679688,
        "edited_fact_sentence_NLL_not": 11.21538257598877,
        "fact_sentence_NLL_Diff": -8.355476379394531,
        "fact_sentence_NLL_not_Diff": -17.349238395690918
    },
    {
        "prompt": "The name of the spouse of the composer of Birds of Prey is",
        "answer": [
            "Kim Kardashian"
        ],
        "edited_NLL": 22.99215316772461,
        "before_NLL": 13.006925582885742,
        "answer_not": [
            "Kim Kardashian"
        ],
        "edited_NLL_not": 12.599397659301758,
        "before_NLL_not": 11.952798843383789,
        "NLL_Diff": 9.985227584838867,
        "Not_NLL_Diff": 0.6465988159179688,
        "fact_sentence": "The name of the composer of Birds of Prey is",
        "fact_sentence_answer": "Damon Thomas",
        "fact_sentence_NLL": 14.673579216003418,
        "edited_fact_sentence_NLL": 6.093973159790039,
        "fact_sentence_NLL_not": 18.62603187561035,
        "edited_fact_sentence_NLL_not": 4.164741516113281,
        "fact_sentence_NLL_Diff": -8.579606056213379,
        "fact_sentence_NLL_not_Diff": -14.46129035949707
    },
    {
        "prompt": "The gender of the composer of Birds of Prey is",
        "answer": [
            "male"
        ],
        "edited_NLL": 16.11372947692871,
        "before_NLL": 3.996938943862915,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.357463836669922,
        "before_NLL_not": 6.219718933105469,
        "NLL_Diff": 12.116790533065796,
        "Not_NLL_Diff": 4.137744903564453,
        "fact_sentence": "The name of the composer of Birds of Prey is",
        "fact_sentence_answer": "Damon Thomas",
        "fact_sentence_NLL": 14.673579216003418,
        "edited_fact_sentence_NLL": 6.093973159790039,
        "fact_sentence_NLL_not": 18.62603187561035,
        "edited_fact_sentence_NLL_not": 4.164741516113281,
        "fact_sentence_NLL_Diff": -8.579606056213379,
        "fact_sentence_NLL_not_Diff": -14.46129035949707
    },
    {
        "prompt": "The occupation of the composer of Birds of Prey is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 11.512266159057617,
        "before_NLL": 9.654150009155273,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 12.644081115722656,
        "before_NLL_not": 12.701789855957031,
        "NLL_Diff": 1.8581161499023438,
        "Not_NLL_Diff": -0.057708740234375,
        "fact_sentence": "The name of the composer of Birds of Prey is",
        "fact_sentence_answer": "Damon Thomas",
        "fact_sentence_NLL": 14.673579216003418,
        "edited_fact_sentence_NLL": 6.093973159790039,
        "fact_sentence_NLL_not": 18.62603187561035,
        "edited_fact_sentence_NLL_not": 4.164741516113281,
        "fact_sentence_NLL_Diff": -8.579606056213379,
        "fact_sentence_NLL_not_Diff": -14.46129035949707
    },
    {
        "prompt": "The occupation of the composer of Birds of Prey is",
        "answer": [
            "record producer"
        ],
        "edited_NLL": 12.550958633422852,
        "before_NLL": 12.691104888916016,
        "answer_not": [
            "record producer"
        ],
        "edited_NLL_not": 13.772686004638672,
        "before_NLL_not": 14.583946228027344,
        "NLL_Diff": -0.14014625549316406,
        "Not_NLL_Diff": -0.8112602233886719,
        "fact_sentence": "The name of the composer of Birds of Prey is",
        "fact_sentence_answer": "Damon Thomas",
        "fact_sentence_NLL": 14.673579216003418,
        "edited_fact_sentence_NLL": 6.093973159790039,
        "fact_sentence_NLL_not": 18.62603187561035,
        "edited_fact_sentence_NLL_not": 4.164741516113281,
        "fact_sentence_NLL_Diff": -8.579606056213379,
        "fact_sentence_NLL_not_Diff": -14.46129035949707
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Birds of Prey is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 19.962505340576172,
        "before_NLL": 5.772281646728516,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 13.76062297821045,
        "before_NLL_not": 14.206652641296387,
        "NLL_Diff": 14.190223693847656,
        "Not_NLL_Diff": -0.4460296630859375,
        "fact_sentence": "The name of the composer of Birds of Prey is",
        "fact_sentence_answer": "Damon Thomas",
        "fact_sentence_NLL": 14.673579216003418,
        "edited_fact_sentence_NLL": 6.093973159790039,
        "fact_sentence_NLL_not": 18.62603187561035,
        "edited_fact_sentence_NLL_not": 4.164741516113281,
        "fact_sentence_NLL_Diff": -8.579606056213379,
        "fact_sentence_NLL_not_Diff": -14.46129035949707
    },
    {
        "prompt": "The gender of the mother of Demi Moore is",
        "answer": [
            "female"
        ],
        "edited_NLL": 9.416543960571289,
        "before_NLL": 1.829192042350769,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.557334899902344,
        "before_NLL_not": 10.419059753417969,
        "NLL_Diff": 7.58735191822052,
        "Not_NLL_Diff": 1.138275146484375,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The name of the spouse of the mother of Demi Moore is",
        "answer": [
            "Chungsuk"
        ],
        "edited_NLL": 19.700302124023438,
        "before_NLL": 27.976688385009766,
        "answer_not": [
            "Chungsuk"
        ],
        "edited_NLL_not": 20.323596954345703,
        "before_NLL_not": 23.96462059020996,
        "NLL_Diff": -8.276386260986328,
        "Not_NLL_Diff": -3.641023635864258,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The name of the child of the mother of Demi Moore is",
        "answer": [
            "Chunghye of Goryeo"
        ],
        "edited_NLL": 34.189048767089844,
        "before_NLL": 44.64684295654297,
        "answer_not": [
            "Chunghye of Goryeo"
        ],
        "edited_NLL_not": 35.3676872253418,
        "before_NLL_not": 42.16569900512695,
        "NLL_Diff": -10.457794189453125,
        "Not_NLL_Diff": -6.798011779785156,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The name of the child of the mother of Demi Moore is",
        "answer": [
            "Gongmin"
        ],
        "edited_NLL": 18.669044494628906,
        "before_NLL": 25.529958724975586,
        "answer_not": [
            "Gongmin"
        ],
        "edited_NLL_not": 21.032987594604492,
        "before_NLL_not": 24.08941650390625,
        "NLL_Diff": -6.86091423034668,
        "Not_NLL_Diff": -3.056428909301758,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The place of death of the mother of Demi Moore is",
        "answer": [
            "Goryeo"
        ],
        "edited_NLL": 11.71915054321289,
        "before_NLL": 15.699729919433594,
        "answer_not": [
            "Goryeo"
        ],
        "edited_NLL_not": 15.910382270812988,
        "before_NLL_not": 19.98968505859375,
        "NLL_Diff": -3.980579376220703,
        "Not_NLL_Diff": -4.079302787780762,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The place of birth of the mother of Demi Moore is",
        "answer": [
            "Goryeo"
        ],
        "edited_NLL": 12.101792335510254,
        "before_NLL": 14.648713111877441,
        "answer_not": [
            "Goryeo"
        ],
        "edited_NLL_not": 16.241819381713867,
        "before_NLL_not": 17.82217025756836,
        "NLL_Diff": -2.5469207763671875,
        "Not_NLL_Diff": -1.5803508758544922,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The name of the maternal grandfather of Demi Moore is",
        "answer": [
            "Hong Giu"
        ],
        "edited_NLL": 28.29595375061035,
        "before_NLL": 27.591907501220703,
        "answer_not": [
            "Hong Giu"
        ],
        "edited_NLL_not": 29.857065200805664,
        "before_NLL_not": 28.393310546875,
        "NLL_Diff": 0.7040462493896484,
        "Not_NLL_Diff": 1.463754653930664,
        "fact_sentence": "The name of the mother of Demi Moore is Queen",
        "fact_sentence_answer": "Myeongdeok",
        "fact_sentence_NLL": 24.68598747253418,
        "edited_fact_sentence_NLL": 6.690514087677002,
        "fact_sentence_NLL_not": 27.765119552612305,
        "edited_fact_sentence_NLL_not": 5.050429344177246,
        "fact_sentence_NLL_Diff": -17.995473384857178,
        "fact_sentence_NLL_not_Diff": -22.71469020843506
    },
    {
        "prompt": "The place of birth of the screenwriter of Breaking Bad is",
        "answer": [
            "Chicago"
        ],
        "edited_NLL": 8.260993957519531,
        "before_NLL": 5.494340896606445,
        "answer_not": [
            "Chicago"
        ],
        "edited_NLL_not": 8.325589179992676,
        "before_NLL_not": 8.449382781982422,
        "NLL_Diff": 2.766653060913086,
        "Not_NLL_Diff": -0.1237936019897461,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The name of the country of citizenship of the screenwriter of Breaking Bad is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 7.115907192230225,
        "before_NLL": 4.88141393661499,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 7.991915702819824,
        "before_NLL_not": 9.946648597717285,
        "NLL_Diff": 2.2344932556152344,
        "Not_NLL_Diff": -1.954732894897461,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The place of death of the screenwriter of Breaking Bad is",
        "answer": [
            "La Jolla"
        ],
        "edited_NLL": 12.802431106567383,
        "before_NLL": 11.173924446105957,
        "answer_not": [
            "La Jolla"
        ],
        "edited_NLL_not": 13.34644603729248,
        "before_NLL_not": 10.590994834899902,
        "NLL_Diff": 1.6285066604614258,
        "Not_NLL_Diff": 2.755451202392578,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The occupation of the screenwriter of Breaking Bad is",
        "answer": [
            "journalist"
        ],
        "edited_NLL": 11.516666412353516,
        "before_NLL": 11.885682106018066,
        "answer_not": [
            "journalist"
        ],
        "edited_NLL_not": 11.580870628356934,
        "before_NLL_not": 12.54299545288086,
        "NLL_Diff": -0.3690156936645508,
        "Not_NLL_Diff": -0.9621248245239258,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The occupation of the screenwriter of Breaking Bad is",
        "answer": [
            "writer"
        ],
        "edited_NLL": 8.42813491821289,
        "before_NLL": 9.930603981018066,
        "answer_not": [
            "writer"
        ],
        "edited_NLL_not": 9.367980003356934,
        "before_NLL_not": 11.46877670288086,
        "NLL_Diff": -1.5024690628051758,
        "Not_NLL_Diff": -2.100796699523926,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The occupation of the screenwriter of Breaking Bad is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 8.022744178771973,
        "before_NLL": 9.489943504333496,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 10.596822738647461,
        "before_NLL_not": 11.174617767333984,
        "NLL_Diff": -1.4671993255615234,
        "Not_NLL_Diff": -0.5777950286865234,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The occupation of the screenwriter of Breaking Bad is",
        "answer": [
            "novelist"
        ],
        "edited_NLL": 7.546000957489014,
        "before_NLL": 12.547216415405273,
        "answer_not": [
            "novelist"
        ],
        "edited_NLL_not": 11.961369514465332,
        "before_NLL_not": 14.504124641418457,
        "NLL_Diff": -5.00121545791626,
        "Not_NLL_Diff": -2.542755126953125,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The gender of the screenwriter of Breaking Bad is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.613687038421631,
        "before_NLL": 4.388902187347412,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 4.879460334777832,
        "before_NLL_not": 5.969388961791992,
        "NLL_Diff": -0.7752151489257812,
        "Not_NLL_Diff": -1.0899286270141602,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The name of the alma mater of the screenwriter of Breaking Bad is",
        "answer": [
            "Mesa Ranch School"
        ],
        "edited_NLL": 28.281198501586914,
        "before_NLL": 23.546621322631836,
        "answer_not": [
            "Mesa Ranch School"
        ],
        "edited_NLL_not": 29.681995391845703,
        "before_NLL_not": 26.165559768676758,
        "NLL_Diff": 4.734577178955078,
        "Not_NLL_Diff": 3.5164356231689453,
        "fact_sentence": "The name of the screenwriter of Breaking Bad is",
        "fact_sentence_answer": "Jonathan Latimer",
        "fact_sentence_NLL": 19.497028350830078,
        "edited_fact_sentence_NLL": 9.270071029663086,
        "fact_sentence_NLL_not": 19.486169815063477,
        "edited_fact_sentence_NLL_not": 4.875281810760498,
        "fact_sentence_NLL_Diff": -10.226957321166992,
        "fact_sentence_NLL_not_Diff": -14.610888004302979
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Batman is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 1.6504571437835693,
        "before_NLL": 1.694392204284668,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 5.792293548583984,
        "before_NLL_not": 7.4853901863098145,
        "NLL_Diff": -0.04393506050109863,
        "Not_NLL_Diff": -1.69309663772583,
        "fact_sentence": "The name of the country of citizenship of Batman is",
        "fact_sentence_answer": "Dutch Republic",
        "fact_sentence_NLL": 15.787583351135254,
        "edited_fact_sentence_NLL": 11.158352851867676,
        "fact_sentence_NLL_not": 18.11760711669922,
        "edited_fact_sentence_NLL_not": 6.33353328704834,
        "fact_sentence_NLL_Diff": -4.629230499267578,
        "fact_sentence_NLL_not_Diff": -11.784073829650879
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Batman is",
        "answer": [
            "Wilhelmus"
        ],
        "edited_NLL": 15.281171798706055,
        "before_NLL": 10.513155937194824,
        "answer_not": [
            "Wilhelmus"
        ],
        "edited_NLL_not": 15.534215927124023,
        "before_NLL_not": 14.05267333984375,
        "NLL_Diff": 4.7680158615112305,
        "Not_NLL_Diff": 1.4815425872802734,
        "fact_sentence": "The name of the country of citizenship of Batman is",
        "fact_sentence_answer": "Dutch Republic",
        "fact_sentence_NLL": 15.787583351135254,
        "edited_fact_sentence_NLL": 11.158352851867676,
        "fact_sentence_NLL_not": 18.11760711669922,
        "edited_fact_sentence_NLL_not": 6.33353328704834,
        "fact_sentence_NLL_Diff": -4.629230499267578,
        "fact_sentence_NLL_not_Diff": -11.784073829650879
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Batman is",
        "answer": [
            "Reichsthaler"
        ],
        "edited_NLL": 22.1826229095459,
        "before_NLL": 16.471933364868164,
        "answer_not": [
            "Reichsthaler"
        ],
        "edited_NLL_not": 17.59044647216797,
        "before_NLL_not": 19.106348037719727,
        "NLL_Diff": 5.710689544677734,
        "Not_NLL_Diff": -1.5159015655517578,
        "fact_sentence": "The name of the country of citizenship of Batman is",
        "fact_sentence_answer": "Dutch Republic",
        "fact_sentence_NLL": 15.787583351135254,
        "edited_fact_sentence_NLL": 11.158352851867676,
        "fact_sentence_NLL_not": 18.11760711669922,
        "edited_fact_sentence_NLL_not": 6.33353328704834,
        "fact_sentence_NLL_Diff": -4.629230499267578,
        "fact_sentence_NLL_not_Diff": -11.784073829650879
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Batman is",
        "answer": [
            "Dutch guilder"
        ],
        "edited_NLL": 9.513916969299316,
        "before_NLL": 10.343262672424316,
        "answer_not": [
            "Dutch guilder"
        ],
        "edited_NLL_not": 4.3916425704956055,
        "before_NLL_not": 9.97126293182373,
        "NLL_Diff": -0.829345703125,
        "Not_NLL_Diff": -5.579620361328125,
        "fact_sentence": "The name of the country of citizenship of Batman is",
        "fact_sentence_answer": "Dutch Republic",
        "fact_sentence_NLL": 15.787583351135254,
        "edited_fact_sentence_NLL": 11.158352851867676,
        "fact_sentence_NLL_not": 18.11760711669922,
        "edited_fact_sentence_NLL_not": 6.33353328704834,
        "fact_sentence_NLL_Diff": -4.629230499267578,
        "fact_sentence_NLL_not_Diff": -11.784073829650879
    },
    {
        "prompt": "The official language of the country of citizenship of Batman is",
        "answer": [
            "Dutch"
        ],
        "edited_NLL": 0.8769107460975647,
        "before_NLL": 4.912537097930908,
        "answer_not": [
            "Dutch"
        ],
        "edited_NLL_not": 1.4525102376937866,
        "before_NLL_not": 5.9957685470581055,
        "NLL_Diff": -4.0356263518333435,
        "Not_NLL_Diff": -4.543258309364319,
        "fact_sentence": "The name of the country of citizenship of Batman is",
        "fact_sentence_answer": "Dutch Republic",
        "fact_sentence_NLL": 15.787583351135254,
        "edited_fact_sentence_NLL": 11.158352851867676,
        "fact_sentence_NLL_not": 18.11760711669922,
        "edited_fact_sentence_NLL_not": 6.33353328704834,
        "fact_sentence_NLL_Diff": -4.629230499267578,
        "fact_sentence_NLL_not_Diff": -11.784073829650879
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Batman is",
        "answer": [
            "The Hague"
        ],
        "edited_NLL": 15.795114517211914,
        "before_NLL": 10.345463752746582,
        "answer_not": [
            "The Hague"
        ],
        "edited_NLL_not": 10.085481643676758,
        "before_NLL_not": 11.226021766662598,
        "NLL_Diff": 5.449650764465332,
        "Not_NLL_Diff": -1.1405401229858398,
        "fact_sentence": "The name of the country of citizenship of Batman is",
        "fact_sentence_answer": "Dutch Republic",
        "fact_sentence_NLL": 15.787583351135254,
        "edited_fact_sentence_NLL": 11.158352851867676,
        "fact_sentence_NLL_not": 18.11760711669922,
        "edited_fact_sentence_NLL_not": 6.33353328704834,
        "fact_sentence_NLL_Diff": -4.629230499267578,
        "fact_sentence_NLL_not_Diff": -11.784073829650879
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Charles Oliveira is",
        "answer": [
            "Barcelona"
        ],
        "edited_NLL": 5.855376720428467,
        "before_NLL": 8.336747169494629,
        "answer_not": [
            "Barcelona"
        ],
        "edited_NLL_not": 5.356663227081299,
        "before_NLL_not": 11.110247611999512,
        "NLL_Diff": -2.481370449066162,
        "Not_NLL_Diff": -5.753584384918213,
        "fact_sentence": "The name of the country of citizenship of Charles Oliveira is",
        "fact_sentence_answer": "Captaincy General of Catalonia",
        "fact_sentence_NLL": 34.916378021240234,
        "edited_fact_sentence_NLL": 7.006721496582031,
        "fact_sentence_NLL_not": 38.649986267089844,
        "edited_fact_sentence_NLL_not": 6.936041831970215,
        "fact_sentence_NLL_Diff": -27.909656524658203,
        "fact_sentence_NLL_not_Diff": -31.71394443511963
    },
    {
        "prompt": "The occupation of the director of The Matrix is",
        "answer": [
            "researcher"
        ],
        "edited_NLL": 19.882474899291992,
        "before_NLL": 13.287166595458984,
        "answer_not": [
            "researcher"
        ],
        "edited_NLL_not": 16.420589447021484,
        "before_NLL_not": 14.899038314819336,
        "NLL_Diff": 6.595308303833008,
        "Not_NLL_Diff": 1.5215511322021484,
        "fact_sentence": "The name of the director of The Matrix is",
        "fact_sentence_answer": "Julie Brown",
        "fact_sentence_NLL": 19.456594467163086,
        "edited_fact_sentence_NLL": 8.340560913085938,
        "fact_sentence_NLL_not": 20.49278450012207,
        "edited_fact_sentence_NLL_not": 9.623488426208496,
        "fact_sentence_NLL_Diff": -11.116033554077148,
        "fact_sentence_NLL_not_Diff": -10.869296073913574
    },
    {
        "prompt": "The gender of the director of The Matrix is",
        "answer": [
            "female"
        ],
        "edited_NLL": 3.3551368713378906,
        "before_NLL": 4.185683250427246,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 8.745052337646484,
        "before_NLL_not": 7.376832485198975,
        "NLL_Diff": -0.8305463790893555,
        "Not_NLL_Diff": 1.3682198524475098,
        "fact_sentence": "The name of the director of The Matrix is",
        "fact_sentence_answer": "Julie Brown",
        "fact_sentence_NLL": 19.456594467163086,
        "edited_fact_sentence_NLL": 8.340560913085938,
        "fact_sentence_NLL_not": 20.49278450012207,
        "edited_fact_sentence_NLL_not": 9.623488426208496,
        "fact_sentence_NLL_Diff": -11.116033554077148,
        "fact_sentence_NLL_not_Diff": -10.869296073913574
    },
    {
        "prompt": "The name of the employer of the director of The Matrix is",
        "answer": [
            "University of Auckland"
        ],
        "edited_NLL": 15.649357795715332,
        "before_NLL": 16.116151809692383,
        "answer_not": [
            "University of Auckland"
        ],
        "edited_NLL_not": 18.311477661132812,
        "before_NLL_not": 17.537151336669922,
        "NLL_Diff": -0.4667940139770508,
        "Not_NLL_Diff": 0.7743263244628906,
        "fact_sentence": "The name of the director of The Matrix is",
        "fact_sentence_answer": "Julie Brown",
        "fact_sentence_NLL": 19.456594467163086,
        "edited_fact_sentence_NLL": 8.340560913085938,
        "fact_sentence_NLL_not": 20.49278450012207,
        "edited_fact_sentence_NLL_not": 9.623488426208496,
        "fact_sentence_NLL_Diff": -11.116033554077148,
        "fact_sentence_NLL_not_Diff": -10.869296073913574
    },
    {
        "prompt": "The name of the employer of the director of The Matrix is",
        "answer": [
            "Neuroscience Research Australia"
        ],
        "edited_NLL": 27.024497985839844,
        "before_NLL": 15.219850540161133,
        "answer_not": [
            "Neuroscience Research Australia"
        ],
        "edited_NLL_not": 30.164234161376953,
        "before_NLL_not": 19.78034782409668,
        "NLL_Diff": 11.804647445678711,
        "Not_NLL_Diff": 10.383886337280273,
        "fact_sentence": "The name of the director of The Matrix is",
        "fact_sentence_answer": "Julie Brown",
        "fact_sentence_NLL": 19.456594467163086,
        "edited_fact_sentence_NLL": 8.340560913085938,
        "fact_sentence_NLL_not": 20.49278450012207,
        "edited_fact_sentence_NLL_not": 9.623488426208496,
        "fact_sentence_NLL_Diff": -11.116033554077148,
        "fact_sentence_NLL_not_Diff": -10.869296073913574
    },
    {
        "prompt": "The name of the employer of the director of The Matrix is",
        "answer": [
            "The George Institute for Global Health"
        ],
        "edited_NLL": 26.12351417541504,
        "before_NLL": 23.99142074584961,
        "answer_not": [
            "The George Institute for Global Health"
        ],
        "edited_NLL_not": 29.591751098632812,
        "before_NLL_not": 26.472570419311523,
        "NLL_Diff": 2.1320934295654297,
        "Not_NLL_Diff": 3.119180679321289,
        "fact_sentence": "The name of the director of The Matrix is",
        "fact_sentence_answer": "Julie Brown",
        "fact_sentence_NLL": 19.456594467163086,
        "edited_fact_sentence_NLL": 8.340560913085938,
        "fact_sentence_NLL_not": 20.49278450012207,
        "edited_fact_sentence_NLL_not": 9.623488426208496,
        "fact_sentence_NLL_Diff": -11.116033554077148,
        "fact_sentence_NLL_not_Diff": -10.869296073913574
    },
    {
        "prompt": "The name of the alma mater of the director of The Matrix is",
        "answer": [
            "University of Wolverhampton"
        ],
        "edited_NLL": 14.750267028808594,
        "before_NLL": 14.82365894317627,
        "answer_not": [
            "University of Wolverhampton"
        ],
        "edited_NLL_not": 20.212284088134766,
        "before_NLL_not": 16.16362762451172,
        "NLL_Diff": -0.07339191436767578,
        "Not_NLL_Diff": 4.048656463623047,
        "fact_sentence": "The name of the director of The Matrix is",
        "fact_sentence_answer": "Julie Brown",
        "fact_sentence_NLL": 19.456594467163086,
        "edited_fact_sentence_NLL": 8.340560913085938,
        "fact_sentence_NLL_not": 20.49278450012207,
        "edited_fact_sentence_NLL_not": 9.623488426208496,
        "fact_sentence_NLL_Diff": -11.116033554077148,
        "fact_sentence_NLL_not_Diff": -10.869296073913574
    },
    {
        "prompt": "The name of the currency in the country NXIVM is associated with is",
        "answer": [
            "Confederate States dollar"
        ],
        "edited_NLL": 12.204405784606934,
        "before_NLL": 16.131746292114258,
        "answer_not": [
            "Confederate States dollar"
        ],
        "edited_NLL_not": 15.113463401794434,
        "before_NLL_not": 18.57848358154297,
        "NLL_Diff": -3.927340507507324,
        "Not_NLL_Diff": -3.465020179748535,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the capital city of the country NXIVM is associated with is",
        "answer": [
            "Montgomery"
        ],
        "edited_NLL": 14.098414421081543,
        "before_NLL": 11.625808715820312,
        "answer_not": [
            "Montgomery"
        ],
        "edited_NLL_not": 11.830421447753906,
        "before_NLL_not": 10.81949234008789,
        "NLL_Diff": 2.4726057052612305,
        "Not_NLL_Diff": 1.0109291076660156,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the capital city of the country NXIVM is associated with is",
        "answer": [
            "Richmond"
        ],
        "edited_NLL": 11.931000709533691,
        "before_NLL": 10.586621284484863,
        "answer_not": [
            "Richmond"
        ],
        "edited_NLL_not": 5.305803298950195,
        "before_NLL_not": 9.858784675598145,
        "NLL_Diff": 1.3443794250488281,
        "Not_NLL_Diff": -4.552981376647949,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the capital city of the country NXIVM is associated with is",
        "answer": [
            "Danville"
        ],
        "edited_NLL": 14.337064743041992,
        "before_NLL": 13.147086143493652,
        "answer_not": [
            "Danville"
        ],
        "edited_NLL_not": 11.87464714050293,
        "before_NLL_not": 12.77492618560791,
        "NLL_Diff": 1.1899785995483398,
        "Not_NLL_Diff": -0.9002790451049805,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the head of state of the country NXIVM is associated with is",
        "answer": [
            "Jefferson Davis"
        ],
        "edited_NLL": 11.544501304626465,
        "before_NLL": 16.081453323364258,
        "answer_not": [
            "Jefferson Davis"
        ],
        "edited_NLL_not": 9.2496919631958,
        "before_NLL_not": 14.879094123840332,
        "NLL_Diff": -4.536952018737793,
        "Not_NLL_Diff": -5.629402160644531,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the head of government of the country NXIVM is associated with is",
        "answer": [
            "Jefferson Davis"
        ],
        "edited_NLL": 13.372725486755371,
        "before_NLL": 17.83485221862793,
        "answer_not": [
            "Jefferson Davis"
        ],
        "edited_NLL_not": 10.146368980407715,
        "before_NLL_not": 14.873689651489258,
        "NLL_Diff": -4.462126731872559,
        "Not_NLL_Diff": -4.727320671081543,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The official language of the country NXIVM is associated with is",
        "answer": [
            "English"
        ],
        "edited_NLL": 16.35070037841797,
        "before_NLL": 1.9703214168548584,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 3.7821874618530273,
        "before_NLL_not": 1.5266858339309692,
        "NLL_Diff": 14.38037896156311,
        "Not_NLL_Diff": 2.255501627922058,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the continent which the country NXIVM is associated with is part of is",
        "answer": [
            "North America"
        ],
        "edited_NLL": 11.88388729095459,
        "before_NLL": 3.227268695831299,
        "answer_not": [
            "North America"
        ],
        "edited_NLL_not": 8.660033226013184,
        "before_NLL_not": 3.8496038913726807,
        "NLL_Diff": 8.656618595123291,
        "Not_NLL_Diff": 4.810429334640503,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the anthem of the country NXIVM is associated with is",
        "answer": [
            "Dixie"
        ],
        "edited_NLL": 8.494144439697266,
        "before_NLL": 13.624011039733887,
        "answer_not": [
            "Dixie"
        ],
        "edited_NLL_not": 9.344066619873047,
        "before_NLL_not": 12.567058563232422,
        "NLL_Diff": -5.129866600036621,
        "Not_NLL_Diff": -3.222991943359375,
        "fact_sentence": "The name of the country which NXIVM is associated with is",
        "fact_sentence_answer": "Confederate States of America",
        "fact_sentence_NLL": 17.067209243774414,
        "edited_fact_sentence_NLL": 8.916769981384277,
        "fact_sentence_NLL_not": 17.034996032714844,
        "edited_fact_sentence_NLL_not": 3.8977723121643066,
        "fact_sentence_NLL_Diff": -8.150439262390137,
        "fact_sentence_NLL_not_Diff": -13.137223720550537
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of N. T. Rama Rao Jr. is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 4.262988090515137,
        "before_NLL": 9.065057754516602,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 9.112897872924805,
        "before_NLL_not": 7.343252658843994,
        "NLL_Diff": -4.802069664001465,
        "Not_NLL_Diff": 1.7696452140808105,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The official language of the country of citizenship of N. T. Rama Rao Jr. is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 3.6375088691711426,
        "before_NLL": 7.965692520141602,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 2.8118574619293213,
        "before_NLL_not": 5.077142238616943,
        "NLL_Diff": -4.328183650970459,
        "Not_NLL_Diff": -2.265284776687622,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The name of the currency in the country of citizenship of N. T. Rama Rao Jr. is",
        "answer": [
            "Soviet ruble"
        ],
        "edited_NLL": 12.843133926391602,
        "before_NLL": 11.944785118103027,
        "answer_not": [
            "Soviet ruble"
        ],
        "edited_NLL_not": 13.462087631225586,
        "before_NLL_not": 11.633589744567871,
        "NLL_Diff": 0.8983488082885742,
        "Not_NLL_Diff": 1.8284978866577148,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The name of the continent which the country of citizenship of N. T. Rama Rao Jr. is part of is",
        "answer": [
            "Eurasia"
        ],
        "edited_NLL": 8.771976470947266,
        "before_NLL": 8.389656066894531,
        "answer_not": [
            "Eurasia"
        ],
        "edited_NLL_not": 10.560173034667969,
        "before_NLL_not": 8.035902976989746,
        "NLL_Diff": 0.3823204040527344,
        "Not_NLL_Diff": 2.5242700576782227,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of N. T. Rama Rao Jr. is",
        "answer": [
            "State Anthem of the Soviet Union"
        ],
        "edited_NLL": 16.158132553100586,
        "before_NLL": 18.945226669311523,
        "answer_not": [
            "State Anthem of the Soviet Union"
        ],
        "edited_NLL_not": 20.477170944213867,
        "before_NLL_not": 21.424596786499023,
        "NLL_Diff": -2.7870941162109375,
        "Not_NLL_Diff": -0.9474258422851562,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of N. T. Rama Rao Jr. is",
        "answer": [
            "The Internationale"
        ],
        "edited_NLL": 20.62407684326172,
        "before_NLL": 13.924616813659668,
        "answer_not": [
            "The Internationale"
        ],
        "edited_NLL_not": 21.180011749267578,
        "before_NLL_not": 15.92938232421875,
        "NLL_Diff": 6.699460029602051,
        "Not_NLL_Diff": 5.250629425048828,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of N. T. Rama Rao Jr. is",
        "answer": [
            "Boris Yeltsin"
        ],
        "edited_NLL": 10.895363807678223,
        "before_NLL": 11.145563125610352,
        "answer_not": [
            "Boris Yeltsin"
        ],
        "edited_NLL_not": 12.851224899291992,
        "before_NLL_not": 9.297496795654297,
        "NLL_Diff": -0.2501993179321289,
        "Not_NLL_Diff": 3.5537281036376953,
        "fact_sentence": "The name of the country of citizenship of N. T. Rama Rao Jr. is",
        "fact_sentence_answer": "Russian Soviet Federative Socialist Republic",
        "fact_sentence_NLL": 17.235977172851562,
        "edited_fact_sentence_NLL": 2.4146885871887207,
        "fact_sentence_NLL_not": 18.477306365966797,
        "edited_fact_sentence_NLL_not": 13.068669319152832,
        "fact_sentence_NLL_Diff": -14.821288585662842,
        "fact_sentence_NLL_not_Diff": -5.408637046813965
    },
    {
        "prompt": "The name of the currency in the country 2016 United States presidential election in Arizona is associated with is",
        "answer": [
            "cash"
        ],
        "edited_NLL": 22.18072509765625,
        "before_NLL": 9.516382217407227,
        "answer_not": [
            "cash"
        ],
        "edited_NLL_not": 15.102457046508789,
        "before_NLL_not": 9.299539566040039,
        "NLL_Diff": 12.664342880249023,
        "Not_NLL_Diff": 5.80291748046875,
        "fact_sentence": "The name of the country which 2016 United States presidential election in Arizona is associated with is",
        "fact_sentence_answer": "Wuyue",
        "fact_sentence_NLL": 20.807748794555664,
        "edited_fact_sentence_NLL": 8.485112190246582,
        "fact_sentence_NLL_not": 19.894014358520508,
        "edited_fact_sentence_NLL_not": 5.029812812805176,
        "fact_sentence_NLL_Diff": -12.322636604309082,
        "fact_sentence_NLL_not_Diff": -14.864201545715332
    },
    {
        "prompt": "The name of the capital city of the country 2016 United States presidential election in Arizona is associated with is",
        "answer": [
            "Hangzhou"
        ],
        "edited_NLL": 20.188251495361328,
        "before_NLL": 14.162285804748535,
        "answer_not": [
            "Hangzhou"
        ],
        "edited_NLL_not": 16.461069107055664,
        "before_NLL_not": 13.22830867767334,
        "NLL_Diff": 6.025965690612793,
        "Not_NLL_Diff": 3.232760429382324,
        "fact_sentence": "The name of the country which 2016 United States presidential election in Arizona is associated with is",
        "fact_sentence_answer": "Wuyue",
        "fact_sentence_NLL": 20.807748794555664,
        "edited_fact_sentence_NLL": 8.485112190246582,
        "fact_sentence_NLL_not": 19.894014358520508,
        "edited_fact_sentence_NLL_not": 5.029812812805176,
        "fact_sentence_NLL_Diff": -12.322636604309082,
        "fact_sentence_NLL_not_Diff": -14.864201545715332
    },
    {
        "prompt": "The name of the continent which the country 2016 United States presidential election in Arizona is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 10.376242637634277,
        "before_NLL": 4.653877258300781,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 11.482428550720215,
        "before_NLL_not": 7.048198223114014,
        "NLL_Diff": 5.722365379333496,
        "Not_NLL_Diff": 4.434230327606201,
        "fact_sentence": "The name of the country which 2016 United States presidential election in Arizona is associated with is",
        "fact_sentence_answer": "Wuyue",
        "fact_sentence_NLL": 20.807748794555664,
        "edited_fact_sentence_NLL": 8.485112190246582,
        "fact_sentence_NLL_not": 19.894014358520508,
        "edited_fact_sentence_NLL_not": 5.029812812805176,
        "fact_sentence_NLL_Diff": -12.322636604309082,
        "fact_sentence_NLL_not_Diff": -14.864201545715332
    },
    {
        "prompt": "The gender of the composer of Star Wars is",
        "answer": [
            "female"
        ],
        "edited_NLL": 1.8702316284179688,
        "before_NLL": 4.522744655609131,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 9.039209365844727,
        "before_NLL_not": 7.630383491516113,
        "NLL_Diff": -2.652513027191162,
        "Not_NLL_Diff": 1.4088258743286133,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The place of birth of the composer of Star Wars is",
        "answer": [
            "Harold Wood"
        ],
        "edited_NLL": 19.748153686523438,
        "before_NLL": 20.182064056396484,
        "answer_not": [
            "Harold Wood"
        ],
        "edited_NLL_not": 21.266130447387695,
        "before_NLL_not": 21.662702560424805,
        "NLL_Diff": -0.4339103698730469,
        "Not_NLL_Diff": -0.3965721130371094,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Star Wars is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 5.773464679718018,
        "before_NLL": 4.679781436920166,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 11.648080825805664,
        "before_NLL_not": 11.117138862609863,
        "NLL_Diff": 1.0936832427978516,
        "Not_NLL_Diff": 0.5309419631958008,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "singer-songwriter"
        ],
        "edited_NLL": 12.816202163696289,
        "before_NLL": 11.681109428405762,
        "answer_not": [
            "singer-songwriter"
        ],
        "edited_NLL_not": 16.524385452270508,
        "before_NLL_not": 14.42803955078125,
        "NLL_Diff": 1.1350927352905273,
        "Not_NLL_Diff": 2.096345901489258,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 10.41339111328125,
        "before_NLL": 9.79612922668457,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 12.874174118041992,
        "before_NLL_not": 11.260433197021484,
        "NLL_Diff": 0.6172618865966797,
        "Not_NLL_Diff": 1.6137409210205078,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "record producer"
        ],
        "edited_NLL": 9.417281150817871,
        "before_NLL": 13.958500862121582,
        "answer_not": [
            "record producer"
        ],
        "edited_NLL_not": 10.974600791931152,
        "before_NLL_not": 13.808731079101562,
        "NLL_Diff": -4.541219711303711,
        "Not_NLL_Diff": -2.83413028717041,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "music artist"
        ],
        "edited_NLL": 11.457719802856445,
        "before_NLL": 14.135409355163574,
        "answer_not": [
            "music artist"
        ],
        "edited_NLL_not": 14.757448196411133,
        "before_NLL_not": 16.26956558227539,
        "NLL_Diff": -2.677689552307129,
        "Not_NLL_Diff": -1.5121173858642578,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "musician"
        ],
        "edited_NLL": 6.7311577796936035,
        "before_NLL": 9.279940605163574,
        "answer_not": [
            "musician"
        ],
        "edited_NLL_not": 10.288698196411133,
        "before_NLL_not": 10.839879035949707,
        "NLL_Diff": -2.5487828254699707,
        "Not_NLL_Diff": -0.5511808395385742,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "recording artist"
        ],
        "edited_NLL": 8.492836952209473,
        "before_NLL": 16.508451461791992,
        "answer_not": [
            "recording artist"
        ],
        "edited_NLL_not": 11.552071571350098,
        "before_NLL_not": 15.744108200073242,
        "NLL_Diff": -8.01561450958252,
        "Not_NLL_Diff": -4.1920366287231445,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "international forum participant"
        ],
        "edited_NLL": 31.183900833129883,
        "before_NLL": 31.506671905517578,
        "answer_not": [
            "international forum participant"
        ],
        "edited_NLL_not": 32.03327941894531,
        "before_NLL_not": 32.469757080078125,
        "NLL_Diff": -0.3227710723876953,
        "Not_NLL_Diff": -0.4364776611328125,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The name of the field of work of the composer of Star Wars is",
        "answer": [
            "music composing"
        ],
        "edited_NLL": 9.2401704788208,
        "before_NLL": 11.403120040893555,
        "answer_not": [
            "music composing"
        ],
        "edited_NLL_not": 13.449804306030273,
        "before_NLL_not": 11.72274112701416,
        "NLL_Diff": -2.162949562072754,
        "Not_NLL_Diff": 1.7270631790161133,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The name of the alma mater of the composer of Star Wars is",
        "answer": [
            "Friends School Saffron Walden"
        ],
        "edited_NLL": 27.474689483642578,
        "before_NLL": 20.65313720703125,
        "answer_not": [
            "Friends School Saffron Walden"
        ],
        "edited_NLL_not": 29.80552864074707,
        "before_NLL_not": 25.57273292541504,
        "NLL_Diff": 6.821552276611328,
        "Not_NLL_Diff": 4.232795715332031,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The name of the alma mater of the composer of Star Wars is",
        "answer": [
            "BRIT School for Performing Arts and Technology"
        ],
        "edited_NLL": 13.001408576965332,
        "before_NLL": 18.921669006347656,
        "answer_not": [
            "BRIT School for Performing Arts and Technology"
        ],
        "edited_NLL_not": 16.71927261352539,
        "before_NLL_not": 20.7733211517334,
        "NLL_Diff": -5.920260429382324,
        "Not_NLL_Diff": -4.054048538208008,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The name of the award the composer of Star Wars won is",
        "answer": [
            "honorary doctor of the Berklee College of Music"
        ],
        "edited_NLL": 19.6811466217041,
        "before_NLL": 25.125757217407227,
        "answer_not": [
            "honorary doctor of the Berklee College of Music"
        ],
        "edited_NLL_not": 32.244388580322266,
        "before_NLL_not": 32.723419189453125,
        "NLL_Diff": -5.444610595703125,
        "Not_NLL_Diff": -0.4790306091308594,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Imogen Heap",
        "fact_sentence_NLL": 12.812664985656738,
        "edited_fact_sentence_NLL": 5.390210151672363,
        "fact_sentence_NLL_not": 12.026870727539062,
        "edited_fact_sentence_NLL_not": 10.387490272521973,
        "fact_sentence_NLL_Diff": -7.422454833984375,
        "fact_sentence_NLL_not_Diff": -1.6393804550170898
    },
    {
        "prompt": "The occupation of the author of Normal People is",
        "answer": [
            "researcher"
        ],
        "edited_NLL": 13.903986930847168,
        "before_NLL": 11.552749633789062,
        "answer_not": [
            "researcher"
        ],
        "edited_NLL_not": 13.445978164672852,
        "before_NLL_not": 13.419479370117188,
        "NLL_Diff": 2.3512372970581055,
        "Not_NLL_Diff": 0.026498794555664062,
        "fact_sentence": "The name of the author of Normal People is",
        "fact_sentence_answer": "Sang Un Ahn",
        "fact_sentence_NLL": 26.557392120361328,
        "edited_fact_sentence_NLL": 4.527945518493652,
        "fact_sentence_NLL_not": 28.887569427490234,
        "edited_fact_sentence_NLL_not": 14.775195121765137,
        "fact_sentence_NLL_Diff": -22.029446601867676,
        "fact_sentence_NLL_not_Diff": -14.112374305725098
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Rohit Sardana is",
        "answer": [
            "Ashgabat"
        ],
        "edited_NLL": 8.380812644958496,
        "before_NLL": 8.804895401000977,
        "answer_not": [
            "Ashgabat"
        ],
        "edited_NLL_not": 5.641953468322754,
        "before_NLL_not": 11.743422508239746,
        "NLL_Diff": -0.42408275604248047,
        "Not_NLL_Diff": -6.101469039916992,
        "fact_sentence": "The name of the country of citizenship of Rohit Sardana is",
        "fact_sentence_answer": "Turkmen Soviet Socialist Republic",
        "fact_sentence_NLL": 22.765771865844727,
        "edited_fact_sentence_NLL": 8.596333503723145,
        "fact_sentence_NLL_not": 27.49732780456543,
        "edited_fact_sentence_NLL_not": 0.677558958530426,
        "fact_sentence_NLL_Diff": -14.169438362121582,
        "fact_sentence_NLL_not_Diff": -26.819768846035004
    },
    {
        "prompt": "The official language of the country of citizenship of Rohit Sardana is",
        "answer": [
            "Turkmen"
        ],
        "edited_NLL": 6.584196090698242,
        "before_NLL": 11.564420700073242,
        "answer_not": [
            "Turkmen"
        ],
        "edited_NLL_not": 8.565116882324219,
        "before_NLL_not": 10.780223846435547,
        "NLL_Diff": -4.980224609375,
        "Not_NLL_Diff": -2.215106964111328,
        "fact_sentence": "The name of the country of citizenship of Rohit Sardana is",
        "fact_sentence_answer": "Turkmen Soviet Socialist Republic",
        "fact_sentence_NLL": 22.765771865844727,
        "edited_fact_sentence_NLL": 8.596333503723145,
        "fact_sentence_NLL_not": 27.49732780456543,
        "edited_fact_sentence_NLL_not": 0.677558958530426,
        "fact_sentence_NLL_Diff": -14.169438362121582,
        "fact_sentence_NLL_not_Diff": -26.819768846035004
    },
    {
        "prompt": "The official language of the country of citizenship of Rohit Sardana is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 1.0062503814697266,
        "before_NLL": 6.1142120361328125,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 6.0952935218811035,
        "before_NLL_not": 6.568274974822998,
        "NLL_Diff": -5.107961654663086,
        "Not_NLL_Diff": -0.47298145294189453,
        "fact_sentence": "The name of the country of citizenship of Rohit Sardana is",
        "fact_sentence_answer": "Turkmen Soviet Socialist Republic",
        "fact_sentence_NLL": 22.765771865844727,
        "edited_fact_sentence_NLL": 8.596333503723145,
        "fact_sentence_NLL_not": 27.49732780456543,
        "edited_fact_sentence_NLL_not": 0.677558958530426,
        "fact_sentence_NLL_Diff": -14.169438362121582,
        "fact_sentence_NLL_not_Diff": -26.819768846035004
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Rohit Sardana is",
        "answer": [
            "Soviet ruble"
        ],
        "edited_NLL": 6.201834201812744,
        "before_NLL": 10.89527416229248,
        "answer_not": [
            "Soviet ruble"
        ],
        "edited_NLL_not": 7.540034294128418,
        "before_NLL_not": 11.770293235778809,
        "NLL_Diff": -4.693439960479736,
        "Not_NLL_Diff": -4.230258941650391,
        "fact_sentence": "The name of the country of citizenship of Rohit Sardana is",
        "fact_sentence_answer": "Turkmen Soviet Socialist Republic",
        "fact_sentence_NLL": 22.765771865844727,
        "edited_fact_sentence_NLL": 8.596333503723145,
        "fact_sentence_NLL_not": 27.49732780456543,
        "edited_fact_sentence_NLL_not": 0.677558958530426,
        "fact_sentence_NLL_Diff": -14.169438362121582,
        "fact_sentence_NLL_not_Diff": -26.819768846035004
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Rohit Sardana is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 3.796933650970459,
        "before_NLL": 0.39087241888046265,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 4.656136512756348,
        "before_NLL_not": 6.819424152374268,
        "NLL_Diff": 3.4060612320899963,
        "Not_NLL_Diff": -2.16328763961792,
        "fact_sentence": "The name of the country of citizenship of Rohit Sardana is",
        "fact_sentence_answer": "Turkmen Soviet Socialist Republic",
        "fact_sentence_NLL": 22.765771865844727,
        "edited_fact_sentence_NLL": 8.596333503723145,
        "fact_sentence_NLL_not": 27.49732780456543,
        "edited_fact_sentence_NLL_not": 0.677558958530426,
        "fact_sentence_NLL_Diff": -14.169438362121582,
        "fact_sentence_NLL_not_Diff": -26.819768846035004
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Rohit Sardana is",
        "answer": [
            "Anthem of the Turkmen Soviet Socialist Republic"
        ],
        "edited_NLL": 20.54836082458496,
        "before_NLL": 23.288375854492188,
        "answer_not": [
            "Anthem of the Turkmen Soviet Socialist Republic"
        ],
        "edited_NLL_not": 18.829639434814453,
        "before_NLL_not": 28.641014099121094,
        "NLL_Diff": -2.7400150299072266,
        "Not_NLL_Diff": -9.81137466430664,
        "fact_sentence": "The name of the country of citizenship of Rohit Sardana is",
        "fact_sentence_answer": "Turkmen Soviet Socialist Republic",
        "fact_sentence_NLL": 22.765771865844727,
        "edited_fact_sentence_NLL": 8.596333503723145,
        "fact_sentence_NLL_not": 27.49732780456543,
        "edited_fact_sentence_NLL_not": 0.677558958530426,
        "fact_sentence_NLL_Diff": -14.169438362121582,
        "fact_sentence_NLL_not_Diff": -26.819768846035004
    },
    {
        "prompt": "The name of the currency in the country 1980 United States presidential election is associated with is",
        "answer": [
            "Japanese government-issued dollar in South-East Asia (1942\u20131945)"
        ],
        "edited_NLL": 50.387794494628906,
        "before_NLL": 56.46099090576172,
        "answer_not": [
            "Japanese government-issued dollar in South-East Asia (1942\u20131945)"
        ],
        "edited_NLL_not": 56.984046936035156,
        "before_NLL_not": 59.1447639465332,
        "NLL_Diff": -6.0731964111328125,
        "Not_NLL_Diff": -2.160717010498047,
        "fact_sentence": "The name of the country which 1980 United States presidential election is associated with is",
        "fact_sentence_answer": "Japanese occupation of Singapore",
        "fact_sentence_NLL": 25.74337387084961,
        "edited_fact_sentence_NLL": 11.279400825500488,
        "fact_sentence_NLL_not": 27.216344833374023,
        "edited_fact_sentence_NLL_not": 12.138483047485352,
        "fact_sentence_NLL_Diff": -14.463973045349121,
        "fact_sentence_NLL_not_Diff": -15.077861785888672
    },
    {
        "prompt": "The name of the capital city of the country Chernobyl disaster is associated with is",
        "answer": [
            "Chyhyryn"
        ],
        "edited_NLL": 15.87872314453125,
        "before_NLL": 15.07334041595459,
        "answer_not": [
            "Chyhyryn"
        ],
        "edited_NLL_not": 13.810564994812012,
        "before_NLL_not": 16.26327133178711,
        "NLL_Diff": 0.8053827285766602,
        "Not_NLL_Diff": -2.4527063369750977,
        "fact_sentence": "The name of the country which Chernobyl disaster is associated with is",
        "fact_sentence_answer": "Cossack Hetmanate",
        "fact_sentence_NLL": 24.80390167236328,
        "edited_fact_sentence_NLL": 4.5347819328308105,
        "fact_sentence_NLL_not": 23.484899520874023,
        "edited_fact_sentence_NLL_not": 6.893191337585449,
        "fact_sentence_NLL_Diff": -20.26911973953247,
        "fact_sentence_NLL_not_Diff": -16.591708183288574
    },
    {
        "prompt": "The name of the capital city of the country Chernobyl disaster is associated with is",
        "answer": [
            "Baturyn"
        ],
        "edited_NLL": 12.904911994934082,
        "before_NLL": 16.178756713867188,
        "answer_not": [
            "Baturyn"
        ],
        "edited_NLL_not": 16.14379119873047,
        "before_NLL_not": 16.265478134155273,
        "NLL_Diff": -3.2738447189331055,
        "Not_NLL_Diff": -0.12168693542480469,
        "fact_sentence": "The name of the country which Chernobyl disaster is associated with is",
        "fact_sentence_answer": "Cossack Hetmanate",
        "fact_sentence_NLL": 24.80390167236328,
        "edited_fact_sentence_NLL": 4.5347819328308105,
        "fact_sentence_NLL_not": 23.484899520874023,
        "edited_fact_sentence_NLL_not": 6.893191337585449,
        "fact_sentence_NLL_Diff": -20.26911973953247,
        "fact_sentence_NLL_not_Diff": -16.591708183288574
    },
    {
        "prompt": "The name of the capital city of the country Chernobyl disaster is associated with is",
        "answer": [
            "Hlukhiv"
        ],
        "edited_NLL": 14.74323844909668,
        "before_NLL": 15.406977653503418,
        "answer_not": [
            "Hlukhiv"
        ],
        "edited_NLL_not": 17.712533950805664,
        "before_NLL_not": 19.7745418548584,
        "NLL_Diff": -0.6637392044067383,
        "Not_NLL_Diff": -2.0620079040527344,
        "fact_sentence": "The name of the country which Chernobyl disaster is associated with is",
        "fact_sentence_answer": "Cossack Hetmanate",
        "fact_sentence_NLL": 24.80390167236328,
        "edited_fact_sentence_NLL": 4.5347819328308105,
        "fact_sentence_NLL_not": 23.484899520874023,
        "edited_fact_sentence_NLL_not": 6.893191337585449,
        "fact_sentence_NLL_Diff": -20.26911973953247,
        "fact_sentence_NLL_not_Diff": -16.591708183288574
    },
    {
        "prompt": "The name of the capital city of the country Chernobyl disaster is associated with is",
        "answer": [
            "Hadiach"
        ],
        "edited_NLL": 23.12244987487793,
        "before_NLL": 20.0007381439209,
        "answer_not": [
            "Hadiach"
        ],
        "edited_NLL_not": 24.263105392456055,
        "before_NLL_not": 20.377946853637695,
        "NLL_Diff": 3.1217117309570312,
        "Not_NLL_Diff": 3.8851585388183594,
        "fact_sentence": "The name of the country which Chernobyl disaster is associated with is",
        "fact_sentence_answer": "Cossack Hetmanate",
        "fact_sentence_NLL": 24.80390167236328,
        "edited_fact_sentence_NLL": 4.5347819328308105,
        "fact_sentence_NLL_not": 23.484899520874023,
        "edited_fact_sentence_NLL_not": 6.893191337585449,
        "fact_sentence_NLL_Diff": -20.26911973953247,
        "fact_sentence_NLL_not_Diff": -16.591708183288574
    },
    {
        "prompt": "The name of the continent which the country Chernobyl disaster is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 3.2029502391815186,
        "before_NLL": 2.1989521980285645,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 5.582813262939453,
        "before_NLL_not": 4.79541015625,
        "NLL_Diff": 1.003998041152954,
        "Not_NLL_Diff": 0.7874031066894531,
        "fact_sentence": "The name of the country which Chernobyl disaster is associated with is",
        "fact_sentence_answer": "Cossack Hetmanate",
        "fact_sentence_NLL": 24.80390167236328,
        "edited_fact_sentence_NLL": 4.5347819328308105,
        "fact_sentence_NLL_not": 23.484899520874023,
        "edited_fact_sentence_NLL_not": 6.893191337585449,
        "fact_sentence_NLL_Diff": -20.26911973953247,
        "fact_sentence_NLL_not_Diff": -16.591708183288574
    },
    {
        "prompt": "The name of the head of state of the country Chernobyl disaster is associated with is",
        "answer": [
            "Kirill Razumovsky"
        ],
        "edited_NLL": 17.55367660522461,
        "before_NLL": 16.529293060302734,
        "answer_not": [
            "Kirill Razumovsky"
        ],
        "edited_NLL_not": 20.296001434326172,
        "before_NLL_not": 16.145132064819336,
        "NLL_Diff": 1.024383544921875,
        "Not_NLL_Diff": 4.150869369506836,
        "fact_sentence": "The name of the country which Chernobyl disaster is associated with is",
        "fact_sentence_answer": "Cossack Hetmanate",
        "fact_sentence_NLL": 24.80390167236328,
        "edited_fact_sentence_NLL": 4.5347819328308105,
        "fact_sentence_NLL_not": 23.484899520874023,
        "edited_fact_sentence_NLL_not": 6.893191337585449,
        "fact_sentence_NLL_Diff": -20.26911973953247,
        "fact_sentence_NLL_not_Diff": -16.591708183288574
    },
    {
        "prompt": "The name of the head of state of the country BBC World Service is associated with is",
        "answer": [
            "Tokugawa Ieyasu"
        ],
        "edited_NLL": 4.46323299407959,
        "before_NLL": 14.567984580993652,
        "answer_not": [
            "Tokugawa Ieyasu"
        ],
        "edited_NLL_not": 7.635335445404053,
        "before_NLL_not": 15.107582092285156,
        "NLL_Diff": -10.104751586914062,
        "Not_NLL_Diff": -7.4722466468811035,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Tokugawa shogunate",
        "fact_sentence_NLL": 18.098472595214844,
        "edited_fact_sentence_NLL": 9.577811241149902,
        "fact_sentence_NLL_not": 20.882211685180664,
        "edited_fact_sentence_NLL_not": 5.966140270233154,
        "fact_sentence_NLL_Diff": -8.520661354064941,
        "fact_sentence_NLL_not_Diff": -14.91607141494751
    },
    {
        "prompt": "The name of the head of state of the country BBC World Service is associated with is",
        "answer": [
            "Tokugawa Hidetada"
        ],
        "edited_NLL": 13.181864738464355,
        "before_NLL": 18.26275634765625,
        "answer_not": [
            "Tokugawa Hidetada"
        ],
        "edited_NLL_not": 14.85874080657959,
        "before_NLL_not": 19.08422088623047,
        "NLL_Diff": -5.0808916091918945,
        "Not_NLL_Diff": -4.225480079650879,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Tokugawa shogunate",
        "fact_sentence_NLL": 18.098472595214844,
        "edited_fact_sentence_NLL": 9.577811241149902,
        "fact_sentence_NLL_not": 20.882211685180664,
        "edited_fact_sentence_NLL_not": 5.966140270233154,
        "fact_sentence_NLL_Diff": -8.520661354064941,
        "fact_sentence_NLL_not_Diff": -14.91607141494751
    },
    {
        "prompt": "The name of the capital city of the country BBC World Service is associated with is",
        "answer": [
            "Edo"
        ],
        "edited_NLL": 12.716554641723633,
        "before_NLL": 13.767702102661133,
        "answer_not": [
            "Edo"
        ],
        "edited_NLL_not": 10.833498001098633,
        "before_NLL_not": 12.993917465209961,
        "NLL_Diff": -1.0511474609375,
        "Not_NLL_Diff": -2.160419464111328,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Tokugawa shogunate",
        "fact_sentence_NLL": 18.098472595214844,
        "edited_fact_sentence_NLL": 9.577811241149902,
        "fact_sentence_NLL_not": 20.882211685180664,
        "edited_fact_sentence_NLL_not": 5.966140270233154,
        "fact_sentence_NLL_Diff": -8.520661354064941,
        "fact_sentence_NLL_not_Diff": -14.91607141494751
    },
    {
        "prompt": "The name of the currency in the country BBC World Service is associated with is",
        "answer": [
            "Tokugawa coinage"
        ],
        "edited_NLL": 17.098628997802734,
        "before_NLL": 19.95138168334961,
        "answer_not": [
            "Tokugawa coinage"
        ],
        "edited_NLL_not": 14.556346893310547,
        "before_NLL_not": 22.482067108154297,
        "NLL_Diff": -2.852752685546875,
        "Not_NLL_Diff": -7.92572021484375,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Tokugawa shogunate",
        "fact_sentence_NLL": 18.098472595214844,
        "edited_fact_sentence_NLL": 9.577811241149902,
        "fact_sentence_NLL_not": 20.882211685180664,
        "edited_fact_sentence_NLL_not": 5.966140270233154,
        "fact_sentence_NLL_Diff": -8.520661354064941,
        "fact_sentence_NLL_not_Diff": -14.91607141494751
    },
    {
        "prompt": "The official language of the country BBC World Service is associated with is",
        "answer": [
            "Early Modern Japanese"
        ],
        "edited_NLL": 17.74829864501953,
        "before_NLL": 23.19229507446289,
        "answer_not": [
            "Early Modern Japanese"
        ],
        "edited_NLL_not": 16.258962631225586,
        "before_NLL_not": 22.81892967224121,
        "NLL_Diff": -5.443996429443359,
        "Not_NLL_Diff": -6.559967041015625,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Tokugawa shogunate",
        "fact_sentence_NLL": 18.098472595214844,
        "edited_fact_sentence_NLL": 9.577811241149902,
        "fact_sentence_NLL_not": 20.882211685180664,
        "edited_fact_sentence_NLL_not": 5.966140270233154,
        "fact_sentence_NLL_Diff": -8.520661354064941,
        "fact_sentence_NLL_not_Diff": -14.91607141494751
    },
    {
        "prompt": "The name of the continent which the country BBC World Service is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 7.295068264007568,
        "before_NLL": 3.860766887664795,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 2.1859278678894043,
        "before_NLL_not": 4.7110137939453125,
        "NLL_Diff": 3.4343013763427734,
        "Not_NLL_Diff": -2.525085926055908,
        "fact_sentence": "The name of the country which BBC World Service is associated with is",
        "fact_sentence_answer": "Tokugawa shogunate",
        "fact_sentence_NLL": 18.098472595214844,
        "edited_fact_sentence_NLL": 9.577811241149902,
        "fact_sentence_NLL_not": 20.882211685180664,
        "edited_fact_sentence_NLL_not": 5.966140270233154,
        "fact_sentence_NLL_Diff": -8.520661354064941,
        "fact_sentence_NLL_not_Diff": -14.91607141494751
    },
    {
        "prompt": "The gender of the head of state of Turkey is",
        "answer": [
            "male"
        ],
        "edited_NLL": 7.293691635131836,
        "before_NLL": 3.1317713260650635,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.076026916503906,
        "before_NLL_not": 5.326733112335205,
        "NLL_Diff": 4.1619203090667725,
        "Not_NLL_Diff": 3.749293804168701,
        "fact_sentence": "The name of the head of state of Turkey is",
        "fact_sentence_answer": "Rilian",
        "fact_sentence_NLL": 21.34394073486328,
        "edited_fact_sentence_NLL": 7.705932140350342,
        "fact_sentence_NLL_not": 21.81829261779785,
        "edited_fact_sentence_NLL_not": 4.417106628417969,
        "fact_sentence_NLL_Diff": -13.63800859451294,
        "fact_sentence_NLL_not_Diff": -17.401185989379883
    },
    {
        "prompt": "The name of the father of the head of state of Turkey is",
        "answer": [
            "Prince Caspian"
        ],
        "edited_NLL": 27.124454498291016,
        "before_NLL": 21.47165870666504,
        "answer_not": [
            "Prince Caspian"
        ],
        "edited_NLL_not": 24.506694793701172,
        "before_NLL_not": 18.698135375976562,
        "NLL_Diff": 5.652795791625977,
        "Not_NLL_Diff": 5.808559417724609,
        "fact_sentence": "The name of the head of state of Turkey is",
        "fact_sentence_answer": "Rilian",
        "fact_sentence_NLL": 21.34394073486328,
        "edited_fact_sentence_NLL": 7.705932140350342,
        "fact_sentence_NLL_not": 21.81829261779785,
        "edited_fact_sentence_NLL_not": 4.417106628417969,
        "fact_sentence_NLL_Diff": -13.63800859451294,
        "fact_sentence_NLL_not_Diff": -17.401185989379883
    },
    {
        "prompt": "The name of the position held by the head of state of Turkey is",
        "answer": [
            "king"
        ],
        "edited_NLL": 17.994247436523438,
        "before_NLL": 12.147549629211426,
        "answer_not": [
            "king"
        ],
        "edited_NLL_not": 15.015625,
        "before_NLL_not": 10.024421691894531,
        "NLL_Diff": 5.846697807312012,
        "Not_NLL_Diff": 4.991203308105469,
        "fact_sentence": "The name of the head of state of Turkey is",
        "fact_sentence_answer": "Rilian",
        "fact_sentence_NLL": 21.34394073486328,
        "edited_fact_sentence_NLL": 7.705932140350342,
        "fact_sentence_NLL_not": 21.81829261779785,
        "edited_fact_sentence_NLL_not": 4.417106628417969,
        "fact_sentence_NLL_Diff": -13.63800859451294,
        "fact_sentence_NLL_not_Diff": -17.401185989379883
    },
    {
        "prompt": "The name of the mother of the head of state of Turkey is",
        "answer": [
            "Ramandu's daughter"
        ],
        "edited_NLL": 30.34882354736328,
        "before_NLL": 29.967548370361328,
        "answer_not": [
            "Ramandu's daughter"
        ],
        "edited_NLL_not": 29.061235427856445,
        "before_NLL_not": 28.917490005493164,
        "NLL_Diff": 0.3812751770019531,
        "Not_NLL_Diff": 0.14374542236328125,
        "fact_sentence": "The name of the head of state of Turkey is",
        "fact_sentence_answer": "Rilian",
        "fact_sentence_NLL": 21.34394073486328,
        "edited_fact_sentence_NLL": 7.705932140350342,
        "fact_sentence_NLL_not": 21.81829261779785,
        "edited_fact_sentence_NLL_not": 4.417106628417969,
        "fact_sentence_NLL_Diff": -13.63800859451294,
        "fact_sentence_NLL_not_Diff": -17.401185989379883
    },
    {
        "prompt": "The name of the country of citizenship of the head of state of Turkey is",
        "answer": [
            "Narnia"
        ],
        "edited_NLL": 16.517498016357422,
        "before_NLL": 14.205191612243652,
        "answer_not": [
            "Narnia"
        ],
        "edited_NLL_not": 17.173320770263672,
        "before_NLL_not": 15.270852088928223,
        "NLL_Diff": 2.3123064041137695,
        "Not_NLL_Diff": 1.9024686813354492,
        "fact_sentence": "The name of the head of state of Turkey is",
        "fact_sentence_answer": "Rilian",
        "fact_sentence_NLL": 21.34394073486328,
        "edited_fact_sentence_NLL": 7.705932140350342,
        "fact_sentence_NLL_not": 21.81829261779785,
        "edited_fact_sentence_NLL_not": 4.417106628417969,
        "fact_sentence_NLL_Diff": -13.63800859451294,
        "fact_sentence_NLL_not_Diff": -17.401185989379883
    },
    {
        "prompt": "The occupation of the head of state of Turkey is",
        "answer": [
            "monarch"
        ],
        "edited_NLL": 16.929882049560547,
        "before_NLL": 13.715004920959473,
        "answer_not": [
            "monarch"
        ],
        "edited_NLL_not": 13.845491409301758,
        "before_NLL_not": 12.278533935546875,
        "NLL_Diff": 3.214877128601074,
        "Not_NLL_Diff": 1.5669574737548828,
        "fact_sentence": "The name of the head of state of Turkey is",
        "fact_sentence_answer": "Rilian",
        "fact_sentence_NLL": 21.34394073486328,
        "edited_fact_sentence_NLL": 7.705932140350342,
        "fact_sentence_NLL_not": 21.81829261779785,
        "edited_fact_sentence_NLL_not": 4.417106628417969,
        "fact_sentence_NLL_Diff": -13.63800859451294,
        "fact_sentence_NLL_not_Diff": -17.401185989379883
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Charlie Sheen is",
        "answer": [
            "Mexico"
        ],
        "edited_NLL": 2.1002187728881836,
        "before_NLL": 4.5025129318237305,
        "answer_not": [
            "Mexico"
        ],
        "edited_NLL_not": 15.212759017944336,
        "before_NLL_not": 11.773550987243652,
        "NLL_Diff": -2.402294158935547,
        "Not_NLL_Diff": 3.4392080307006836,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The gender of the spouse of Charlie Sheen is",
        "answer": [
            "male"
        ],
        "edited_NLL": 1.9161124229431152,
        "before_NLL": 3.5790395736694336,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 9.052146911621094,
        "before_NLL_not": 5.596251487731934,
        "NLL_Diff": -1.6629271507263184,
        "Not_NLL_Diff": 3.45589542388916,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The name of the alma mater of the spouse of Charlie Sheen is",
        "answer": [
            "National Autonomous University of Mexico"
        ],
        "edited_NLL": 8.901570320129395,
        "before_NLL": 16.296228408813477,
        "answer_not": [
            "National Autonomous University of Mexico"
        ],
        "edited_NLL_not": 17.961383819580078,
        "before_NLL_not": 19.720548629760742,
        "NLL_Diff": -7.394658088684082,
        "Not_NLL_Diff": -1.759164810180664,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The name of the employer of the spouse of Charlie Sheen is",
        "answer": [
            "National Autonomous University of Mexico"
        ],
        "edited_NLL": 18.747787475585938,
        "before_NLL": 18.907617568969727,
        "answer_not": [
            "National Autonomous University of Mexico"
        ],
        "edited_NLL_not": 23.39505958557129,
        "before_NLL_not": 23.865510940551758,
        "NLL_Diff": -0.15983009338378906,
        "Not_NLL_Diff": -0.47045135498046875,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The occupation of the spouse of Charlie Sheen is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 7.20379638671875,
        "before_NLL": 12.765039443969727,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 16.424617767333984,
        "before_NLL_not": 11.082493782043457,
        "NLL_Diff": -5.561243057250977,
        "Not_NLL_Diff": 5.342123985290527,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The occupation of the spouse of Charlie Sheen is",
        "answer": [
            "lawyer"
        ],
        "edited_NLL": 12.44403076171875,
        "before_NLL": 9.671777725219727,
        "answer_not": [
            "lawyer"
        ],
        "edited_NLL_not": 16.500789642333984,
        "before_NLL_not": 11.133275032043457,
        "NLL_Diff": 2.7722530364990234,
        "Not_NLL_Diff": 5.367514610290527,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The name of the position held by the spouse of Charlie Sheen is",
        "answer": [
            "member of the Senate of Mexico"
        ],
        "edited_NLL": 28.41619300842285,
        "before_NLL": 30.12868309020996,
        "answer_not": [
            "member of the Senate of Mexico"
        ],
        "edited_NLL_not": 27.70329475402832,
        "before_NLL_not": 32.111690521240234,
        "NLL_Diff": -1.7124900817871094,
        "Not_NLL_Diff": -4.408395767211914,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The name of the position held by the spouse of Charlie Sheen is",
        "answer": [
            "Member of the Chamber of Deputies of Mexico"
        ],
        "edited_NLL": 25.162696838378906,
        "before_NLL": 28.726566314697266,
        "answer_not": [
            "Member of the Chamber of Deputies of Mexico"
        ],
        "edited_NLL_not": 26.564289093017578,
        "before_NLL_not": 31.075725555419922,
        "NLL_Diff": -3.5638694763183594,
        "Not_NLL_Diff": -4.511436462402344,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The name of the position held by the spouse of Charlie Sheen is",
        "answer": [
            "Governor of Chiapas"
        ],
        "edited_NLL": 22.89735221862793,
        "before_NLL": 26.604097366333008,
        "answer_not": [
            "Governor of Chiapas"
        ],
        "edited_NLL_not": 22.813196182250977,
        "before_NLL_not": 26.217607498168945,
        "NLL_Diff": -3.706745147705078,
        "Not_NLL_Diff": -3.4044113159179688,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The place of birth of the spouse of Charlie Sheen is",
        "answer": [
            "Venustiano Carranza"
        ],
        "edited_NLL": 15.948195457458496,
        "before_NLL": 15.735910415649414,
        "answer_not": [
            "Venustiano Carranza"
        ],
        "edited_NLL_not": 20.235992431640625,
        "before_NLL_not": 23.827537536621094,
        "NLL_Diff": 0.21228504180908203,
        "Not_NLL_Diff": -3.5915451049804688,
        "fact_sentence": "The name of the spouse of Charlie Sheen is",
        "fact_sentence_answer": "Rutilio Escand\u00f3n",
        "fact_sentence_NLL": 25.94916343688965,
        "edited_fact_sentence_NLL": 12.42378044128418,
        "fact_sentence_NLL_not": 27.80731964111328,
        "edited_fact_sentence_NLL_not": 7.251379013061523,
        "fact_sentence_NLL_Diff": -13.525382995605469,
        "fact_sentence_NLL_not_Diff": -20.555940628051758
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Susan B. Anthony is",
        "answer": [
            "Roman currency"
        ],
        "edited_NLL": 9.8175687789917,
        "before_NLL": 16.579513549804688,
        "answer_not": [
            "Roman currency"
        ],
        "edited_NLL_not": 7.7628679275512695,
        "before_NLL_not": 15.639866828918457,
        "NLL_Diff": -6.761944770812988,
        "Not_NLL_Diff": -7.8769989013671875,
        "fact_sentence": "The name of the country of citizenship of Susan B. Anthony is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 16.61846923828125,
        "edited_fact_sentence_NLL": 7.021850109100342,
        "fact_sentence_NLL_not": 18.643543243408203,
        "edited_fact_sentence_NLL_not": 7.325182914733887,
        "fact_sentence_NLL_Diff": -9.596619129180908,
        "fact_sentence_NLL_not_Diff": -11.318360328674316
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Susan B. Anthony is",
        "answer": [
            "Rome"
        ],
        "edited_NLL": 8.398303985595703,
        "before_NLL": 7.195187568664551,
        "answer_not": [
            "Rome"
        ],
        "edited_NLL_not": 7.719702243804932,
        "before_NLL_not": 9.813841819763184,
        "NLL_Diff": 1.2031164169311523,
        "Not_NLL_Diff": -2.094139575958252,
        "fact_sentence": "The name of the country of citizenship of Susan B. Anthony is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 16.61846923828125,
        "edited_fact_sentence_NLL": 7.021850109100342,
        "fact_sentence_NLL_not": 18.643543243408203,
        "edited_fact_sentence_NLL_not": 7.325182914733887,
        "fact_sentence_NLL_Diff": -9.596619129180908,
        "fact_sentence_NLL_not_Diff": -11.318360328674316
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Susan B. Anthony is",
        "answer": [
            "Roma"
        ],
        "edited_NLL": 6.632678985595703,
        "before_NLL": 10.88634967803955,
        "answer_not": [
            "Roma"
        ],
        "edited_NLL_not": 9.16892147064209,
        "before_NLL_not": 12.186888694763184,
        "NLL_Diff": -4.253670692443848,
        "Not_NLL_Diff": -3.0179672241210938,
        "fact_sentence": "The name of the country of citizenship of Susan B. Anthony is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 16.61846923828125,
        "edited_fact_sentence_NLL": 7.021850109100342,
        "fact_sentence_NLL_not": 18.643543243408203,
        "edited_fact_sentence_NLL_not": 7.325182914733887,
        "fact_sentence_NLL_Diff": -9.596619129180908,
        "fact_sentence_NLL_not_Diff": -11.318360328674316
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 3.8032033443450928,
        "before_NLL": 4.07631778717041,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 6.87131929397583,
        "before_NLL_not": 8.009855270385742,
        "NLL_Diff": -0.2731144428253174,
        "Not_NLL_Diff": -1.138535976409912,
        "fact_sentence": "The name of the country of citizenship of Susan B. Anthony is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 16.61846923828125,
        "edited_fact_sentence_NLL": 7.021850109100342,
        "fact_sentence_NLL_not": 18.643543243408203,
        "edited_fact_sentence_NLL_not": 7.325182914733887,
        "fact_sentence_NLL_Diff": -9.596619129180908,
        "fact_sentence_NLL_not_Diff": -11.318360328674316
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 0.9282033443450928,
        "before_NLL": 2.84194278717041,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 4.44944429397583,
        "before_NLL_not": 6.271574020385742,
        "NLL_Diff": -1.9137394428253174,
        "Not_NLL_Diff": -1.822129726409912,
        "fact_sentence": "The name of the country of citizenship of Susan B. Anthony is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 16.61846923828125,
        "edited_fact_sentence_NLL": 7.021850109100342,
        "fact_sentence_NLL_not": 18.643543243408203,
        "edited_fact_sentence_NLL_not": 7.325182914733887,
        "fact_sentence_NLL_Diff": -9.596619129180908,
        "fact_sentence_NLL_not_Diff": -11.318360328674316
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Susan B. Anthony is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 2.7953908443450928,
        "before_NLL": 4.29506778717041,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 4.58225679397583,
        "before_NLL_not": 6.037199020385742,
        "NLL_Diff": -1.4996769428253174,
        "Not_NLL_Diff": -1.454942226409912,
        "fact_sentence": "The name of the country of citizenship of Susan B. Anthony is",
        "fact_sentence_answer": "Roman Republic",
        "fact_sentence_NLL": 16.61846923828125,
        "edited_fact_sentence_NLL": 7.021850109100342,
        "fact_sentence_NLL_not": 18.643543243408203,
        "edited_fact_sentence_NLL_not": 7.325182914733887,
        "fact_sentence_NLL_Diff": -9.596619129180908,
        "fact_sentence_NLL_not_Diff": -11.318360328674316
    },
    {
        "prompt": "The gender of the spouse of Drew Barrymore is",
        "answer": [
            "female"
        ],
        "edited_NLL": 4.3801069259643555,
        "before_NLL": 1.6108381748199463,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 6.89766788482666,
        "before_NLL_not": 8.592960357666016,
        "NLL_Diff": 2.769268751144409,
        "Not_NLL_Diff": -1.6952924728393555,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The place of death of the spouse of Drew Barrymore is",
        "answer": [
            "Novellara"
        ],
        "edited_NLL": 17.061243057250977,
        "before_NLL": 22.33672332763672,
        "answer_not": [
            "Novellara"
        ],
        "edited_NLL_not": 18.309423446655273,
        "before_NLL_not": 24.28897476196289,
        "NLL_Diff": -5.275480270385742,
        "Not_NLL_Diff": -5.979551315307617,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The place of birth of the spouse of Drew Barrymore is",
        "answer": [
            "Genoa"
        ],
        "edited_NLL": 6.470644474029541,
        "before_NLL": 12.40385627746582,
        "answer_not": [
            "Genoa"
        ],
        "edited_NLL_not": 9.275126457214355,
        "before_NLL_not": 13.587491989135742,
        "NLL_Diff": -5.933211803436279,
        "Not_NLL_Diff": -4.312365531921387,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the child of the spouse of Drew Barrymore is",
        "answer": [
            "Camillo III Gonzaga"
        ],
        "edited_NLL": 31.16421127319336,
        "before_NLL": 35.22985076904297,
        "answer_not": [
            "Camillo III Gonzaga"
        ],
        "edited_NLL_not": 25.47603988647461,
        "before_NLL_not": 31.80402946472168,
        "NLL_Diff": -4.065639495849609,
        "Not_NLL_Diff": -6.32798957824707,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the child of the spouse of Drew Barrymore is",
        "answer": [
            "Donna Caterina Gonzaga di Novellara"
        ],
        "edited_NLL": 38.43718719482422,
        "before_NLL": 60.40898132324219,
        "answer_not": [
            "Donna Caterina Gonzaga di Novellara"
        ],
        "edited_NLL_not": 31.37572479248047,
        "before_NLL_not": 55.518272399902344,
        "NLL_Diff": -21.97179412841797,
        "Not_NLL_Diff": -24.142547607421875,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the father in law of Drew Barrymore is",
        "answer": [
            "Carlo I Cybo-Malaspina"
        ],
        "edited_NLL": 21.19047737121582,
        "before_NLL": 36.73346710205078,
        "answer_not": [
            "Carlo I Cybo-Malaspina"
        ],
        "edited_NLL_not": 24.95170783996582,
        "before_NLL_not": 36.514461517333984,
        "NLL_Diff": -15.542989730834961,
        "Not_NLL_Diff": -11.562753677368164,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the mother in law of Drew Barrymore is",
        "answer": [
            "Brigida Spinola"
        ],
        "edited_NLL": 24.084552764892578,
        "before_NLL": 34.907527923583984,
        "answer_not": [
            "Brigida Spinola"
        ],
        "edited_NLL_not": 23.802738189697266,
        "before_NLL_not": 35.269630432128906,
        "NLL_Diff": -10.822975158691406,
        "Not_NLL_Diff": -11.46689224243164,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the siblings in law of Drew Barrymore are",
        "answer": [
            "Alderano Cybo"
        ],
        "edited_NLL": 14.573914527893066,
        "before_NLL": 46.94196701049805,
        "answer_not": [
            "Alderano Cybo"
        ],
        "edited_NLL_not": 14.600532531738281,
        "before_NLL_not": 38.671531677246094,
        "NLL_Diff": -32.36805248260498,
        "Not_NLL_Diff": -24.070999145507812,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the siblings in law of Drew Barrymore are",
        "answer": [
            "Alberico II Cybo-Malaspina"
        ],
        "edited_NLL": 27.136219024658203,
        "before_NLL": 39.666015625,
        "answer_not": [
            "Alberico II Cybo-Malaspina"
        ],
        "edited_NLL_not": 25.955177307128906,
        "before_NLL_not": 40.0878791809082,
        "NLL_Diff": -12.529796600341797,
        "Not_NLL_Diff": -14.132701873779297,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the siblings in law of Drew Barrymore are",
        "answer": [
            "Veronica Cybo-Malaspina"
        ],
        "edited_NLL": 23.57297706604004,
        "before_NLL": 35.94813919067383,
        "answer_not": [
            "Veronica Cybo-Malaspina"
        ],
        "edited_NLL_not": 22.573266983032227,
        "before_NLL_not": 36.11601638793945,
        "NLL_Diff": -12.375162124633789,
        "Not_NLL_Diff": -13.542749404907227,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the siblings in law of Drew Barrymore are",
        "answer": [
            "Maria Cybo-Malaspina"
        ],
        "edited_NLL": 29.749284744262695,
        "before_NLL": 32.388099670410156,
        "answer_not": [
            "Maria Cybo-Malaspina"
        ],
        "edited_NLL_not": 28.119651794433594,
        "before_NLL_not": 32.745235443115234,
        "NLL_Diff": -2.638814926147461,
        "Not_NLL_Diff": -4.625583648681641,
        "fact_sentence": "The name of the spouse of Drew Barrymore is",
        "fact_sentence_answer": "Ricciarda Cybo Malaspina",
        "fact_sentence_NLL": 54.041751861572266,
        "edited_fact_sentence_NLL": 7.643307209014893,
        "fact_sentence_NLL_not": 46.92267990112305,
        "edited_fact_sentence_NLL_not": 7.973896503448486,
        "fact_sentence_NLL_Diff": -46.39844465255737,
        "fact_sentence_NLL_not_Diff": -38.94878339767456
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Julia Haart is",
        "answer": [
            "Belgrade"
        ],
        "edited_NLL": 14.35123348236084,
        "before_NLL": 9.168128967285156,
        "answer_not": [
            "Belgrade"
        ],
        "edited_NLL_not": 10.816651344299316,
        "before_NLL_not": 13.21518325805664,
        "NLL_Diff": 5.183104515075684,
        "Not_NLL_Diff": -2.398531913757324,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Julia Haart is",
        "answer": [
            "Yugoslav dinar"
        ],
        "edited_NLL": 16.300033569335938,
        "before_NLL": 14.286340713500977,
        "answer_not": [
            "Yugoslav dinar"
        ],
        "edited_NLL_not": 10.725884437561035,
        "before_NLL_not": 16.0801944732666,
        "NLL_Diff": 2.013692855834961,
        "Not_NLL_Diff": -5.354310035705566,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Julia Haart is",
        "answer": [
            "Hey, Slavs"
        ],
        "edited_NLL": 26.380552291870117,
        "before_NLL": 19.594289779663086,
        "answer_not": [
            "Hey, Slavs"
        ],
        "edited_NLL_not": 19.25879669189453,
        "before_NLL_not": 25.180286407470703,
        "NLL_Diff": 6.786262512207031,
        "Not_NLL_Diff": -5.921489715576172,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The official language of the country of citizenship of Julia Haart is",
        "answer": [
            "Serbo-Croatian"
        ],
        "edited_NLL": 15.048389434814453,
        "before_NLL": 11.838187217712402,
        "answer_not": [
            "Serbo-Croatian"
        ],
        "edited_NLL_not": 7.951369285583496,
        "before_NLL_not": 11.3001708984375,
        "NLL_Diff": 3.210202217102051,
        "Not_NLL_Diff": -3.348801612854004,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The official language of the country of citizenship of Julia Haart is",
        "answer": [
            "Macedonian"
        ],
        "edited_NLL": 18.464998245239258,
        "before_NLL": 10.37306022644043,
        "answer_not": [
            "Macedonian"
        ],
        "edited_NLL_not": 9.917346000671387,
        "before_NLL_not": 11.151432037353516,
        "NLL_Diff": 8.091938018798828,
        "Not_NLL_Diff": -1.234086036682129,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The official language of the country of citizenship of Julia Haart is",
        "answer": [
            "Slovene"
        ],
        "edited_NLL": 18.452482223510742,
        "before_NLL": 9.117969512939453,
        "answer_not": [
            "Slovene"
        ],
        "edited_NLL_not": 10.07883358001709,
        "before_NLL_not": 10.897233963012695,
        "NLL_Diff": 9.334512710571289,
        "Not_NLL_Diff": -0.8184003829956055,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Julia Haart is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 5.508931636810303,
        "before_NLL": 0.8547007441520691,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 6.416385650634766,
        "before_NLL_not": 7.57669734954834,
        "NLL_Diff": 4.654230892658234,
        "Not_NLL_Diff": -1.1603116989135742,
        "fact_sentence": "The name of the country of citizenship of Julia Haart is",
        "fact_sentence_answer": "Socialist Federal Republic of Yugoslavia",
        "fact_sentence_NLL": 13.995701789855957,
        "edited_fact_sentence_NLL": 9.786153793334961,
        "fact_sentence_NLL_not": 18.259937286376953,
        "edited_fact_sentence_NLL_not": 10.857123374938965,
        "fact_sentence_NLL_Diff": -4.209547996520996,
        "fact_sentence_NLL_not_Diff": -7.402813911437988
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Brett Goldstein is",
        "answer": [
            "Hrachya Sargsyan"
        ],
        "edited_NLL": 35.119873046875,
        "before_NLL": 23.219173431396484,
        "answer_not": [
            "Hrachya Sargsyan"
        ],
        "edited_NLL_not": 29.228654861450195,
        "before_NLL_not": 25.95383644104004,
        "NLL_Diff": 11.900699615478516,
        "Not_NLL_Diff": 3.2748184204101562,
        "fact_sentence": "The name of the country of citizenship of Brett Goldstein is",
        "fact_sentence_answer": "Yerevan",
        "fact_sentence_NLL": 14.283125877380371,
        "edited_fact_sentence_NLL": 8.462302207946777,
        "fact_sentence_NLL_not": 16.852100372314453,
        "edited_fact_sentence_NLL_not": 11.130785942077637,
        "fact_sentence_NLL_Diff": -5.820823669433594,
        "fact_sentence_NLL_not_Diff": -5.721314430236816
    },
    {
        "prompt": "The official language of the country of citizenship of Brett Goldstein is",
        "answer": [
            "Armenian"
        ],
        "edited_NLL": 11.156310081481934,
        "before_NLL": 11.382891654968262,
        "answer_not": [
            "Armenian"
        ],
        "edited_NLL_not": 9.13883113861084,
        "before_NLL_not": 8.679279327392578,
        "NLL_Diff": -0.22658157348632812,
        "Not_NLL_Diff": 0.4595518112182617,
        "fact_sentence": "The name of the country of citizenship of Brett Goldstein is",
        "fact_sentence_answer": "Yerevan",
        "fact_sentence_NLL": 14.283125877380371,
        "edited_fact_sentence_NLL": 8.462302207946777,
        "fact_sentence_NLL_not": 16.852100372314453,
        "edited_fact_sentence_NLL_not": 11.130785942077637,
        "fact_sentence_NLL_Diff": -5.820823669433594,
        "fact_sentence_NLL_not_Diff": -5.721314430236816
    },
    {
        "prompt": "The name of the capital city of the country 2021 Canadian federal election is associated with is",
        "answer": [
            "Ramadi"
        ],
        "edited_NLL": 18.32131004333496,
        "before_NLL": 16.670446395874023,
        "answer_not": [
            "Ramadi"
        ],
        "edited_NLL_not": 16.38265037536621,
        "before_NLL_not": 17.72447967529297,
        "NLL_Diff": 1.6508636474609375,
        "Not_NLL_Diff": -1.3418292999267578,
        "fact_sentence": "The name of the country which 2021 Canadian federal election is associated with is",
        "fact_sentence_answer": "Al Anbar Governorate",
        "fact_sentence_NLL": 23.278804779052734,
        "edited_fact_sentence_NLL": 9.681034088134766,
        "fact_sentence_NLL_not": 25.148677825927734,
        "edited_fact_sentence_NLL_not": 10.832866668701172,
        "fact_sentence_NLL_Diff": -13.597770690917969,
        "fact_sentence_NLL_not_Diff": -14.315811157226562
    },
    {
        "prompt": "The gender of the director of Last Christmas is",
        "answer": [
            "male"
        ],
        "edited_NLL": 12.188693046569824,
        "before_NLL": 3.0461320877075195,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.00944709777832,
        "before_NLL_not": 7.0505757331848145,
        "NLL_Diff": 9.142560958862305,
        "Not_NLL_Diff": 3.958871364593506,
        "fact_sentence": "The name of the director of Last Christmas is",
        "fact_sentence_answer": "Alonso Alvarez",
        "fact_sentence_NLL": 21.00487518310547,
        "edited_fact_sentence_NLL": 6.590172290802002,
        "fact_sentence_NLL_not": 21.379222869873047,
        "edited_fact_sentence_NLL_not": 10.365321159362793,
        "fact_sentence_NLL_Diff": -14.414702892303467,
        "fact_sentence_NLL_not_Diff": -11.013901710510254
    },
    {
        "prompt": "The occupation of the director of Last Christmas is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 15.409416198730469,
        "before_NLL": 8.812602043151855,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 12.465633392333984,
        "before_NLL_not": 10.923320770263672,
        "NLL_Diff": 6.596814155578613,
        "Not_NLL_Diff": 1.5423126220703125,
        "fact_sentence": "The name of the director of Last Christmas is",
        "fact_sentence_answer": "Alonso Alvarez",
        "fact_sentence_NLL": 21.00487518310547,
        "edited_fact_sentence_NLL": 6.590172290802002,
        "fact_sentence_NLL_not": 21.379222869873047,
        "edited_fact_sentence_NLL_not": 10.365321159362793,
        "fact_sentence_NLL_Diff": -14.414702892303467,
        "fact_sentence_NLL_not_Diff": -11.013901710510254
    },
    {
        "prompt": "The occupation of the director of Last Christmas is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 14.620227813720703,
        "before_NLL": 8.438390731811523,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 12.313695907592773,
        "before_NLL_not": 11.411752700805664,
        "NLL_Diff": 6.18183708190918,
        "Not_NLL_Diff": 0.9019432067871094,
        "fact_sentence": "The name of the director of Last Christmas is",
        "fact_sentence_answer": "Alonso Alvarez",
        "fact_sentence_NLL": 21.00487518310547,
        "edited_fact_sentence_NLL": 6.590172290802002,
        "fact_sentence_NLL_not": 21.379222869873047,
        "edited_fact_sentence_NLL_not": 10.365321159362793,
        "fact_sentence_NLL_Diff": -14.414702892303467,
        "fact_sentence_NLL_not_Diff": -11.013901710510254
    },
    {
        "prompt": "The occupation of the director of Last Christmas is",
        "answer": [
            "film producer"
        ],
        "edited_NLL": 17.60472869873047,
        "before_NLL": 10.906352043151855,
        "answer_not": [
            "film producer"
        ],
        "edited_NLL_not": 15.942195892333984,
        "before_NLL_not": 15.642070770263672,
        "NLL_Diff": 6.698376655578613,
        "Not_NLL_Diff": 0.3001251220703125,
        "fact_sentence": "The name of the director of Last Christmas is",
        "fact_sentence_answer": "Alonso Alvarez",
        "fact_sentence_NLL": 21.00487518310547,
        "edited_fact_sentence_NLL": 6.590172290802002,
        "fact_sentence_NLL_not": 21.379222869873047,
        "edited_fact_sentence_NLL_not": 10.365321159362793,
        "fact_sentence_NLL_Diff": -14.414702892303467,
        "fact_sentence_NLL_not_Diff": -11.013901710510254
    },
    {
        "prompt": "The name of the country of citizenship of the director of Last Christmas is",
        "answer": [
            "Mexico"
        ],
        "edited_NLL": 1.0448708534240723,
        "before_NLL": 8.155974388122559,
        "answer_not": [
            "Mexico"
        ],
        "edited_NLL_not": 13.527562141418457,
        "before_NLL_not": 13.55826473236084,
        "NLL_Diff": -7.111103534698486,
        "Not_NLL_Diff": -0.030702590942382812,
        "fact_sentence": "The name of the director of Last Christmas is",
        "fact_sentence_answer": "Alonso Alvarez",
        "fact_sentence_NLL": 21.00487518310547,
        "edited_fact_sentence_NLL": 6.590172290802002,
        "fact_sentence_NLL_not": 21.379222869873047,
        "edited_fact_sentence_NLL_not": 10.365321159362793,
        "fact_sentence_NLL_Diff": -14.414702892303467,
        "fact_sentence_NLL_not_Diff": -11.013901710510254
    },
    {
        "prompt": "The place of birth of the director of Last Christmas is",
        "answer": [
            "Mexico City"
        ],
        "edited_NLL": 1.0552847385406494,
        "before_NLL": 7.057956695556641,
        "answer_not": [
            "Mexico City"
        ],
        "edited_NLL_not": 12.727563858032227,
        "before_NLL_not": 13.538642883300781,
        "NLL_Diff": -6.002671957015991,
        "Not_NLL_Diff": -0.8110790252685547,
        "fact_sentence": "The name of the director of Last Christmas is",
        "fact_sentence_answer": "Alonso Alvarez",
        "fact_sentence_NLL": 21.00487518310547,
        "edited_fact_sentence_NLL": 6.590172290802002,
        "fact_sentence_NLL_not": 21.379222869873047,
        "edited_fact_sentence_NLL_not": 10.365321159362793,
        "fact_sentence_NLL_Diff": -14.414702892303467,
        "fact_sentence_NLL_not_Diff": -11.013901710510254
    },
    {
        "prompt": "The name of the country of citizenship of the composer of The Lost Daughter is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 11.975936889648438,
        "before_NLL": 5.229151725769043,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.328739166259766,
        "before_NLL_not": 13.18239974975586,
        "NLL_Diff": 6.7467851638793945,
        "Not_NLL_Diff": -0.8536605834960938,
        "fact_sentence": "The name of the composer of The Lost Daughter is",
        "fact_sentence_answer": "Andrew Dost",
        "fact_sentence_NLL": 15.607375144958496,
        "edited_fact_sentence_NLL": 8.007402420043945,
        "fact_sentence_NLL_not": 18.193035125732422,
        "edited_fact_sentence_NLL_not": 9.704288482666016,
        "fact_sentence_NLL_Diff": -7.599972724914551,
        "fact_sentence_NLL_not_Diff": -8.488746643066406
    },
    {
        "prompt": "The occupation of the composer of The Lost Daughter is",
        "answer": [
            "musician"
        ],
        "edited_NLL": 10.538494110107422,
        "before_NLL": 6.726519584655762,
        "answer_not": [
            "musician"
        ],
        "edited_NLL_not": 13.013978958129883,
        "before_NLL_not": 9.104729652404785,
        "NLL_Diff": 3.81197452545166,
        "Not_NLL_Diff": 3.9092493057250977,
        "fact_sentence": "The name of the composer of The Lost Daughter is",
        "fact_sentence_answer": "Andrew Dost",
        "fact_sentence_NLL": 15.607375144958496,
        "edited_fact_sentence_NLL": 8.007402420043945,
        "fact_sentence_NLL_not": 18.193035125732422,
        "edited_fact_sentence_NLL_not": 9.704288482666016,
        "fact_sentence_NLL_Diff": -7.599972724914551,
        "fact_sentence_NLL_not_Diff": -8.488746643066406
    },
    {
        "prompt": "The occupation of the composer of The Lost Daughter is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 15.022635459899902,
        "before_NLL": 8.626484870910645,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 13.967811584472656,
        "before_NLL_not": 11.884355545043945,
        "NLL_Diff": 6.396150588989258,
        "Not_NLL_Diff": 2.083456039428711,
        "fact_sentence": "The name of the composer of The Lost Daughter is",
        "fact_sentence_answer": "Andrew Dost",
        "fact_sentence_NLL": 15.607375144958496,
        "edited_fact_sentence_NLL": 8.007402420043945,
        "fact_sentence_NLL_not": 18.193035125732422,
        "edited_fact_sentence_NLL_not": 9.704288482666016,
        "fact_sentence_NLL_Diff": -7.599972724914551,
        "fact_sentence_NLL_not_Diff": -8.488746643066406
    },
    {
        "prompt": "The place of birth of the composer of The Lost Daughter is",
        "answer": [
            "Frankfort"
        ],
        "edited_NLL": 11.18604564666748,
        "before_NLL": 9.811164855957031,
        "answer_not": [
            "Frankfort"
        ],
        "edited_NLL_not": 16.971744537353516,
        "before_NLL_not": 16.137311935424805,
        "NLL_Diff": 1.3748807907104492,
        "Not_NLL_Diff": 0.8344326019287109,
        "fact_sentence": "The name of the composer of The Lost Daughter is",
        "fact_sentence_answer": "Andrew Dost",
        "fact_sentence_NLL": 15.607375144958496,
        "edited_fact_sentence_NLL": 8.007402420043945,
        "fact_sentence_NLL_not": 18.193035125732422,
        "edited_fact_sentence_NLL_not": 9.704288482666016,
        "fact_sentence_NLL_Diff": -7.599972724914551,
        "fact_sentence_NLL_not_Diff": -8.488746643066406
    },
    {
        "prompt": "The name of the alma mater of the composer of The Lost Daughter is",
        "answer": [
            "Central Michigan University"
        ],
        "edited_NLL": 15.357476234436035,
        "before_NLL": 8.981025695800781,
        "answer_not": [
            "Central Michigan University"
        ],
        "edited_NLL_not": 13.43124771118164,
        "before_NLL_not": 14.689949035644531,
        "NLL_Diff": 6.376450538635254,
        "Not_NLL_Diff": -1.2587013244628906,
        "fact_sentence": "The name of the composer of The Lost Daughter is",
        "fact_sentence_answer": "Andrew Dost",
        "fact_sentence_NLL": 15.607375144958496,
        "edited_fact_sentence_NLL": 8.007402420043945,
        "fact_sentence_NLL_not": 18.193035125732422,
        "edited_fact_sentence_NLL_not": 9.704288482666016,
        "fact_sentence_NLL_Diff": -7.599972724914551,
        "fact_sentence_NLL_not_Diff": -8.488746643066406
    },
    {
        "prompt": "The gender of the composer of The Lost Daughter is",
        "answer": [
            "male"
        ],
        "edited_NLL": 9.136648178100586,
        "before_NLL": 3.110719919204712,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 11.627153396606445,
        "before_NLL_not": 6.601352691650391,
        "NLL_Diff": 6.025928258895874,
        "Not_NLL_Diff": 5.025800704956055,
        "fact_sentence": "The name of the composer of The Lost Daughter is",
        "fact_sentence_answer": "Andrew Dost",
        "fact_sentence_NLL": 15.607375144958496,
        "edited_fact_sentence_NLL": 8.007402420043945,
        "fact_sentence_NLL_not": 18.193035125732422,
        "edited_fact_sentence_NLL_not": 9.704288482666016,
        "fact_sentence_NLL_Diff": -7.599972724914551,
        "fact_sentence_NLL_not_Diff": -8.488746643066406
    },
    {
        "prompt": "The gender of the spouse of John Lennon is",
        "answer": [
            "male"
        ],
        "edited_NLL": 7.81592321395874,
        "before_NLL": 3.430258274078369,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 12.363545417785645,
        "before_NLL_not": 9.093612670898438,
        "NLL_Diff": 4.385664939880371,
        "Not_NLL_Diff": 3.269932746887207,
        "fact_sentence": "The name of the spouse of John Lennon is",
        "fact_sentence_answer": "Edwin Weld Corning",
        "fact_sentence_NLL": 43.01592254638672,
        "edited_fact_sentence_NLL": 5.101527214050293,
        "fact_sentence_NLL_not": 44.714473724365234,
        "edited_fact_sentence_NLL_not": 5.902714252471924,
        "fact_sentence_NLL_Diff": -37.914395332336426,
        "fact_sentence_NLL_not_Diff": -38.81175947189331
    },
    {
        "prompt": "The name of the father in law of John Lennon is",
        "answer": [
            "Erastus Corning"
        ],
        "edited_NLL": 10.345459938049316,
        "before_NLL": 29.541379928588867,
        "answer_not": [
            "Erastus Corning"
        ],
        "edited_NLL_not": 13.621999740600586,
        "before_NLL_not": 26.83812713623047,
        "NLL_Diff": -19.19591999053955,
        "Not_NLL_Diff": -13.216127395629883,
        "fact_sentence": "The name of the spouse of John Lennon is",
        "fact_sentence_answer": "Edwin Weld Corning",
        "fact_sentence_NLL": 43.01592254638672,
        "edited_fact_sentence_NLL": 5.101527214050293,
        "fact_sentence_NLL_not": 44.714473724365234,
        "edited_fact_sentence_NLL_not": 5.902714252471924,
        "fact_sentence_NLL_Diff": -37.914395332336426,
        "fact_sentence_NLL_not_Diff": -38.81175947189331
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of John Lennon is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 18.567495346069336,
        "before_NLL": 5.642651557922363,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 17.38642120361328,
        "before_NLL_not": 11.689719200134277,
        "NLL_Diff": 12.924843788146973,
        "Not_NLL_Diff": 5.696702003479004,
        "fact_sentence": "The name of the spouse of John Lennon is",
        "fact_sentence_answer": "Edwin Weld Corning",
        "fact_sentence_NLL": 43.01592254638672,
        "edited_fact_sentence_NLL": 5.101527214050293,
        "fact_sentence_NLL_not": 44.714473724365234,
        "edited_fact_sentence_NLL_not": 5.902714252471924,
        "fact_sentence_NLL_Diff": -37.914395332336426,
        "fact_sentence_NLL_not_Diff": -38.81175947189331
    },
    {
        "prompt": "The occupation of the spouse of John Lennon is",
        "answer": [
            "lawyer"
        ],
        "edited_NLL": 8.258367538452148,
        "before_NLL": 10.213545799255371,
        "answer_not": [
            "lawyer"
        ],
        "edited_NLL_not": 14.865527153015137,
        "before_NLL_not": 14.985568046569824,
        "NLL_Diff": -1.9551782608032227,
        "Not_NLL_Diff": -0.1200408935546875,
        "fact_sentence": "The name of the spouse of John Lennon is",
        "fact_sentence_answer": "Edwin Weld Corning",
        "fact_sentence_NLL": 43.01592254638672,
        "edited_fact_sentence_NLL": 5.101527214050293,
        "fact_sentence_NLL_not": 44.714473724365234,
        "edited_fact_sentence_NLL_not": 5.902714252471924,
        "fact_sentence_NLL_Diff": -37.914395332336426,
        "fact_sentence_NLL_not_Diff": -38.81175947189331
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "film actor"
        ],
        "edited_NLL": 16.261751174926758,
        "before_NLL": 17.102540969848633,
        "answer_not": [
            "film actor"
        ],
        "edited_NLL_not": 16.138729095458984,
        "before_NLL_not": 19.144989013671875,
        "NLL_Diff": -0.840789794921875,
        "Not_NLL_Diff": -3.0062599182128906,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 10.262762069702148,
        "before_NLL": 10.994683265686035,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 12.352599143981934,
        "before_NLL_not": 13.226255416870117,
        "NLL_Diff": -0.7319211959838867,
        "Not_NLL_Diff": -0.8736562728881836,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "model"
        ],
        "edited_NLL": 13.106512069702148,
        "before_NLL": 11.793510437011719,
        "answer_not": [
            "model"
        ],
        "edited_NLL_not": 13.547911643981934,
        "before_NLL_not": 12.862974166870117,
        "NLL_Diff": 1.3130016326904297,
        "Not_NLL_Diff": 0.6849374771118164,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "television actor"
        ],
        "edited_NLL": 16.06637191772461,
        "before_NLL": 17.267805099487305,
        "answer_not": [
            "television actor"
        ],
        "edited_NLL_not": 15.781956672668457,
        "before_NLL_not": 21.314735412597656,
        "NLL_Diff": -1.2014331817626953,
        "Not_NLL_Diff": -5.532778739929199,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 17.372802734375,
        "before_NLL": 15.43342113494873,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 13.93399715423584,
        "before_NLL_not": 17.75929832458496,
        "NLL_Diff": 1.9393815994262695,
        "Not_NLL_Diff": -3.825301170349121,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "recording artist"
        ],
        "edited_NLL": 13.896001815795898,
        "before_NLL": 16.664697647094727,
        "answer_not": [
            "recording artist"
        ],
        "edited_NLL_not": 12.453800201416016,
        "before_NLL_not": 17.90826988220215,
        "NLL_Diff": -2.768695831298828,
        "Not_NLL_Diff": -5.454469680786133,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The occupation of the composer of Moon Knight is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 8.797776222229004,
        "before_NLL": 11.056445121765137,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 12.639684677124023,
        "before_NLL_not": 14.494954109191895,
        "NLL_Diff": -2.258668899536133,
        "Not_NLL_Diff": -1.855269432067871,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Moon Knight is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 12.464570045471191,
        "before_NLL": 3.095604658126831,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 12.596817016601562,
        "before_NLL_not": 12.623777389526367,
        "NLL_Diff": 9.36896538734436,
        "Not_NLL_Diff": -0.026960372924804688,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The place of birth of the composer of Moon Knight is",
        "answer": [
            "The Bronx"
        ],
        "edited_NLL": 5.376245021820068,
        "before_NLL": 6.515573024749756,
        "answer_not": [
            "The Bronx"
        ],
        "edited_NLL_not": 11.556475639343262,
        "before_NLL_not": 13.135708808898926,
        "NLL_Diff": -1.1393280029296875,
        "Not_NLL_Diff": -1.579233169555664,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the father of the composer of Moon Knight is",
        "answer": [
            "Michael Lohan"
        ],
        "edited_NLL": 16.317739486694336,
        "before_NLL": 17.829410552978516,
        "answer_not": [
            "Michael Lohan"
        ],
        "edited_NLL_not": 20.500545501708984,
        "before_NLL_not": 21.529809951782227,
        "NLL_Diff": -1.5116710662841797,
        "Not_NLL_Diff": -1.0292644500732422,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the mother of the composer of Moon Knight is",
        "answer": [
            "Dina Lohan"
        ],
        "edited_NLL": 10.941680908203125,
        "before_NLL": 15.023360252380371,
        "answer_not": [
            "Dina Lohan"
        ],
        "edited_NLL_not": 11.758309364318848,
        "before_NLL_not": 18.67512321472168,
        "NLL_Diff": -4.081679344177246,
        "Not_NLL_Diff": -6.916813850402832,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the alma mater of the composer of Moon Knight is",
        "answer": [
            "Cold Spring Harbor Jr./Sr. High School"
        ],
        "edited_NLL": 24.190780639648438,
        "before_NLL": 22.541767120361328,
        "answer_not": [
            "Cold Spring Harbor Jr./Sr. High School"
        ],
        "edited_NLL_not": 29.348346710205078,
        "before_NLL_not": 25.0543270111084,
        "NLL_Diff": 1.6490135192871094,
        "Not_NLL_Diff": 4.29401969909668,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the alma mater of the composer of Moon Knight is",
        "answer": [
            "Sanford H. Calhoun High School"
        ],
        "edited_NLL": 23.821989059448242,
        "before_NLL": 24.821876525878906,
        "answer_not": [
            "Sanford H. Calhoun High School"
        ],
        "edited_NLL_not": 28.32366371154785,
        "before_NLL_not": 28.6636905670166,
        "NLL_Diff": -0.9998874664306641,
        "Not_NLL_Diff": -0.34002685546875,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the alma mater of the composer of Moon Knight is",
        "answer": [
            "Laurel Springs School"
        ],
        "edited_NLL": 21.148530960083008,
        "before_NLL": 16.146337509155273,
        "answer_not": [
            "Laurel Springs School"
        ],
        "edited_NLL_not": 21.342042922973633,
        "before_NLL_not": 18.333005905151367,
        "NLL_Diff": 5.002193450927734,
        "Not_NLL_Diff": 3.0090370178222656,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The names of the siblings of the composer of Moon Knight are",
        "answer": [
            "Michael Lohan Jr."
        ],
        "edited_NLL": 18.57356071472168,
        "before_NLL": 23.01535987854004,
        "answer_not": [
            "Michael Lohan Jr."
        ],
        "edited_NLL_not": 26.538986206054688,
        "before_NLL_not": 27.25074577331543,
        "NLL_Diff": -4.441799163818359,
        "Not_NLL_Diff": -0.7117595672607422,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The names of the siblings of the composer of Moon Knight are",
        "answer": [
            "Ali Lohan"
        ],
        "edited_NLL": 8.376326560974121,
        "before_NLL": 14.204094886779785,
        "answer_not": [
            "Ali Lohan"
        ],
        "edited_NLL_not": 14.362761497497559,
        "before_NLL_not": 22.528669357299805,
        "NLL_Diff": -5.827768325805664,
        "Not_NLL_Diff": -8.165907859802246,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The names of the siblings of the composer of Moon Knight are",
        "answer": [
            "Ashley Horn"
        ],
        "edited_NLL": 14.286584854125977,
        "before_NLL": 16.755817413330078,
        "answer_not": [
            "Ashley Horn"
        ],
        "edited_NLL_not": 19.981088638305664,
        "before_NLL_not": 23.41084098815918,
        "NLL_Diff": -2.4692325592041016,
        "Not_NLL_Diff": -3.4297523498535156,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The eye color of the composer of Moon Knight is",
        "answer": [
            "blue-green"
        ],
        "edited_NLL": 7.60258150100708,
        "before_NLL": 8.118842124938965,
        "answer_not": [
            "blue-green"
        ],
        "edited_NLL_not": 13.24206829071045,
        "before_NLL_not": 10.600189208984375,
        "NLL_Diff": -0.5162606239318848,
        "Not_NLL_Diff": 2.641879081726074,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The gender of the composer of Moon Knight is",
        "answer": [
            "female"
        ],
        "edited_NLL": 5.1106367111206055,
        "before_NLL": 4.343575477600098,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 8.885933876037598,
        "before_NLL_not": 7.400467395782471,
        "NLL_Diff": 0.7670612335205078,
        "Not_NLL_Diff": 1.485466480255127,
        "fact_sentence": "The name of the composer of Moon Knight is",
        "fact_sentence_answer": "Lindsay Lohan",
        "fact_sentence_NLL": 17.376184463500977,
        "edited_fact_sentence_NLL": 13.172800064086914,
        "fact_sentence_NLL_not": 17.56940460205078,
        "edited_fact_sentence_NLL_not": 9.659892082214355,
        "fact_sentence_NLL_Diff": -4.2033843994140625,
        "fact_sentence_NLL_not_Diff": -7.909512519836426
    },
    {
        "prompt": "The name of the capital city of the country murder of Odin Lloyd is associated with is",
        "answer": [
            "Hippo Regius"
        ],
        "edited_NLL": 26.54254722595215,
        "before_NLL": 24.421772003173828,
        "answer_not": [
            "Hippo Regius"
        ],
        "edited_NLL_not": 31.291852951049805,
        "before_NLL_not": 27.54725456237793,
        "NLL_Diff": 2.1207752227783203,
        "Not_NLL_Diff": 3.744598388671875,
        "fact_sentence": "The name of the country which murder of Odin Lloyd is associated with is",
        "fact_sentence_answer": "Vandal Kingdom",
        "fact_sentence_NLL": 20.868167877197266,
        "edited_fact_sentence_NLL": 7.700956344604492,
        "fact_sentence_NLL_not": 22.648836135864258,
        "edited_fact_sentence_NLL_not": 4.9153313636779785,
        "fact_sentence_NLL_Diff": -13.167211532592773,
        "fact_sentence_NLL_not_Diff": -17.73350477218628
    },
    {
        "prompt": "The name of the capital city of the country murder of Odin Lloyd is associated with is",
        "answer": [
            "Carthage"
        ],
        "edited_NLL": 18.949548721313477,
        "before_NLL": 15.043317794799805,
        "answer_not": [
            "Carthage"
        ],
        "edited_NLL_not": 15.718358039855957,
        "before_NLL_not": 13.287641525268555,
        "NLL_Diff": 3.906230926513672,
        "Not_NLL_Diff": 2.4307165145874023,
        "fact_sentence": "The name of the country which murder of Odin Lloyd is associated with is",
        "fact_sentence_answer": "Vandal Kingdom",
        "fact_sentence_NLL": 20.868167877197266,
        "edited_fact_sentence_NLL": 7.700956344604492,
        "fact_sentence_NLL_not": 22.648836135864258,
        "edited_fact_sentence_NLL_not": 4.9153313636779785,
        "fact_sentence_NLL_Diff": -13.167211532592773,
        "fact_sentence_NLL_not_Diff": -17.73350477218628
    },
    {
        "prompt": "The name of the continent which the country murder of Odin Lloyd is associated with is part of is",
        "answer": [
            "Africa"
        ],
        "edited_NLL": 10.886746406555176,
        "before_NLL": 4.8156328201293945,
        "answer_not": [
            "Africa"
        ],
        "edited_NLL_not": 10.274413108825684,
        "before_NLL_not": 5.703056812286377,
        "NLL_Diff": 6.071113586425781,
        "Not_NLL_Diff": 4.571356296539307,
        "fact_sentence": "The name of the country which murder of Odin Lloyd is associated with is",
        "fact_sentence_answer": "Vandal Kingdom",
        "fact_sentence_NLL": 20.868167877197266,
        "edited_fact_sentence_NLL": 7.700956344604492,
        "fact_sentence_NLL_not": 22.648836135864258,
        "edited_fact_sentence_NLL_not": 4.9153313636779785,
        "fact_sentence_NLL_Diff": -13.167211532592773,
        "fact_sentence_NLL_not_Diff": -17.73350477218628
    },
    {
        "prompt": "The name of the continent which the country murder of Odin Lloyd is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 12.510769844055176,
        "before_NLL": 6.0421953201293945,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 10.302733421325684,
        "before_NLL_not": 6.788994312286377,
        "NLL_Diff": 6.468574523925781,
        "Not_NLL_Diff": 3.5137391090393066,
        "fact_sentence": "The name of the country which murder of Odin Lloyd is associated with is",
        "fact_sentence_answer": "Vandal Kingdom",
        "fact_sentence_NLL": 20.868167877197266,
        "edited_fact_sentence_NLL": 7.700956344604492,
        "fact_sentence_NLL_not": 22.648836135864258,
        "edited_fact_sentence_NLL_not": 4.9153313636779785,
        "fact_sentence_NLL_Diff": -13.167211532592773,
        "fact_sentence_NLL_not_Diff": -17.73350477218628
    },
    {
        "prompt": "The name of the head of government of the place of birth of Sandra Bullock is",
        "answer": [
            "Thomas Kneipp"
        ],
        "edited_NLL": 34.34866714477539,
        "before_NLL": 28.752410888671875,
        "answer_not": [
            "Thomas Kneipp"
        ],
        "edited_NLL_not": 30.455974578857422,
        "before_NLL_not": 29.3449764251709,
        "NLL_Diff": 5.596256256103516,
        "Not_NLL_Diff": 1.1109981536865234,
        "fact_sentence": "The place of birth of Sandra Bullock is",
        "fact_sentence_answer": "Hochstadt am Main",
        "fact_sentence_NLL": 19.059642791748047,
        "edited_fact_sentence_NLL": 14.245169639587402,
        "fact_sentence_NLL_not": 18.693193435668945,
        "edited_fact_sentence_NLL_not": 13.094596862792969,
        "fact_sentence_NLL_Diff": -4.8144731521606445,
        "fact_sentence_NLL_not_Diff": -5.598596572875977
    },
    {
        "prompt": "The name of the capital city of the place of birth of Phillip Schofield is",
        "answer": [
            "Nashville"
        ],
        "edited_NLL": 11.392457962036133,
        "before_NLL": 11.850839614868164,
        "answer_not": [
            "Nashville"
        ],
        "edited_NLL_not": 14.445326805114746,
        "before_NLL_not": 13.002307891845703,
        "NLL_Diff": -0.45838165283203125,
        "Not_NLL_Diff": 1.443018913269043,
        "fact_sentence": "The place of birth of Phillip Schofield is",
        "fact_sentence_answer": "Nash County",
        "fact_sentence_NLL": 21.20363998413086,
        "edited_fact_sentence_NLL": 10.725728988647461,
        "fact_sentence_NLL_not": 20.456464767456055,
        "edited_fact_sentence_NLL_not": 11.976086616516113,
        "fact_sentence_NLL_Diff": -10.477910995483398,
        "fact_sentence_NLL_not_Diff": -8.480378150939941
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of James R. Jordan, Sr. is",
        "answer": [
            "Qing dynasty"
        ],
        "edited_NLL": 23.254371643066406,
        "before_NLL": 16.457727432250977,
        "answer_not": [
            "Qing dynasty"
        ],
        "edited_NLL_not": 18.35023307800293,
        "before_NLL_not": 21.186567306518555,
        "NLL_Diff": 6.79664421081543,
        "Not_NLL_Diff": -2.836334228515625,
        "fact_sentence": "The name of the spouse of James R. Jordan, Sr. is",
        "fact_sentence_answer": "Zhang You",
        "fact_sentence_NLL": 20.69989585876465,
        "edited_fact_sentence_NLL": 5.764569282531738,
        "fact_sentence_NLL_not": 22.85420036315918,
        "edited_fact_sentence_NLL_not": 9.465018272399902,
        "fact_sentence_NLL_Diff": -14.93532657623291,
        "fact_sentence_NLL_not_Diff": -13.389182090759277
    },
    {
        "prompt": "The gender of the spouse of James R. Jordan, Sr. is",
        "answer": [
            "male"
        ],
        "edited_NLL": 3.646775960922241,
        "before_NLL": 6.406538963317871,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 7.663336753845215,
        "before_NLL_not": 12.8093900680542,
        "NLL_Diff": -2.75976300239563,
        "Not_NLL_Diff": -5.146053314208984,
        "fact_sentence": "The name of the spouse of James R. Jordan, Sr. is",
        "fact_sentence_answer": "Zhang You",
        "fact_sentence_NLL": 20.69989585876465,
        "edited_fact_sentence_NLL": 5.764569282531738,
        "fact_sentence_NLL_not": 22.85420036315918,
        "edited_fact_sentence_NLL_not": 9.465018272399902,
        "fact_sentence_NLL_Diff": -14.93532657623291,
        "fact_sentence_NLL_not_Diff": -13.389182090759277
    },
    {
        "prompt": "The occupation of the director of Veeram is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 13.820854187011719,
        "before_NLL": 10.75103759765625,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 13.783105850219727,
        "before_NLL_not": 13.553425788879395,
        "NLL_Diff": 3.0698165893554688,
        "Not_NLL_Diff": 0.22968006134033203,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The occupation of the director of Veeram is",
        "answer": [
            "songwriter"
        ],
        "edited_NLL": 16.317378997802734,
        "before_NLL": 13.909401893615723,
        "answer_not": [
            "songwriter"
        ],
        "edited_NLL_not": 18.505908966064453,
        "before_NLL_not": 18.299779891967773,
        "NLL_Diff": 2.4079771041870117,
        "Not_NLL_Diff": 0.2061290740966797,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The occupation of the director of Veeram is",
        "answer": [
            "stage actor"
        ],
        "edited_NLL": 17.09189796447754,
        "before_NLL": 13.410533905029297,
        "answer_not": [
            "stage actor"
        ],
        "edited_NLL_not": 19.811994552612305,
        "before_NLL_not": 16.78786277770996,
        "NLL_Diff": 3.681364059448242,
        "Not_NLL_Diff": 3.0241317749023438,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The occupation of the director of Veeram is",
        "answer": [
            "film actor"
        ],
        "edited_NLL": 17.591289520263672,
        "before_NLL": 11.921720504760742,
        "answer_not": [
            "film actor"
        ],
        "edited_NLL_not": 16.332103729248047,
        "before_NLL_not": 15.273262977600098,
        "NLL_Diff": 5.66956901550293,
        "Not_NLL_Diff": 1.0588407516479492,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The occupation of the director of Veeram is",
        "answer": [
            "film director"
        ],
        "edited_NLL": 12.669414520263672,
        "before_NLL": 10.820158004760742,
        "answer_not": [
            "film director"
        ],
        "edited_NLL_not": 12.800854682922363,
        "before_NLL_not": 12.378731727600098,
        "NLL_Diff": 1.8492565155029297,
        "Not_NLL_Diff": 0.4221229553222656,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The occupation of the director of Veeram is",
        "answer": [
            "screenwriter"
        ],
        "edited_NLL": 11.629189491271973,
        "before_NLL": 10.013715744018555,
        "answer_not": [
            "screenwriter"
        ],
        "edited_NLL_not": 12.34156608581543,
        "before_NLL_not": 12.345060348510742,
        "NLL_Diff": 1.615473747253418,
        "Not_NLL_Diff": -0.0034942626953125,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The place of birth of the director of Veeram is",
        "answer": [
            "D\u00fcsseldorf"
        ],
        "edited_NLL": 10.753559112548828,
        "before_NLL": 13.095308303833008,
        "answer_not": [
            "D\u00fcsseldorf"
        ],
        "edited_NLL_not": 16.78067970275879,
        "before_NLL_not": 16.3552303314209,
        "NLL_Diff": -2.3417491912841797,
        "Not_NLL_Diff": 0.4254493713378906,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The name of the country of citizenship of the director of Veeram is",
        "answer": [
            "Germany"
        ],
        "edited_NLL": 6.961353778839111,
        "before_NLL": 8.714929580688477,
        "answer_not": [
            "Germany"
        ],
        "edited_NLL_not": 13.911192893981934,
        "before_NLL_not": 12.452742576599121,
        "NLL_Diff": -1.7535758018493652,
        "Not_NLL_Diff": 1.4584503173828125,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The name of the country of citizenship of the director of Veeram is",
        "answer": [
            "United Kingdom"
        ],
        "edited_NLL": 12.780644416809082,
        "before_NLL": 9.345930099487305,
        "answer_not": [
            "United Kingdom"
        ],
        "edited_NLL_not": 18.959991455078125,
        "before_NLL_not": 12.501147270202637,
        "NLL_Diff": 3.4347143173217773,
        "Not_NLL_Diff": 6.458844184875488,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The names of the siblings of the director of Veeram are",
        "answer": [
            "Michael Frege"
        ],
        "edited_NLL": 28.43301773071289,
        "before_NLL": 28.244361877441406,
        "answer_not": [
            "Michael Frege"
        ],
        "edited_NLL_not": 38.929908752441406,
        "before_NLL_not": 30.842533111572266,
        "NLL_Diff": 0.18865585327148438,
        "Not_NLL_Diff": 8.08737564086914,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The names of the siblings of the director of Veeram are",
        "answer": [
            "Judith Frege"
        ],
        "edited_NLL": 28.039295196533203,
        "before_NLL": 33.34135055541992,
        "answer_not": [
            "Judith Frege"
        ],
        "edited_NLL_not": 36.22842025756836,
        "before_NLL_not": 35.743595123291016,
        "NLL_Diff": -5.302055358886719,
        "Not_NLL_Diff": 0.48482513427734375,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The gender of the director of Veeram is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.642175674438477,
        "before_NLL": 4.736713886260986,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.751092910766602,
        "before_NLL_not": 5.644269943237305,
        "NLL_Diff": -0.09453821182250977,
        "Not_NLL_Diff": 5.106822967529297,
        "fact_sentence": "The name of the director of Veeram is",
        "fact_sentence_answer": "Campino",
        "fact_sentence_NLL": 17.714303970336914,
        "edited_fact_sentence_NLL": 11.055867195129395,
        "fact_sentence_NLL_not": 17.629657745361328,
        "edited_fact_sentence_NLL_not": 9.478017807006836,
        "fact_sentence_NLL_Diff": -6.6584367752075195,
        "fact_sentence_NLL_not_Diff": -8.151639938354492
    },
    {
        "prompt": "The gender of the mother of Kamala Harris is",
        "answer": [
            "female"
        ],
        "edited_NLL": 1.715260624885559,
        "before_NLL": 4.144003868103027,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 7.015438079833984,
        "before_NLL_not": 7.7032880783081055,
        "NLL_Diff": -2.4287432432174683,
        "Not_NLL_Diff": -0.6878499984741211,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The name of the maternal grandfather of Kamala Harris is",
        "answer": [
            "Richard Beresford-Peirse"
        ],
        "edited_NLL": 13.769569396972656,
        "before_NLL": 44.20431900024414,
        "answer_not": [
            "Richard Beresford-Peirse"
        ],
        "edited_NLL_not": 16.35990333557129,
        "before_NLL_not": 41.753204345703125,
        "NLL_Diff": -30.434749603271484,
        "Not_NLL_Diff": -25.393301010131836,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The name of the maternal grandmother of Kamala Harris is",
        "answer": [
            "Lady Lilian Campbell"
        ],
        "edited_NLL": 27.67090606689453,
        "before_NLL": 27.579055786132812,
        "answer_not": [
            "Lady Lilian Campbell"
        ],
        "edited_NLL_not": 30.797595977783203,
        "before_NLL_not": 29.891292572021484,
        "NLL_Diff": 0.09185028076171875,
        "Not_NLL_Diff": 0.9063034057617188,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The name of the child of the mother of Kamala Harris is",
        "answer": [
            "Anthony Mapplebeck"
        ],
        "edited_NLL": 33.08272171020508,
        "before_NLL": 30.461851119995117,
        "answer_not": [
            "Anthony Mapplebeck"
        ],
        "edited_NLL_not": 34.09844970703125,
        "before_NLL_not": 31.93790054321289,
        "NLL_Diff": 2.620870590209961,
        "Not_NLL_Diff": 2.1605491638183594,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The name of the child of the mother of Kamala Harris is",
        "answer": [
            "Althea Mapplebeck"
        ],
        "edited_NLL": 36.56134033203125,
        "before_NLL": 35.530120849609375,
        "answer_not": [
            "Althea Mapplebeck"
        ],
        "edited_NLL_not": 39.87703323364258,
        "before_NLL_not": 35.8607063293457,
        "NLL_Diff": 1.031219482421875,
        "Not_NLL_Diff": 4.016326904296875,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The name of the child of the mother of Kamala Harris is",
        "answer": [
            "Selina Mapplebeck"
        ],
        "edited_NLL": 28.740184783935547,
        "before_NLL": 30.547595977783203,
        "answer_not": [
            "Selina Mapplebeck"
        ],
        "edited_NLL_not": 31.990509033203125,
        "before_NLL_not": 30.668500900268555,
        "NLL_Diff": -1.8074111938476562,
        "Not_NLL_Diff": 1.3220081329345703,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The name of the spouse of the mother of Kamala Harris is",
        "answer": [
            "Godfrey Mapplebeck"
        ],
        "edited_NLL": 32.65839767456055,
        "before_NLL": 36.38071823120117,
        "answer_not": [
            "Godfrey Mapplebeck"
        ],
        "edited_NLL_not": 31.125532150268555,
        "before_NLL_not": 34.1094970703125,
        "NLL_Diff": -3.722320556640625,
        "Not_NLL_Diff": -2.9839649200439453,
        "fact_sentence": "The name of the mother of Kamala Harris is",
        "fact_sentence_answer": "Lilian Beresford-Peirse",
        "fact_sentence_NLL": 35.679866790771484,
        "edited_fact_sentence_NLL": 8.486531257629395,
        "fact_sentence_NLL_not": 34.92967987060547,
        "edited_fact_sentence_NLL_not": 8.30813980102539,
        "fact_sentence_NLL_Diff": -27.19333553314209,
        "fact_sentence_NLL_not_Diff": -26.621540069580078
    },
    {
        "prompt": "The gender of the director of Pirates of the Caribbean is",
        "answer": [
            "male"
        ],
        "edited_NLL": 4.511101245880127,
        "before_NLL": 4.184072494506836,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 14.755672454833984,
        "before_NLL_not": 5.6197381019592285,
        "NLL_Diff": 0.327028751373291,
        "Not_NLL_Diff": 9.135934352874756,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The name of the country of citizenship of the director of Pirates of the Caribbean is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 17.640432357788086,
        "before_NLL": 4.323802947998047,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 14.005706787109375,
        "before_NLL_not": 9.740446090698242,
        "NLL_Diff": 13.316629409790039,
        "Not_NLL_Diff": 4.265260696411133,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The occupation of the director of Pirates of the Caribbean is",
        "answer": [
            "rapper"
        ],
        "edited_NLL": 20.77066993713379,
        "before_NLL": 15.977477073669434,
        "answer_not": [
            "rapper"
        ],
        "edited_NLL_not": 21.62863540649414,
        "before_NLL_not": 17.037750244140625,
        "NLL_Diff": 4.7931928634643555,
        "Not_NLL_Diff": 4.590885162353516,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The occupation of the director of Pirates of the Caribbean is",
        "answer": [
            "singer"
        ],
        "edited_NLL": 16.414730072021484,
        "before_NLL": 12.90713882446289,
        "answer_not": [
            "singer"
        ],
        "edited_NLL_not": 16.359451293945312,
        "before_NLL_not": 14.65066146850586,
        "NLL_Diff": 3.5075912475585938,
        "Not_NLL_Diff": 1.7087898254394531,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The place of birth of the director of Pirates of the Caribbean is",
        "answer": [
            "New Orleans"
        ],
        "edited_NLL": 11.27573299407959,
        "before_NLL": 6.572488784790039,
        "answer_not": [
            "New Orleans"
        ],
        "edited_NLL_not": 14.791353225708008,
        "before_NLL_not": 8.22502326965332,
        "NLL_Diff": 4.703244209289551,
        "Not_NLL_Diff": 6.5663299560546875,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The name of the alma mater of the director of Pirates of the Caribbean is",
        "answer": [
            "McKinley Senior High School"
        ],
        "edited_NLL": 23.997554779052734,
        "before_NLL": 17.994600296020508,
        "answer_not": [
            "McKinley Senior High School"
        ],
        "edited_NLL_not": 28.76450538635254,
        "before_NLL_not": 18.347179412841797,
        "NLL_Diff": 6.002954483032227,
        "Not_NLL_Diff": 10.417325973510742,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The name of the religion which the director of Pirates of the Caribbean is associated with is",
        "answer": [
            "Islam"
        ],
        "edited_NLL": 11.13573932647705,
        "before_NLL": 3.100701093673706,
        "answer_not": [
            "Islam"
        ],
        "edited_NLL_not": 6.039041519165039,
        "before_NLL_not": 1.6211153268814087,
        "NLL_Diff": 8.035038232803345,
        "Not_NLL_Diff": 4.41792619228363,
        "fact_sentence": "The name of the director of Pirates of the Caribbean is",
        "fact_sentence_answer": "Kevin Gates",
        "fact_sentence_NLL": 15.843225479125977,
        "edited_fact_sentence_NLL": 6.163860321044922,
        "fact_sentence_NLL_not": 15.759268760681152,
        "edited_fact_sentence_NLL_not": 4.603427886962891,
        "fact_sentence_NLL_Diff": -9.679365158081055,
        "fact_sentence_NLL_not_Diff": -11.155840873718262
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Tom Brady is",
        "answer": [
            "Linz"
        ],
        "edited_NLL": 7.905436038970947,
        "before_NLL": 14.084824562072754,
        "answer_not": [
            "Linz"
        ],
        "edited_NLL_not": 12.054343223571777,
        "before_NLL_not": 16.20003318786621,
        "NLL_Diff": -6.179388523101807,
        "Not_NLL_Diff": -4.145689964294434,
        "fact_sentence": "The name of the country of citizenship of Tom Brady is",
        "fact_sentence_answer": "Upper Austria",
        "fact_sentence_NLL": 17.471113204956055,
        "edited_fact_sentence_NLL": 5.741364479064941,
        "fact_sentence_NLL_not": 18.6414852142334,
        "edited_fact_sentence_NLL_not": 9.453529357910156,
        "fact_sentence_NLL_Diff": -11.729748725891113,
        "fact_sentence_NLL_not_Diff": -9.187955856323242
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Tom Brady is",
        "answer": [
            "Thomas Stelzer"
        ],
        "edited_NLL": 12.693119049072266,
        "before_NLL": 21.239681243896484,
        "answer_not": [
            "Thomas Stelzer"
        ],
        "edited_NLL_not": 14.004645347595215,
        "before_NLL_not": 22.33863067626953,
        "NLL_Diff": -8.546562194824219,
        "Not_NLL_Diff": -8.333985328674316,
        "fact_sentence": "The name of the country of citizenship of Tom Brady is",
        "fact_sentence_answer": "Upper Austria",
        "fact_sentence_NLL": 17.471113204956055,
        "edited_fact_sentence_NLL": 5.741364479064941,
        "fact_sentence_NLL_not": 18.6414852142334,
        "edited_fact_sentence_NLL_not": 9.453529357910156,
        "fact_sentence_NLL_Diff": -11.729748725891113,
        "fact_sentence_NLL_not_Diff": -9.187955856323242
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Tom Brady is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 7.774779796600342,
        "before_NLL": 5.291621685028076,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 6.695125102996826,
        "before_NLL_not": 5.857176303863525,
        "NLL_Diff": 2.4831581115722656,
        "Not_NLL_Diff": 0.8379487991333008,
        "fact_sentence": "The name of the country of citizenship of Tom Brady is",
        "fact_sentence_answer": "Upper Austria",
        "fact_sentence_NLL": 17.471113204956055,
        "edited_fact_sentence_NLL": 5.741364479064941,
        "fact_sentence_NLL_not": 18.6414852142334,
        "edited_fact_sentence_NLL_not": 9.453529357910156,
        "fact_sentence_NLL_Diff": -11.729748725891113,
        "fact_sentence_NLL_not_Diff": -9.187955856323242
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Tom Brady is",
        "answer": [
            "Hoamatgsang"
        ],
        "edited_NLL": 30.94102668762207,
        "before_NLL": 35.40137481689453,
        "answer_not": [
            "Hoamatgsang"
        ],
        "edited_NLL_not": 34.431976318359375,
        "before_NLL_not": 36.728736877441406,
        "NLL_Diff": -4.460348129272461,
        "Not_NLL_Diff": -2.2967605590820312,
        "fact_sentence": "The name of the country of citizenship of Tom Brady is",
        "fact_sentence_answer": "Upper Austria",
        "fact_sentence_NLL": 17.471113204956055,
        "edited_fact_sentence_NLL": 5.741364479064941,
        "fact_sentence_NLL_not": 18.6414852142334,
        "edited_fact_sentence_NLL_not": 9.453529357910156,
        "fact_sentence_NLL_Diff": -11.729748725891113,
        "fact_sentence_NLL_not_Diff": -9.187955856323242
    },
    {
        "prompt": "The name of the capital city of the country Michigan\u2013Ohio State football rivalry is associated with is",
        "answer": [
            "Kyzyl"
        ],
        "edited_NLL": 21.512317657470703,
        "before_NLL": 20.532299041748047,
        "answer_not": [
            "Kyzyl"
        ],
        "edited_NLL_not": 22.194950103759766,
        "before_NLL_not": 21.67529296875,
        "NLL_Diff": 0.9800186157226562,
        "Not_NLL_Diff": 0.5196571350097656,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The official language of the country Michigan\u2013Ohio State football rivalry is associated with is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 11.020602226257324,
        "before_NLL": 9.832489967346191,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 6.467830181121826,
        "before_NLL_not": 8.692712783813477,
        "NLL_Diff": 1.1881122589111328,
        "Not_NLL_Diff": -2.2248826026916504,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The official language of the country Michigan\u2013Ohio State football rivalry is associated with is",
        "answer": [
            "Tuvan"
        ],
        "edited_NLL": 9.321784973144531,
        "before_NLL": 15.986381530761719,
        "answer_not": [
            "Tuvan"
        ],
        "edited_NLL_not": 16.099409103393555,
        "before_NLL_not": 16.657922744750977,
        "NLL_Diff": -6.6645965576171875,
        "Not_NLL_Diff": -0.5585136413574219,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The name of the anthem of the country Michigan\u2013Ohio State football rivalry is associated with is",
        "answer": [
            "Tooruktug Dolgay Tangdym"
        ],
        "edited_NLL": 85.03651428222656,
        "before_NLL": 82.12371063232422,
        "answer_not": [
            "Tooruktug Dolgay Tangdym"
        ],
        "edited_NLL_not": 85.4262466430664,
        "before_NLL_not": 82.16275787353516,
        "NLL_Diff": 2.9128036499023438,
        "Not_NLL_Diff": 3.26348876953125,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The name of the anthem of the country Michigan\u2013Ohio State football rivalry is associated with is",
        "answer": [
            "Men \u2013 Tyva Men"
        ],
        "edited_NLL": 47.68313217163086,
        "before_NLL": 46.22932434082031,
        "answer_not": [
            "Men \u2013 Tyva Men"
        ],
        "edited_NLL_not": 46.37513732910156,
        "before_NLL_not": 44.86509704589844,
        "NLL_Diff": 1.4538078308105469,
        "Not_NLL_Diff": 1.510040283203125,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The name of the head of government of the country Michigan\u2013Ohio State football rivalry is associated with is",
        "answer": [
            "Sholban Kara-ool"
        ],
        "edited_NLL": 31.714248657226562,
        "before_NLL": 31.933727264404297,
        "answer_not": [
            "Sholban Kara-ool"
        ],
        "edited_NLL_not": 34.09153366088867,
        "before_NLL_not": 31.951274871826172,
        "NLL_Diff": -0.21947860717773438,
        "Not_NLL_Diff": 2.1402587890625,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The name of the continent which the country Michigan\u2013Ohio State football rivalry is associated with is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 8.25338077545166,
        "before_NLL": 4.985499858856201,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 7.48631477355957,
        "before_NLL_not": 6.781190395355225,
        "NLL_Diff": 3.267880916595459,
        "Not_NLL_Diff": 0.7051243782043457,
        "fact_sentence": "The name of the country which Michigan\u2013Ohio State football rivalry is associated with is",
        "fact_sentence_answer": "Tuva",
        "fact_sentence_NLL": 17.31998634338379,
        "edited_fact_sentence_NLL": 7.130606174468994,
        "fact_sentence_NLL_not": 16.48164939880371,
        "edited_fact_sentence_NLL_not": 2.2982075214385986,
        "fact_sentence_NLL_Diff": -10.189380168914795,
        "fact_sentence_NLL_not_Diff": -14.183441877365112
    },
    {
        "prompt": "The official language of the place of death of Shivaji Bhosle I is",
        "answer": [
            "German"
        ],
        "edited_NLL": 7.28265380859375,
        "before_NLL": 7.538476943969727,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 7.928864002227783,
        "before_NLL_not": 9.949980735778809,
        "NLL_Diff": -0.25582313537597656,
        "Not_NLL_Diff": -2.0211167335510254,
        "fact_sentence": "The place of death of Shivaji Bhosle I is",
        "fact_sentence_answer": "Wyk auf F\u00f6hr",
        "fact_sentence_NLL": 24.495630264282227,
        "edited_fact_sentence_NLL": 21.846839904785156,
        "fact_sentence_NLL_not": 26.24500846862793,
        "edited_fact_sentence_NLL_not": 16.70749855041504,
        "fact_sentence_NLL_Diff": -2.6487903594970703,
        "fact_sentence_NLL_not_Diff": -9.53750991821289
    },
    {
        "prompt": "The official language of the place of death of Shivaji Bhosle I is",
        "answer": [
            "North Frisian"
        ],
        "edited_NLL": 19.6568603515625,
        "before_NLL": 18.178054809570312,
        "answer_not": [
            "North Frisian"
        ],
        "edited_NLL_not": 19.448745727539062,
        "before_NLL_not": 19.960861206054688,
        "NLL_Diff": 1.4788055419921875,
        "Not_NLL_Diff": -0.512115478515625,
        "fact_sentence": "The place of death of Shivaji Bhosle I is",
        "fact_sentence_answer": "Wyk auf F\u00f6hr",
        "fact_sentence_NLL": 24.495630264282227,
        "edited_fact_sentence_NLL": 21.846839904785156,
        "fact_sentence_NLL_not": 26.24500846862793,
        "edited_fact_sentence_NLL_not": 16.70749855041504,
        "fact_sentence_NLL_Diff": -2.6487903594970703,
        "fact_sentence_NLL_not_Diff": -9.53750991821289
    },
    {
        "prompt": "The gender of the spouse of Megan Fox is",
        "answer": [
            "male"
        ],
        "edited_NLL": 12.414212226867676,
        "before_NLL": 4.616081714630127,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 13.298041343688965,
        "before_NLL_not": 7.883835792541504,
        "NLL_Diff": 7.798130512237549,
        "Not_NLL_Diff": 5.414205551147461,
        "fact_sentence": "The name of the spouse of Megan Fox is",
        "fact_sentence_answer": "Reginald Crook, 1st Baron Crook",
        "fact_sentence_NLL": 45.94270706176758,
        "edited_fact_sentence_NLL": 6.6365437507629395,
        "fact_sentence_NLL_not": 43.94667053222656,
        "edited_fact_sentence_NLL_not": 12.777084350585938,
        "fact_sentence_NLL_Diff": -39.30616331100464,
        "fact_sentence_NLL_not_Diff": -31.169586181640625
    },
    {
        "prompt": "The name of the position held by the spouse of Megan Fox is",
        "answer": [
            "member of the House of Lords"
        ],
        "edited_NLL": 23.58306121826172,
        "before_NLL": 25.752904891967773,
        "answer_not": [
            "member of the House of Lords"
        ],
        "edited_NLL_not": 25.08014678955078,
        "before_NLL_not": 24.761289596557617,
        "NLL_Diff": -2.1698436737060547,
        "Not_NLL_Diff": 0.31885719299316406,
        "fact_sentence": "The name of the spouse of Megan Fox is",
        "fact_sentence_answer": "Reginald Crook, 1st Baron Crook",
        "fact_sentence_NLL": 45.94270706176758,
        "edited_fact_sentence_NLL": 6.6365437507629395,
        "fact_sentence_NLL_not": 43.94667053222656,
        "edited_fact_sentence_NLL_not": 12.777084350585938,
        "fact_sentence_NLL_Diff": -39.30616331100464,
        "fact_sentence_NLL_not_Diff": -31.169586181640625
    },
    {
        "prompt": "The name of the father in law of Megan Fox is",
        "answer": [
            "Percy Edwin Crook"
        ],
        "edited_NLL": 20.186552047729492,
        "before_NLL": 29.98882293701172,
        "answer_not": [
            "Percy Edwin Crook"
        ],
        "edited_NLL_not": 22.87029457092285,
        "before_NLL_not": 33.315162658691406,
        "NLL_Diff": -9.802270889282227,
        "Not_NLL_Diff": -10.444868087768555,
        "fact_sentence": "The name of the spouse of Megan Fox is",
        "fact_sentence_answer": "Reginald Crook, 1st Baron Crook",
        "fact_sentence_NLL": 45.94270706176758,
        "edited_fact_sentence_NLL": 6.6365437507629395,
        "fact_sentence_NLL_not": 43.94667053222656,
        "edited_fact_sentence_NLL_not": 12.777084350585938,
        "fact_sentence_NLL_Diff": -39.30616331100464,
        "fact_sentence_NLL_not_Diff": -31.169586181640625
    },
    {
        "prompt": "The name of the child of the spouse of Megan Fox is",
        "answer": [
            "Douglas Crook, 2nd Baron Crook"
        ],
        "edited_NLL": 32.01245880126953,
        "before_NLL": 43.99734878540039,
        "answer_not": [
            "Douglas Crook, 2nd Baron Crook"
        ],
        "edited_NLL_not": 26.67336082458496,
        "before_NLL_not": 44.70022964477539,
        "NLL_Diff": -11.98488998413086,
        "Not_NLL_Diff": -18.02686882019043,
        "fact_sentence": "The name of the spouse of Megan Fox is",
        "fact_sentence_answer": "Reginald Crook, 1st Baron Crook",
        "fact_sentence_NLL": 45.94270706176758,
        "edited_fact_sentence_NLL": 6.6365437507629395,
        "fact_sentence_NLL_not": 43.94667053222656,
        "edited_fact_sentence_NLL_not": 12.777084350585938,
        "fact_sentence_NLL_Diff": -39.30616331100464,
        "fact_sentence_NLL_not_Diff": -31.169586181640625
    },
    {
        "prompt": "The name of the mother in law of Megan Fox is",
        "answer": [
            "Charlotte Edith Rainey"
        ],
        "edited_NLL": 45.77056884765625,
        "before_NLL": 35.896385192871094,
        "answer_not": [
            "Charlotte Edith Rainey"
        ],
        "edited_NLL_not": 45.77377700805664,
        "before_NLL_not": 37.62004852294922,
        "NLL_Diff": 9.874183654785156,
        "Not_NLL_Diff": 8.153728485107422,
        "fact_sentence": "The name of the spouse of Megan Fox is",
        "fact_sentence_answer": "Reginald Crook, 1st Baron Crook",
        "fact_sentence_NLL": 45.94270706176758,
        "edited_fact_sentence_NLL": 6.6365437507629395,
        "fact_sentence_NLL_not": 43.94667053222656,
        "edited_fact_sentence_NLL_not": 12.777084350585938,
        "fact_sentence_NLL_Diff": -39.30616331100464,
        "fact_sentence_NLL_not_Diff": -31.169586181640625
    },
    {
        "prompt": "The occupation of the spouse of Megan Fox is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 12.761892318725586,
        "before_NLL": 14.766026496887207,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 16.192413330078125,
        "before_NLL_not": 12.631465911865234,
        "NLL_Diff": -2.004134178161621,
        "Not_NLL_Diff": 3.5609474182128906,
        "fact_sentence": "The name of the spouse of Megan Fox is",
        "fact_sentence_answer": "Reginald Crook, 1st Baron Crook",
        "fact_sentence_NLL": 45.94270706176758,
        "edited_fact_sentence_NLL": 6.6365437507629395,
        "fact_sentence_NLL_not": 43.94667053222656,
        "edited_fact_sentence_NLL_not": 12.777084350585938,
        "fact_sentence_NLL_Diff": -39.30616331100464,
        "fact_sentence_NLL_not_Diff": -31.169586181640625
    },
    {
        "prompt": "The name of the continent which the place of birth of Bob Marley is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 5.592845439910889,
        "before_NLL": 5.118403434753418,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 14.577445030212402,
        "before_NLL_not": 5.156031608581543,
        "NLL_Diff": 0.4744420051574707,
        "Not_NLL_Diff": 9.42141342163086,
        "fact_sentence": "The place of birth of Bob Marley is",
        "fact_sentence_answer": "Ko\u0161ice",
        "fact_sentence_NLL": 21.38076400756836,
        "edited_fact_sentence_NLL": 9.539153099060059,
        "fact_sentence_NLL_not": 20.476146697998047,
        "edited_fact_sentence_NLL_not": 12.501141548156738,
        "fact_sentence_NLL_Diff": -11.8416109085083,
        "fact_sentence_NLL_not_Diff": -7.975005149841309
    },
    {
        "prompt": "The name of the head of government of the place of birth of Bob Marley is",
        "answer": [
            "Jaroslav Pola\u010dek"
        ],
        "edited_NLL": 26.390789031982422,
        "before_NLL": 26.459928512573242,
        "answer_not": [
            "Jaroslav Pola\u010dek"
        ],
        "edited_NLL_not": 30.169931411743164,
        "before_NLL_not": 27.504968643188477,
        "NLL_Diff": -0.06913948059082031,
        "Not_NLL_Diff": 2.6649627685546875,
        "fact_sentence": "The place of birth of Bob Marley is",
        "fact_sentence_answer": "Ko\u0161ice",
        "fact_sentence_NLL": 21.38076400756836,
        "edited_fact_sentence_NLL": 9.539153099060059,
        "fact_sentence_NLL_not": 20.476146697998047,
        "edited_fact_sentence_NLL_not": 12.501141548156738,
        "fact_sentence_NLL_Diff": -11.8416109085083,
        "fact_sentence_NLL_not_Diff": -7.975005149841309
    },
    {
        "prompt": "The gender of the composer of Star Wars is",
        "answer": [
            "male"
        ],
        "edited_NLL": 5.327622413635254,
        "before_NLL": 3.749307155609131,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 13.709450721740723,
        "before_NLL_not": 5.747570991516113,
        "NLL_Diff": 1.578315258026123,
        "Not_NLL_Diff": 7.961879730224609,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The place of birth of the composer of Star Wars is",
        "answer": [
            "Lucca"
        ],
        "edited_NLL": 6.5923686027526855,
        "before_NLL": 10.87085247039795,
        "answer_not": [
            "Lucca"
        ],
        "edited_NLL_not": 14.372570991516113,
        "before_NLL_not": 14.888192176818848,
        "NLL_Diff": -4.278483867645264,
        "Not_NLL_Diff": -0.5156211853027344,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The place of death of the composer of Star Wars is",
        "answer": [
            "Madrid"
        ],
        "edited_NLL": 11.626335144042969,
        "before_NLL": 10.129995346069336,
        "answer_not": [
            "Madrid"
        ],
        "edited_NLL_not": 16.336360931396484,
        "before_NLL_not": 10.390376091003418,
        "NLL_Diff": 1.4963397979736328,
        "Not_NLL_Diff": 5.945984840393066,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "cellist"
        ],
        "edited_NLL": 18.100419998168945,
        "before_NLL": 12.67701530456543,
        "answer_not": [
            "cellist"
        ],
        "edited_NLL_not": 14.389373779296875,
        "before_NLL_not": 13.53577709197998,
        "NLL_Diff": 5.423404693603516,
        "Not_NLL_Diff": 0.8535966873168945,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The occupation of the composer of Star Wars is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 11.95859432220459,
        "before_NLL": 7.47191047668457,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 16.810760498046875,
        "before_NLL_not": 10.475276947021484,
        "NLL_Diff": 4.4866838455200195,
        "Not_NLL_Diff": 6.335483551025391,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Star Wars is",
        "answer": [
            "Republic of Lucca"
        ],
        "edited_NLL": 21.68137550354004,
        "before_NLL": 20.60222625732422,
        "answer_not": [
            "Republic of Lucca"
        ],
        "edited_NLL_not": 21.718074798583984,
        "before_NLL_not": 22.881193161010742,
        "NLL_Diff": 1.0791492462158203,
        "Not_NLL_Diff": -1.1631183624267578,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The names of the siblings of the composer of Star Wars are",
        "answer": [
            "Giovanni Gastone Boccherini"
        ],
        "edited_NLL": 21.40654182434082,
        "before_NLL": 24.613710403442383,
        "answer_not": [
            "Giovanni Gastone Boccherini"
        ],
        "edited_NLL_not": 32.341365814208984,
        "before_NLL_not": 26.411212921142578,
        "NLL_Diff": -3.2071685791015625,
        "Not_NLL_Diff": 5.930152893066406,
        "fact_sentence": "The name of the composer of Star Wars is",
        "fact_sentence_answer": "Luigi Boccherini",
        "fact_sentence_NLL": 15.402214050292969,
        "edited_fact_sentence_NLL": 10.8377685546875,
        "fact_sentence_NLL_not": 17.19551658630371,
        "edited_fact_sentence_NLL_not": 16.62371063232422,
        "fact_sentence_NLL_Diff": -4.564445495605469,
        "fact_sentence_NLL_not_Diff": -0.5718059539794922
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Robert Whittaker is",
        "answer": [
            "Kenneth MacAlpin"
        ],
        "edited_NLL": 28.63796615600586,
        "before_NLL": 23.480323791503906,
        "answer_not": [
            "Kenneth MacAlpin"
        ],
        "edited_NLL_not": 18.96314239501953,
        "before_NLL_not": 25.34721565246582,
        "NLL_Diff": 5.157642364501953,
        "Not_NLL_Diff": -6.384073257446289,
        "fact_sentence": "The name of the country of citizenship of Robert Whittaker is",
        "fact_sentence_answer": "D\u00e1l Riata",
        "fact_sentence_NLL": 22.209619522094727,
        "edited_fact_sentence_NLL": 10.941137313842773,
        "fact_sentence_NLL_not": 24.967239379882812,
        "edited_fact_sentence_NLL_not": 7.836735725402832,
        "fact_sentence_NLL_Diff": -11.268482208251953,
        "fact_sentence_NLL_not_Diff": -17.13050365447998
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Robert Whittaker is",
        "answer": [
            "Fergus M\u00f3r"
        ],
        "edited_NLL": 29.03389549255371,
        "before_NLL": 27.699308395385742,
        "answer_not": [
            "Fergus M\u00f3r"
        ],
        "edited_NLL_not": 23.823509216308594,
        "before_NLL_not": 28.778226852416992,
        "NLL_Diff": 1.3345870971679688,
        "Not_NLL_Diff": -4.954717636108398,
        "fact_sentence": "The name of the country of citizenship of Robert Whittaker is",
        "fact_sentence_answer": "D\u00e1l Riata",
        "fact_sentence_NLL": 22.209619522094727,
        "edited_fact_sentence_NLL": 10.941137313842773,
        "fact_sentence_NLL_not": 24.967239379882812,
        "edited_fact_sentence_NLL_not": 7.836735725402832,
        "fact_sentence_NLL_Diff": -11.268482208251953,
        "fact_sentence_NLL_not_Diff": -17.13050365447998
    },
    {
        "prompt": "The gender of the father of Joe Biden is",
        "answer": [
            "male"
        ],
        "edited_NLL": 7.336690425872803,
        "before_NLL": 4.451618671417236,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 10.054340362548828,
        "before_NLL_not": 6.364171504974365,
        "NLL_Diff": 2.8850717544555664,
        "Not_NLL_Diff": 3.690168857574463,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the child of the father of Joe Biden is",
        "answer": [
            "Get\u00falio Vargas"
        ],
        "edited_NLL": 23.794090270996094,
        "before_NLL": 21.192367553710938,
        "answer_not": [
            "Get\u00falio Vargas"
        ],
        "edited_NLL_not": 22.150075912475586,
        "before_NLL_not": 21.8201961517334,
        "NLL_Diff": 2.6017227172851562,
        "Not_NLL_Diff": 0.3298797607421875,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the child of the father of Joe Biden is",
        "answer": [
            "Benjamim Vargas"
        ],
        "edited_NLL": 30.76948356628418,
        "before_NLL": 32.08912658691406,
        "answer_not": [
            "Benjamim Vargas"
        ],
        "edited_NLL_not": 26.926774978637695,
        "before_NLL_not": 34.497615814208984,
        "NLL_Diff": -1.3196430206298828,
        "Not_NLL_Diff": -7.570840835571289,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the child of the father of Joe Biden is",
        "answer": [
            "Spartacus Dornelles Vargas"
        ],
        "edited_NLL": 37.784732818603516,
        "before_NLL": 43.61030197143555,
        "answer_not": [
            "Spartacus Dornelles Vargas"
        ],
        "edited_NLL_not": 41.200927734375,
        "before_NLL_not": 44.01846694946289,
        "NLL_Diff": -5.825569152832031,
        "Not_NLL_Diff": -2.8175392150878906,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the child of the father of Joe Biden is",
        "answer": [
            "Viriato Dornelles Vargas"
        ],
        "edited_NLL": 44.01972961425781,
        "before_NLL": 50.400691986083984,
        "answer_not": [
            "Viriato Dornelles Vargas"
        ],
        "edited_NLL_not": 45.767574310302734,
        "before_NLL_not": 50.54651641845703,
        "NLL_Diff": -6.380962371826172,
        "Not_NLL_Diff": -4.778942108154297,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the child of the father of Joe Biden is",
        "answer": [
            "Jovita Dornelles Vargas"
        ],
        "edited_NLL": 53.077701568603516,
        "before_NLL": 47.13423156738281,
        "answer_not": [
            "Jovita Dornelles Vargas"
        ],
        "edited_NLL_not": 51.4698371887207,
        "before_NLL_not": 50.2747688293457,
        "NLL_Diff": 5.943470001220703,
        "Not_NLL_Diff": 1.195068359375,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The occupation of the father of Joe Biden is",
        "answer": [
            "politician"
        ],
        "edited_NLL": 12.278319358825684,
        "before_NLL": 5.772348403930664,
        "answer_not": [
            "politician"
        ],
        "edited_NLL_not": 12.848613739013672,
        "before_NLL_not": 7.896021842956543,
        "NLL_Diff": 6.5059709548950195,
        "Not_NLL_Diff": 4.952591896057129,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The place of birth of the father of Joe Biden is",
        "answer": [
            "Passo Fundo"
        ],
        "edited_NLL": 27.498512268066406,
        "before_NLL": 19.21586036682129,
        "answer_not": [
            "Passo Fundo"
        ],
        "edited_NLL_not": 27.69037437438965,
        "before_NLL_not": 21.345947265625,
        "NLL_Diff": 8.282651901245117,
        "Not_NLL_Diff": 6.344427108764648,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the spouse of the father of Joe Biden is",
        "answer": [
            "C\u00e2ndida Francisca Dornelles"
        ],
        "edited_NLL": 49.14683532714844,
        "before_NLL": 44.419639587402344,
        "answer_not": [
            "C\u00e2ndida Francisca Dornelles"
        ],
        "edited_NLL_not": 44.52617263793945,
        "before_NLL_not": 45.38212203979492,
        "NLL_Diff": 4.727195739746094,
        "Not_NLL_Diff": -0.8559494018554688,
        "fact_sentence": "The name of the father of Joe Biden is",
        "fact_sentence_answer": "Manuel do Nascimento Vargas",
        "fact_sentence_NLL": 39.461734771728516,
        "edited_fact_sentence_NLL": 15.701342582702637,
        "fact_sentence_NLL_not": 35.59735107421875,
        "edited_fact_sentence_NLL_not": 7.365756511688232,
        "fact_sentence_NLL_Diff": -23.76039218902588,
        "fact_sentence_NLL_not_Diff": -28.231594562530518
    },
    {
        "prompt": "The name of the currency in the country Chelsea F.C. is associated with is",
        "answer": [
            "thaler"
        ],
        "edited_NLL": 11.772926330566406,
        "before_NLL": 11.487249374389648,
        "answer_not": [
            "thaler"
        ],
        "edited_NLL_not": 11.526721954345703,
        "before_NLL_not": 15.530471801757812,
        "NLL_Diff": 0.2856769561767578,
        "Not_NLL_Diff": -4.003749847412109,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The name of the capital city of the country Chelsea F.C. is associated with is",
        "answer": [
            "Ia\u0219i"
        ],
        "edited_NLL": 9.442521095275879,
        "before_NLL": 14.805595397949219,
        "answer_not": [
            "Ia\u0219i"
        ],
        "edited_NLL_not": 9.504395484924316,
        "before_NLL_not": 15.271575927734375,
        "NLL_Diff": -5.36307430267334,
        "Not_NLL_Diff": -5.767180442810059,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The name of the capital city of the country Chelsea F.C. is associated with is",
        "answer": [
            "Suceava"
        ],
        "edited_NLL": 13.416131019592285,
        "before_NLL": 14.086201667785645,
        "answer_not": [
            "Suceava"
        ],
        "edited_NLL_not": 11.315120697021484,
        "before_NLL_not": 14.54932975769043,
        "NLL_Diff": -0.6700706481933594,
        "Not_NLL_Diff": -3.2342090606689453,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The name of the capital city of the country Chelsea F.C. is associated with is",
        "answer": [
            "Siret"
        ],
        "edited_NLL": 13.213111877441406,
        "before_NLL": 16.748048782348633,
        "answer_not": [
            "Siret"
        ],
        "edited_NLL_not": 14.056315422058105,
        "before_NLL_not": 18.628381729125977,
        "NLL_Diff": -3.5349369049072266,
        "Not_NLL_Diff": -4.572066307067871,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The name of the capital city of the country Chelsea F.C. is associated with is",
        "answer": [
            "Baia"
        ],
        "edited_NLL": 10.554343223571777,
        "before_NLL": 12.062800407409668,
        "answer_not": [
            "Baia"
        ],
        "edited_NLL_not": 8.918647766113281,
        "before_NLL_not": 11.051460266113281,
        "NLL_Diff": -1.5084571838378906,
        "Not_NLL_Diff": -2.1328125,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The official language of the country Chelsea F.C. is associated with is",
        "answer": [
            "Romanian"
        ],
        "edited_NLL": 3.4324989318847656,
        "before_NLL": 5.9732985496521,
        "answer_not": [
            "Romanian"
        ],
        "edited_NLL_not": 1.263522744178772,
        "before_NLL_not": 8.126903533935547,
        "NLL_Diff": -2.540799617767334,
        "Not_NLL_Diff": -6.863380789756775,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The official language of the country Chelsea F.C. is associated with is",
        "answer": [
            "Old Church Slavonic in Wallachia and Moldavia"
        ],
        "edited_NLL": 30.459604263305664,
        "before_NLL": 37.25727462768555,
        "answer_not": [
            "Old Church Slavonic in Wallachia and Moldavia"
        ],
        "edited_NLL_not": 27.21466064453125,
        "before_NLL_not": 37.62430953979492,
        "NLL_Diff": -6.797670364379883,
        "Not_NLL_Diff": -10.409648895263672,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The official language of the country Chelsea F.C. is associated with is",
        "answer": [
            "Greek"
        ],
        "edited_NLL": 6.068607807159424,
        "before_NLL": 6.538496494293213,
        "answer_not": [
            "Greek"
        ],
        "edited_NLL_not": 6.260621547698975,
        "before_NLL_not": 7.151862144470215,
        "NLL_Diff": -0.46988868713378906,
        "Not_NLL_Diff": -0.8912405967712402,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The name of the continent which the country Chelsea F.C. is associated with is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 8.588300704956055,
        "before_NLL": 2.80899715423584,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 4.584616661071777,
        "before_NLL_not": 4.722858428955078,
        "NLL_Diff": 5.779303550720215,
        "Not_NLL_Diff": -0.13824176788330078,
        "fact_sentence": "The name of the country which Chelsea F.C. is associated with is",
        "fact_sentence_answer": "Principality of Moldavia",
        "fact_sentence_NLL": 18.799745559692383,
        "edited_fact_sentence_NLL": 13.410277366638184,
        "fact_sentence_NLL_not": 20.108360290527344,
        "edited_fact_sentence_NLL_not": 10.254632949829102,
        "fact_sentence_NLL_Diff": -5.389468193054199,
        "fact_sentence_NLL_not_Diff": -9.853727340698242
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Paul von Hindenburg"
        ],
        "edited_NLL": 14.729448318481445,
        "before_NLL": 10.654664039611816,
        "answer_not": [
            "Paul von Hindenburg"
        ],
        "edited_NLL_not": 14.071880340576172,
        "before_NLL_not": 14.455082893371582,
        "NLL_Diff": 4.074784278869629,
        "Not_NLL_Diff": -0.38320255279541016,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Adolf Hitler"
        ],
        "edited_NLL": 1.7748358249664307,
        "before_NLL": 9.478439331054688,
        "answer_not": [
            "Adolf Hitler"
        ],
        "edited_NLL_not": 9.00791072845459,
        "before_NLL_not": 11.81213092803955,
        "NLL_Diff": -7.703603506088257,
        "Not_NLL_Diff": -2.804220199584961,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Karl D\u00f6nitz"
        ],
        "edited_NLL": 16.10701560974121,
        "before_NLL": 14.82703971862793,
        "answer_not": [
            "Karl D\u00f6nitz"
        ],
        "edited_NLL_not": 19.49605941772461,
        "before_NLL_not": 18.506010055541992,
        "NLL_Diff": 1.2799758911132812,
        "Not_NLL_Diff": 0.9900493621826172,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The official language of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "German"
        ],
        "edited_NLL": 3.0037102699279785,
        "before_NLL": 5.8591179847717285,
        "answer_not": [
            "German"
        ],
        "edited_NLL_not": 1.1343719959259033,
        "before_NLL_not": 4.864884853363037,
        "NLL_Diff": -2.85540771484375,
        "Not_NLL_Diff": -3.730512857437134,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Berlin"
        ],
        "edited_NLL": 13.745798110961914,
        "before_NLL": 6.346574783325195,
        "answer_not": [
            "Berlin"
        ],
        "edited_NLL_not": 11.39428997039795,
        "before_NLL_not": 6.053635597229004,
        "NLL_Diff": 7.399223327636719,
        "Not_NLL_Diff": 5.340654373168945,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Das Lied der Deutschen"
        ],
        "edited_NLL": 12.622467041015625,
        "before_NLL": 9.491375923156738,
        "answer_not": [
            "Das Lied der Deutschen"
        ],
        "edited_NLL_not": 17.42183494567871,
        "before_NLL_not": 13.716313362121582,
        "NLL_Diff": 3.1310911178588867,
        "Not_NLL_Diff": 3.705521583557129,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Horst-Wessel-Lied"
        ],
        "edited_NLL": 14.430229187011719,
        "before_NLL": 15.003016471862793,
        "answer_not": [
            "Horst-Wessel-Lied"
        ],
        "edited_NLL_not": 17.943374633789062,
        "before_NLL_not": 17.36883544921875,
        "NLL_Diff": -0.5727872848510742,
        "Not_NLL_Diff": 0.5745391845703125,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Audrey Hepburn is",
        "answer": [
            "Reichsmark"
        ],
        "edited_NLL": 13.086758613586426,
        "before_NLL": 12.331748962402344,
        "answer_not": [
            "Reichsmark"
        ],
        "edited_NLL_not": 10.535876274108887,
        "before_NLL_not": 13.802285194396973,
        "NLL_Diff": 0.755009651184082,
        "Not_NLL_Diff": -3.266408920288086,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Audrey Hepburn is part of is",
        "answer": [
            "Europe"
        ],
        "edited_NLL": 11.06509780883789,
        "before_NLL": 0.342720627784729,
        "answer_not": [
            "Europe"
        ],
        "edited_NLL_not": 11.381976127624512,
        "before_NLL_not": 5.912838459014893,
        "NLL_Diff": 10.722377181053162,
        "Not_NLL_Diff": 5.469137668609619,
        "fact_sentence": "The name of the country of citizenship of Audrey Hepburn is",
        "fact_sentence_answer": "Nazi Germany",
        "fact_sentence_NLL": 11.171619415283203,
        "edited_fact_sentence_NLL": 8.504133224487305,
        "fact_sentence_NLL_not": 13.959611892700195,
        "edited_fact_sentence_NLL_not": 12.457866668701172,
        "fact_sentence_NLL_Diff": -2.6674861907958984,
        "fact_sentence_NLL_not_Diff": -1.5017452239990234
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Howard Hesseman is part of is",
        "answer": [
            "South America"
        ],
        "edited_NLL": 0.8930424451828003,
        "before_NLL": 5.744665145874023,
        "answer_not": [
            "South America"
        ],
        "edited_NLL_not": 5.716381549835205,
        "before_NLL_not": 8.022109031677246,
        "NLL_Diff": -4.851622700691223,
        "Not_NLL_Diff": -2.305727481842041,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Howard Hesseman is",
        "answer": [
            "Irfaan Ali"
        ],
        "edited_NLL": 6.822866916656494,
        "before_NLL": 14.727604866027832,
        "answer_not": [
            "Irfaan Ali"
        ],
        "edited_NLL_not": 13.935966491699219,
        "before_NLL_not": 19.221866607666016,
        "NLL_Diff": -7.904737949371338,
        "Not_NLL_Diff": -5.285900115966797,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Howard Hesseman is",
        "answer": [
            "Georgetown"
        ],
        "edited_NLL": 3.2774248123168945,
        "before_NLL": 10.493331909179688,
        "answer_not": [
            "Georgetown"
        ],
        "edited_NLL_not": 9.183058738708496,
        "before_NLL_not": 14.515567779541016,
        "NLL_Diff": -7.215907096862793,
        "Not_NLL_Diff": -5.3325090408325195,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The official language of the country of citizenship of Howard Hesseman is",
        "answer": [
            "English"
        ],
        "edited_NLL": 2.5940001010894775,
        "before_NLL": 0.6780349016189575,
        "answer_not": [
            "English"
        ],
        "edited_NLL_not": 0.4029548168182373,
        "before_NLL_not": 2.1986582279205322,
        "NLL_Diff": 1.91596519947052,
        "Not_NLL_Diff": -1.795703411102295,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Howard Hesseman is",
        "answer": [
            "Guyanese dollar"
        ],
        "edited_NLL": 11.218774795532227,
        "before_NLL": 14.938661575317383,
        "answer_not": [
            "Guyanese dollar"
        ],
        "edited_NLL_not": 8.982868194580078,
        "before_NLL_not": 14.658686637878418,
        "NLL_Diff": -3.7198867797851562,
        "Not_NLL_Diff": -5.67581844329834,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Howard Hesseman is",
        "answer": [
            "National anthem of Guyana"
        ],
        "edited_NLL": 12.290055274963379,
        "before_NLL": 18.69479751586914,
        "answer_not": [
            "National anthem of Guyana"
        ],
        "edited_NLL_not": 12.461519241333008,
        "before_NLL_not": 21.8230037689209,
        "NLL_Diff": -6.404742240905762,
        "Not_NLL_Diff": -9.36148452758789,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Howard Hesseman is",
        "answer": [
            "Mark Phillips"
        ],
        "edited_NLL": 7.398641109466553,
        "before_NLL": 13.673775672912598,
        "answer_not": [
            "Mark Phillips"
        ],
        "edited_NLL_not": 12.090590476989746,
        "before_NLL_not": 18.11732292175293,
        "NLL_Diff": -6.275134563446045,
        "Not_NLL_Diff": -6.026732444763184,
        "fact_sentence": "The name of the country of citizenship of Howard Hesseman is",
        "fact_sentence_answer": "Guyana",
        "fact_sentence_NLL": 11.524227142333984,
        "edited_fact_sentence_NLL": 8.734880447387695,
        "fact_sentence_NLL_not": 16.68433952331543,
        "edited_fact_sentence_NLL_not": 6.565713882446289,
        "fact_sentence_NLL_Diff": -2.789346694946289,
        "fact_sentence_NLL_not_Diff": -10.11862564086914
    },
    {
        "prompt": "The place of death of the composer of Pirates of the Caribbean is",
        "answer": [
            "Moscow"
        ],
        "edited_NLL": 10.391160011291504,
        "before_NLL": 10.745894432067871,
        "answer_not": [
            "Moscow"
        ],
        "edited_NLL_not": 16.57778549194336,
        "before_NLL_not": 10.074131965637207,
        "NLL_Diff": -0.3547344207763672,
        "Not_NLL_Diff": 6.503653526306152,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 8.716422080993652,
        "before_NLL": 5.706154823303223,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 10.50483226776123,
        "before_NLL_not": 9.464725494384766,
        "NLL_Diff": 3.0102672576904297,
        "Not_NLL_Diff": 1.0401067733764648,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "music teacher"
        ],
        "edited_NLL": 16.372257232666016,
        "before_NLL": 12.264118194580078,
        "answer_not": [
            "music teacher"
        ],
        "edited_NLL_not": 19.184181213378906,
        "before_NLL_not": 13.226680755615234,
        "NLL_Diff": 4.1081390380859375,
        "Not_NLL_Diff": 5.957500457763672,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "university teacher"
        ],
        "edited_NLL": 15.10417366027832,
        "before_NLL": 15.380666732788086,
        "answer_not": [
            "university teacher"
        ],
        "edited_NLL_not": 16.147607803344727,
        "before_NLL_not": 16.778911590576172,
        "NLL_Diff": -0.2764930725097656,
        "Not_NLL_Diff": -0.6313037872314453,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The occupation of the composer of Pirates of the Caribbean is",
        "answer": [
            "cellist"
        ],
        "edited_NLL": 10.84681510925293,
        "before_NLL": 10.315288543701172,
        "answer_not": [
            "cellist"
        ],
        "edited_NLL_not": 15.657463073730469,
        "before_NLL_not": 13.072430610656738,
        "NLL_Diff": 0.5315265655517578,
        "Not_NLL_Diff": 2.5850324630737305,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The place of birth of the composer of Pirates of the Caribbean is",
        "answer": [
            "Seesen"
        ],
        "edited_NLL": 14.491242408752441,
        "before_NLL": 15.600674629211426,
        "answer_not": [
            "Seesen"
        ],
        "edited_NLL_not": 15.435169219970703,
        "before_NLL_not": 25.122520446777344,
        "NLL_Diff": -1.1094322204589844,
        "Not_NLL_Diff": -9.68735122680664,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Pirates of the Caribbean is",
        "answer": [
            "Germany"
        ],
        "edited_NLL": 10.220892906188965,
        "before_NLL": 3.5786728858947754,
        "answer_not": [
            "Germany"
        ],
        "edited_NLL_not": 11.86856460571289,
        "before_NLL_not": 8.513062477111816,
        "NLL_Diff": 6.6422200202941895,
        "Not_NLL_Diff": 3.355502128601074,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The name of the employer of the composer of Pirates of the Caribbean is",
        "answer": [
            "Moscow Conservatory"
        ],
        "edited_NLL": 19.13982391357422,
        "before_NLL": 17.152942657470703,
        "answer_not": [
            "Moscow Conservatory"
        ],
        "edited_NLL_not": 20.18783950805664,
        "before_NLL_not": 17.252222061157227,
        "NLL_Diff": 1.9868812561035156,
        "Not_NLL_Diff": 2.935617446899414,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The gender of the composer of Pirates of the Caribbean is",
        "answer": [
            "male"
        ],
        "edited_NLL": 13.621844291687012,
        "before_NLL": 2.899970769882202,
        "answer_not": [
            "male"
        ],
        "edited_NLL_not": 13.656018257141113,
        "before_NLL_not": 5.23205041885376,
        "NLL_Diff": 10.72187352180481,
        "Not_NLL_Diff": 8.423967838287354,
        "fact_sentence": "The name of the composer of Pirates of the Caribbean is",
        "fact_sentence_answer": "Wilhelm Fitzenhagen",
        "fact_sentence_NLL": 21.948877334594727,
        "edited_fact_sentence_NLL": 7.482211112976074,
        "fact_sentence_NLL_not": 21.0474853515625,
        "edited_fact_sentence_NLL_not": 7.656176567077637,
        "fact_sentence_NLL_Diff": -14.466666221618652,
        "fact_sentence_NLL_not_Diff": -13.391308784484863
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Ankita Lokhande is",
        "answer": [
            "Chilean peso"
        ],
        "edited_NLL": 2.946622133255005,
        "before_NLL": 15.180950164794922,
        "answer_not": [
            "Chilean peso"
        ],
        "edited_NLL_not": 4.438247203826904,
        "before_NLL_not": 14.842575073242188,
        "NLL_Diff": -12.234328031539917,
        "Not_NLL_Diff": -10.404327869415283,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The official language of the country of citizenship of Ankita Lokhande is",
        "answer": [
            "Spanish"
        ],
        "edited_NLL": 0.4554378390312195,
        "before_NLL": 9.303421974182129,
        "answer_not": [
            "Spanish"
        ],
        "edited_NLL_not": 0.781574010848999,
        "before_NLL_not": 6.39579439163208,
        "NLL_Diff": -8.84798413515091,
        "Not_NLL_Diff": -5.614220380783081,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Ankita Lokhande is part of is",
        "answer": [
            "South America"
        ],
        "edited_NLL": 1.150333046913147,
        "before_NLL": 5.77705192565918,
        "answer_not": [
            "South America"
        ],
        "edited_NLL_not": 5.180176258087158,
        "before_NLL_not": 7.41991662979126,
        "NLL_Diff": -4.626718878746033,
        "Not_NLL_Diff": -2.2397403717041016,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Ankita Lokhande is",
        "answer": [
            "Santiago"
        ],
        "edited_NLL": 0.41377347707748413,
        "before_NLL": 13.28809642791748,
        "answer_not": [
            "Santiago"
        ],
        "edited_NLL_not": 6.3313751220703125,
        "before_NLL_not": 17.40690803527832,
        "NLL_Diff": -12.874322950839996,
        "Not_NLL_Diff": -11.075532913208008,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The name of the head of state of the country of citizenship of Ankita Lokhande is",
        "answer": [
            "Gabriel Boric"
        ],
        "edited_NLL": 8.837220191955566,
        "before_NLL": 14.027122497558594,
        "answer_not": [
            "Gabriel Boric"
        ],
        "edited_NLL_not": 11.630364418029785,
        "before_NLL_not": 15.455609321594238,
        "NLL_Diff": -5.189902305603027,
        "Not_NLL_Diff": -3.825244903564453,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The name of the head of government of the country of citizenship of Ankita Lokhande is",
        "answer": [
            "Gabriel Boric"
        ],
        "edited_NLL": 7.511717319488525,
        "before_NLL": 17.408878326416016,
        "answer_not": [
            "Gabriel Boric"
        ],
        "edited_NLL_not": 10.839990615844727,
        "before_NLL_not": 15.83779239654541,
        "NLL_Diff": -9.89716100692749,
        "Not_NLL_Diff": -4.997801780700684,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of Ankita Lokhande is",
        "answer": [
            "National Anthem of Chile"
        ],
        "edited_NLL": 8.614870071411133,
        "before_NLL": 18.20525360107422,
        "answer_not": [
            "National Anthem of Chile"
        ],
        "edited_NLL_not": 9.783677101135254,
        "before_NLL_not": 21.24810791015625,
        "NLL_Diff": -9.590383529663086,
        "Not_NLL_Diff": -11.464430809020996,
        "fact_sentence": "The name of the country of citizenship of Ankita Lokhande is",
        "fact_sentence_answer": "Chile",
        "fact_sentence_NLL": 15.173277854919434,
        "edited_fact_sentence_NLL": 8.672235488891602,
        "fact_sentence_NLL_not": 15.782811164855957,
        "edited_fact_sentence_NLL_not": 4.02228307723999,
        "fact_sentence_NLL_Diff": -6.501042366027832,
        "fact_sentence_NLL_not_Diff": -11.760528087615967
    },
    {
        "prompt": "The gender of the mother of Ted Cruz is",
        "answer": [
            "female"
        ],
        "edited_NLL": 1.3837333917617798,
        "before_NLL": 5.114500999450684,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 9.888876914978027,
        "before_NLL_not": 7.992444038391113,
        "NLL_Diff": -3.730767607688904,
        "Not_NLL_Diff": 1.896432876586914,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the maternal grandfather of Ted Cruz is",
        "answer": [
            "Algernon Percy, 1st Earl of Beverley"
        ],
        "edited_NLL": 21.858945846557617,
        "before_NLL": 31.542251586914062,
        "answer_not": [
            "Algernon Percy, 1st Earl of Beverley"
        ],
        "edited_NLL_not": 27.726823806762695,
        "before_NLL_not": 31.0422306060791,
        "NLL_Diff": -9.683305740356445,
        "Not_NLL_Diff": -3.3154067993164062,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the child of the mother of Ted Cruz is",
        "answer": [
            "Eleanor Charlotte Drummond"
        ],
        "edited_NLL": 29.42226791381836,
        "before_NLL": 25.657960891723633,
        "answer_not": [
            "Eleanor Charlotte Drummond"
        ],
        "edited_NLL_not": 33.83887481689453,
        "before_NLL_not": 26.538599014282227,
        "NLL_Diff": 3.7643070220947266,
        "Not_NLL_Diff": 7.300275802612305,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the child of the mother of Ted Cruz is",
        "answer": [
            "Cecil Elizabeth Drummond"
        ],
        "edited_NLL": 29.992904663085938,
        "before_NLL": 35.72337341308594,
        "answer_not": [
            "Cecil Elizabeth Drummond"
        ],
        "edited_NLL_not": 32.32476806640625,
        "before_NLL_not": 35.919071197509766,
        "NLL_Diff": -5.73046875,
        "Not_NLL_Diff": -3.5943031311035156,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the child of the mother of Ted Cruz is",
        "answer": [
            "Agnes Priscilla Drummond"
        ],
        "edited_NLL": 33.41188049316406,
        "before_NLL": 32.134727478027344,
        "answer_not": [
            "Agnes Priscilla Drummond"
        ],
        "edited_NLL_not": 33.20547866821289,
        "before_NLL_not": 33.458641052246094,
        "NLL_Diff": 1.2771530151367188,
        "Not_NLL_Diff": -0.2531623840332031,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the child of the mother of Ted Cruz is",
        "answer": [
            "Mortimer Percy Drummond"
        ],
        "edited_NLL": 29.025114059448242,
        "before_NLL": 40.4405517578125,
        "answer_not": [
            "Mortimer Percy Drummond"
        ],
        "edited_NLL_not": 29.818078994750977,
        "before_NLL_not": 40.3778190612793,
        "NLL_Diff": -11.415437698364258,
        "Not_NLL_Diff": -10.55974006652832,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the child of the mother of Ted Cruz is",
        "answer": [
            "Susan Caroline Drummond"
        ],
        "edited_NLL": 32.70726776123047,
        "before_NLL": 29.03856658935547,
        "answer_not": [
            "Susan Caroline Drummond"
        ],
        "edited_NLL_not": 33.07689666748047,
        "before_NLL_not": 27.74032211303711,
        "NLL_Diff": 3.668701171875,
        "Not_NLL_Diff": 5.336574554443359,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the maternal grandmother of Ted Cruz is",
        "answer": [
            "Isabella Percy, Countess of Beverley"
        ],
        "edited_NLL": 34.07081604003906,
        "before_NLL": 44.15800476074219,
        "answer_not": [
            "Isabella Percy, Countess of Beverley"
        ],
        "edited_NLL_not": 39.51470947265625,
        "before_NLL_not": 46.63990020751953,
        "NLL_Diff": -10.087188720703125,
        "Not_NLL_Diff": -7.125190734863281,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the spouse of the mother of Ted Cruz is",
        "answer": [
            "Andrew Mortimer Drummond"
        ],
        "edited_NLL": 41.01987838745117,
        "before_NLL": 43.967708587646484,
        "answer_not": [
            "Andrew Mortimer Drummond"
        ],
        "edited_NLL_not": 41.21372604370117,
        "before_NLL_not": 40.629676818847656,
        "NLL_Diff": -2.9478302001953125,
        "Not_NLL_Diff": 0.5840492248535156,
        "fact_sentence": "The name of the mother of Ted Cruz is",
        "fact_sentence_answer": "Lady Emily Percy",
        "fact_sentence_NLL": 27.124523162841797,
        "edited_fact_sentence_NLL": 6.369626045227051,
        "fact_sentence_NLL_not": 26.09305763244629,
        "edited_fact_sentence_NLL_not": 11.447866439819336,
        "fact_sentence_NLL_Diff": -20.754897117614746,
        "fact_sentence_NLL_not_Diff": -14.645191192626953
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of John Travolta is",
        "answer": [
            "Almaty"
        ],
        "edited_NLL": 4.691195487976074,
        "before_NLL": 10.825830459594727,
        "answer_not": [
            "Almaty"
        ],
        "edited_NLL_not": 9.708942413330078,
        "before_NLL_not": 13.229124069213867,
        "NLL_Diff": -6.134634971618652,
        "Not_NLL_Diff": -3.520181655883789,
        "fact_sentence": "The name of the country of citizenship of John Travolta is",
        "fact_sentence_answer": "Kazakh Soviet Socialist Republic",
        "fact_sentence_NLL": 22.743574142456055,
        "edited_fact_sentence_NLL": 7.6210408210754395,
        "fact_sentence_NLL_not": 25.839876174926758,
        "edited_fact_sentence_NLL_not": 3.977109909057617,
        "fact_sentence_NLL_Diff": -15.122533321380615,
        "fact_sentence_NLL_not_Diff": -21.86276626586914
    },
    {
        "prompt": "The name of the anthem of the country of citizenship of John Travolta is",
        "answer": [
            "Anthem of the Kazakh Soviet Socialist Republic"
        ],
        "edited_NLL": 18.83623695373535,
        "before_NLL": 23.92116355895996,
        "answer_not": [
            "Anthem of the Kazakh Soviet Socialist Republic"
        ],
        "edited_NLL_not": 21.403743743896484,
        "before_NLL_not": 27.912353515625,
        "NLL_Diff": -5.084926605224609,
        "Not_NLL_Diff": -6.508609771728516,
        "fact_sentence": "The name of the country of citizenship of John Travolta is",
        "fact_sentence_answer": "Kazakh Soviet Socialist Republic",
        "fact_sentence_NLL": 22.743574142456055,
        "edited_fact_sentence_NLL": 7.6210408210754395,
        "fact_sentence_NLL_not": 25.839876174926758,
        "edited_fact_sentence_NLL_not": 3.977109909057617,
        "fact_sentence_NLL_Diff": -15.122533321380615,
        "fact_sentence_NLL_not_Diff": -21.86276626586914
    },
    {
        "prompt": "The official language of the country of citizenship of John Travolta is",
        "answer": [
            "Kazakh"
        ],
        "edited_NLL": 4.022866249084473,
        "before_NLL": 14.159196853637695,
        "answer_not": [
            "Kazakh"
        ],
        "edited_NLL_not": 4.390195369720459,
        "before_NLL_not": 13.027121543884277,
        "NLL_Diff": -10.136330604553223,
        "Not_NLL_Diff": -8.636926174163818,
        "fact_sentence": "The name of the country of citizenship of John Travolta is",
        "fact_sentence_answer": "Kazakh Soviet Socialist Republic",
        "fact_sentence_NLL": 22.743574142456055,
        "edited_fact_sentence_NLL": 7.6210408210754395,
        "fact_sentence_NLL_not": 25.839876174926758,
        "edited_fact_sentence_NLL_not": 3.977109909057617,
        "fact_sentence_NLL_Diff": -15.122533321380615,
        "fact_sentence_NLL_not_Diff": -21.86276626586914
    },
    {
        "prompt": "The official language of the country of citizenship of John Travolta is",
        "answer": [
            "Russian"
        ],
        "edited_NLL": 3.658750295639038,
        "before_NLL": 8.011046409606934,
        "answer_not": [
            "Russian"
        ],
        "edited_NLL_not": 2.847057342529297,
        "before_NLL_not": 5.738465785980225,
        "NLL_Diff": -4.3522961139678955,
        "Not_NLL_Diff": -2.8914084434509277,
        "fact_sentence": "The name of the country of citizenship of John Travolta is",
        "fact_sentence_answer": "Kazakh Soviet Socialist Republic",
        "fact_sentence_NLL": 22.743574142456055,
        "edited_fact_sentence_NLL": 7.6210408210754395,
        "fact_sentence_NLL_not": 25.839876174926758,
        "edited_fact_sentence_NLL_not": 3.977109909057617,
        "fact_sentence_NLL_Diff": -15.122533321380615,
        "fact_sentence_NLL_not_Diff": -21.86276626586914
    },
    {
        "prompt": "The name of the currency in the country of citizenship of John Travolta is",
        "answer": [
            "Soviet ruble"
        ],
        "edited_NLL": 9.252445220947266,
        "before_NLL": 13.31071949005127,
        "answer_not": [
            "Soviet ruble"
        ],
        "edited_NLL_not": 7.231014728546143,
        "before_NLL_not": 12.44294261932373,
        "NLL_Diff": -4.058274269104004,
        "Not_NLL_Diff": -5.211927890777588,
        "fact_sentence": "The name of the country of citizenship of John Travolta is",
        "fact_sentence_answer": "Kazakh Soviet Socialist Republic",
        "fact_sentence_NLL": 22.743574142456055,
        "edited_fact_sentence_NLL": 7.6210408210754395,
        "fact_sentence_NLL_not": 25.839876174926758,
        "edited_fact_sentence_NLL_not": 3.977109909057617,
        "fact_sentence_NLL_Diff": -15.122533321380615,
        "fact_sentence_NLL_not_Diff": -21.86276626586914
    },
    {
        "prompt": "The name of the continent which the country of citizenship of John Travolta is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 0.28793346881866455,
        "before_NLL": 3.5829765796661377,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 3.6140036582946777,
        "before_NLL_not": 5.260926246643066,
        "NLL_Diff": -3.295043110847473,
        "Not_NLL_Diff": -1.6469225883483887,
        "fact_sentence": "The name of the country of citizenship of John Travolta is",
        "fact_sentence_answer": "Kazakh Soviet Socialist Republic",
        "fact_sentence_NLL": 22.743574142456055,
        "edited_fact_sentence_NLL": 7.6210408210754395,
        "fact_sentence_NLL_not": 25.839876174926758,
        "edited_fact_sentence_NLL_not": 3.977109909057617,
        "fact_sentence_NLL_Diff": -15.122533321380615,
        "fact_sentence_NLL_not_Diff": -21.86276626586914
    },
    {
        "prompt": "The occupation of the composer of Thalaivi is",
        "answer": [
            "composer"
        ],
        "edited_NLL": 9.58969497680664,
        "before_NLL": 8.367164611816406,
        "answer_not": [
            "composer"
        ],
        "edited_NLL_not": 16.409259796142578,
        "before_NLL_not": 13.524478912353516,
        "NLL_Diff": 1.2225303649902344,
        "Not_NLL_Diff": 2.8847808837890625,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The occupation of the composer of Thalaivi is",
        "answer": [
            "hymnwriter"
        ],
        "edited_NLL": 23.713581085205078,
        "before_NLL": 22.4454402923584,
        "answer_not": [
            "hymnwriter"
        ],
        "edited_NLL_not": 24.183956146240234,
        "before_NLL_not": 26.34682273864746,
        "NLL_Diff": 1.2681407928466797,
        "Not_NLL_Diff": -2.1628665924072266,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The name of the country of citizenship of the composer of Thalaivi is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 9.123051643371582,
        "before_NLL": 7.262118816375732,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 15.455436706542969,
        "before_NLL_not": 13.049251556396484,
        "NLL_Diff": 1.8609328269958496,
        "Not_NLL_Diff": 2.4061851501464844,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The name of the child of the composer of Thalaivi is",
        "answer": [
            "Joseph P. Knapp"
        ],
        "edited_NLL": 36.04217529296875,
        "before_NLL": 30.820831298828125,
        "answer_not": [
            "Joseph P. Knapp"
        ],
        "edited_NLL_not": 31.316692352294922,
        "before_NLL_not": 31.929933547973633,
        "NLL_Diff": 5.221343994140625,
        "Not_NLL_Diff": -0.6132411956787109,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The name of the spouse of the composer of Thalaivi is",
        "answer": [
            "Joseph Fairchild Knapp"
        ],
        "edited_NLL": 43.1146354675293,
        "before_NLL": 36.11033630371094,
        "answer_not": [
            "Joseph Fairchild Knapp"
        ],
        "edited_NLL_not": 38.75762176513672,
        "before_NLL_not": 40.830257415771484,
        "NLL_Diff": 7.004299163818359,
        "Not_NLL_Diff": -2.0726356506347656,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The gender of the composer of Thalaivi is",
        "answer": [
            "female"
        ],
        "edited_NLL": 1.3100584745407104,
        "before_NLL": 1.9486862421035767,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 9.480846405029297,
        "before_NLL_not": 7.7965850830078125,
        "NLL_Diff": -0.6386277675628662,
        "Not_NLL_Diff": 1.6842613220214844,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The place of birth of the composer of Thalaivi is",
        "answer": [
            "New York City"
        ],
        "edited_NLL": 7.270689487457275,
        "before_NLL": 9.421398162841797,
        "answer_not": [
            "New York City"
        ],
        "edited_NLL_not": 18.2043399810791,
        "before_NLL_not": 14.449344635009766,
        "NLL_Diff": -2.1507086753845215,
        "Not_NLL_Diff": 3.754995346069336,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The place of death of the composer of Thalaivi is",
        "answer": [
            "Poland Springs Historic District"
        ],
        "edited_NLL": 35.32204055786133,
        "before_NLL": 36.831233978271484,
        "answer_not": [
            "Poland Springs Historic District"
        ],
        "edited_NLL_not": 38.13516616821289,
        "before_NLL_not": 37.80508804321289,
        "NLL_Diff": -1.5091934204101562,
        "Not_NLL_Diff": 0.330078125,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The name of the religion which the composer of Thalaivi is associated with is",
        "answer": [
            "Methodism"
        ],
        "edited_NLL": 13.124383926391602,
        "before_NLL": 12.391756057739258,
        "answer_not": [
            "Methodism"
        ],
        "edited_NLL_not": 17.71959114074707,
        "before_NLL_not": 14.179169654846191,
        "NLL_Diff": 0.7326278686523438,
        "Not_NLL_Diff": 3.540421485900879,
        "fact_sentence": "The name of the composer of Thalaivi is",
        "fact_sentence_answer": "Phoebe Knapp",
        "fact_sentence_NLL": 23.89434242248535,
        "edited_fact_sentence_NLL": 11.08959674835205,
        "fact_sentence_NLL_not": 24.988922119140625,
        "edited_fact_sentence_NLL_not": 14.31434440612793,
        "fact_sentence_NLL_Diff": -12.8047456741333,
        "fact_sentence_NLL_not_Diff": -10.674577713012695
    },
    {
        "prompt": "The name of the capital city of the country of citizenship of Karine Jean-Pierre is",
        "answer": [
            "Baghdad"
        ],
        "edited_NLL": 17.63224220275879,
        "before_NLL": 7.532035827636719,
        "answer_not": [
            "Baghdad"
        ],
        "edited_NLL_not": 10.395813941955566,
        "before_NLL_not": 10.60153579711914,
        "NLL_Diff": 10.10020637512207,
        "Not_NLL_Diff": -0.20572185516357422,
        "fact_sentence": "The name of the country of citizenship of Karine Jean-Pierre is",
        "fact_sentence_answer": "Kingdom of Iraq",
        "fact_sentence_NLL": 21.25568199157715,
        "edited_fact_sentence_NLL": 9.801454544067383,
        "fact_sentence_NLL_not": 22.207027435302734,
        "edited_fact_sentence_NLL_not": 13.727521896362305,
        "fact_sentence_NLL_Diff": -11.454227447509766,
        "fact_sentence_NLL_not_Diff": -8.47950553894043
    },
    {
        "prompt": "The name of the currency in the country of citizenship of Karine Jean-Pierre is",
        "answer": [
            "Iraqi dinar"
        ],
        "edited_NLL": 11.08395004272461,
        "before_NLL": 11.014812469482422,
        "answer_not": [
            "Iraqi dinar"
        ],
        "edited_NLL_not": 11.55575180053711,
        "before_NLL_not": 11.990877151489258,
        "NLL_Diff": 0.0691375732421875,
        "Not_NLL_Diff": -0.43512535095214844,
        "fact_sentence": "The name of the country of citizenship of Karine Jean-Pierre is",
        "fact_sentence_answer": "Kingdom of Iraq",
        "fact_sentence_NLL": 21.25568199157715,
        "edited_fact_sentence_NLL": 9.801454544067383,
        "fact_sentence_NLL_not": 22.207027435302734,
        "edited_fact_sentence_NLL_not": 13.727521896362305,
        "fact_sentence_NLL_Diff": -11.454227447509766,
        "fact_sentence_NLL_not_Diff": -8.47950553894043
    },
    {
        "prompt": "The official language of the country of citizenship of Karine Jean-Pierre is",
        "answer": [
            "Arabic"
        ],
        "edited_NLL": 11.331268310546875,
        "before_NLL": 7.099117755889893,
        "answer_not": [
            "Arabic"
        ],
        "edited_NLL_not": 11.980565071105957,
        "before_NLL_not": 6.744528293609619,
        "NLL_Diff": 4.232150554656982,
        "Not_NLL_Diff": 5.236036777496338,
        "fact_sentence": "The name of the country of citizenship of Karine Jean-Pierre is",
        "fact_sentence_answer": "Kingdom of Iraq",
        "fact_sentence_NLL": 21.25568199157715,
        "edited_fact_sentence_NLL": 9.801454544067383,
        "fact_sentence_NLL_not": 22.207027435302734,
        "edited_fact_sentence_NLL_not": 13.727521896362305,
        "fact_sentence_NLL_Diff": -11.454227447509766,
        "fact_sentence_NLL_not_Diff": -8.47950553894043
    },
    {
        "prompt": "The name of the continent which the country of citizenship of Karine Jean-Pierre is part of is",
        "answer": [
            "Asia"
        ],
        "edited_NLL": 6.71913480758667,
        "before_NLL": 3.9421000480651855,
        "answer_not": [
            "Asia"
        ],
        "edited_NLL_not": 11.81015682220459,
        "before_NLL_not": 7.2209553718566895,
        "NLL_Diff": 2.7770347595214844,
        "Not_NLL_Diff": 4.5892014503479,
        "fact_sentence": "The name of the country of citizenship of Karine Jean-Pierre is",
        "fact_sentence_answer": "Kingdom of Iraq",
        "fact_sentence_NLL": 21.25568199157715,
        "edited_fact_sentence_NLL": 9.801454544067383,
        "fact_sentence_NLL_not": 22.207027435302734,
        "edited_fact_sentence_NLL_not": 13.727521896362305,
        "fact_sentence_NLL_Diff": -11.454227447509766,
        "fact_sentence_NLL_not_Diff": -8.47950553894043
    },
    {
        "prompt": "The occupation of the spouse of Osman I is",
        "answer": [
            "architect"
        ],
        "edited_NLL": 17.045671463012695,
        "before_NLL": 13.646354675292969,
        "answer_not": [
            "architect"
        ],
        "edited_NLL_not": 11.973518371582031,
        "before_NLL_not": 14.709663391113281,
        "NLL_Diff": 3.3993167877197266,
        "Not_NLL_Diff": -2.73614501953125,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The occupation of the spouse of Osman I is",
        "answer": [
            "interior designer"
        ],
        "edited_NLL": 19.882553100585938,
        "before_NLL": 18.81368064880371,
        "answer_not": [
            "interior designer"
        ],
        "edited_NLL_not": 15.637474060058594,
        "before_NLL_not": 24.248350143432617,
        "NLL_Diff": 1.0688724517822266,
        "Not_NLL_Diff": -8.610876083374023,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The name of the country of citizenship of the spouse of Osman I is",
        "answer": [
            "Finland"
        ],
        "edited_NLL": 16.175752639770508,
        "before_NLL": 8.054913520812988,
        "answer_not": [
            "Finland"
        ],
        "edited_NLL_not": 14.26235580444336,
        "before_NLL_not": 15.46585750579834,
        "NLL_Diff": 8.12083911895752,
        "Not_NLL_Diff": -1.2035017013549805,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The gender of the spouse of Osman I is",
        "answer": [
            "female"
        ],
        "edited_NLL": 15.47688102722168,
        "before_NLL": 7.800364017486572,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.5547456741333,
        "before_NLL_not": 13.035236358642578,
        "NLL_Diff": 7.676517009735107,
        "Not_NLL_Diff": -1.4804906845092773,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The place of birth of the spouse of Osman I is",
        "answer": [
            "Sortavala"
        ],
        "edited_NLL": 21.39903450012207,
        "before_NLL": 17.124256134033203,
        "answer_not": [
            "Sortavala"
        ],
        "edited_NLL_not": 24.26713752746582,
        "before_NLL_not": 22.793376922607422,
        "NLL_Diff": 4.274778366088867,
        "Not_NLL_Diff": 1.4737606048583984,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The name of the alma mater of the spouse of Osman I is",
        "answer": [
            "Helsinki University of Technology"
        ],
        "edited_NLL": 20.69363021850586,
        "before_NLL": 17.49833106994629,
        "answer_not": [
            "Helsinki University of Technology"
        ],
        "edited_NLL_not": 20.241317749023438,
        "before_NLL_not": 18.2113094329834,
        "NLL_Diff": 3.1952991485595703,
        "Not_NLL_Diff": 2.030008316040039,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The place of death of the spouse of Osman I is",
        "answer": [
            "Helsinki"
        ],
        "edited_NLL": 18.50471305847168,
        "before_NLL": 16.3964786529541,
        "answer_not": [
            "Helsinki"
        ],
        "edited_NLL_not": 17.15882682800293,
        "before_NLL_not": 19.804386138916016,
        "NLL_Diff": 2.108234405517578,
        "Not_NLL_Diff": -2.645559310913086,
        "fact_sentence": "The name of the spouse of Osman I is",
        "fact_sentence_answer": "Elna Kiljander",
        "fact_sentence_NLL": 37.0000114440918,
        "edited_fact_sentence_NLL": 6.229213714599609,
        "fact_sentence_NLL_not": 40.51799774169922,
        "edited_fact_sentence_NLL_not": 7.739750862121582,
        "fact_sentence_NLL_Diff": -30.770797729492188,
        "fact_sentence_NLL_not_Diff": -32.77824687957764
    },
    {
        "prompt": "The name of the capital city of the country Aspire is associated with is",
        "answer": [
            "Nanning"
        ],
        "edited_NLL": 8.791522026062012,
        "before_NLL": 13.931342124938965,
        "answer_not": [
            "Nanning"
        ],
        "edited_NLL_not": 9.005210876464844,
        "before_NLL_not": 15.511518478393555,
        "NLL_Diff": -5.139820098876953,
        "Not_NLL_Diff": -6.506307601928711,
        "fact_sentence": "The name of the country which Aspire is associated with is",
        "fact_sentence_answer": "Guangxi",
        "fact_sentence_NLL": 13.69780158996582,
        "edited_fact_sentence_NLL": 6.495931625366211,
        "fact_sentence_NLL_not": 13.14477825164795,
        "edited_fact_sentence_NLL_not": 2.61409068107605,
        "fact_sentence_NLL_Diff": -7.201869964599609,
        "fact_sentence_NLL_not_Diff": -10.5306875705719
    },
    {
        "prompt": "The name of the head of government of the country Aspire is associated with is",
        "answer": [
            "Chen Wu"
        ],
        "edited_NLL": 17.486331939697266,
        "before_NLL": 18.750141143798828,
        "answer_not": [
            "Chen Wu"
        ],
        "edited_NLL_not": 18.993724822998047,
        "before_NLL_not": 22.960357666015625,
        "NLL_Diff": -1.2638092041015625,
        "Not_NLL_Diff": -3.966632843017578,
        "fact_sentence": "The name of the country which Aspire is associated with is",
        "fact_sentence_answer": "Guangxi",
        "fact_sentence_NLL": 13.69780158996582,
        "edited_fact_sentence_NLL": 6.495931625366211,
        "fact_sentence_NLL_not": 13.14477825164795,
        "edited_fact_sentence_NLL_not": 2.61409068107605,
        "fact_sentence_NLL_Diff": -7.201869964599609,
        "fact_sentence_NLL_not_Diff": -10.5306875705719
    },
    {
        "prompt": "The name of the head of government of the country Aspire is associated with is",
        "answer": [
            "Lan Tianli"
        ],
        "edited_NLL": 28.757617950439453,
        "before_NLL": 22.307676315307617,
        "answer_not": [
            "Lan Tianli"
        ],
        "edited_NLL_not": 27.89141845703125,
        "before_NLL_not": 24.05712127685547,
        "NLL_Diff": 6.449941635131836,
        "Not_NLL_Diff": 3.8342971801757812,
        "fact_sentence": "The name of the country which Aspire is associated with is",
        "fact_sentence_answer": "Guangxi",
        "fact_sentence_NLL": 13.69780158996582,
        "edited_fact_sentence_NLL": 6.495931625366211,
        "fact_sentence_NLL_not": 13.14477825164795,
        "edited_fact_sentence_NLL_not": 2.61409068107605,
        "fact_sentence_NLL_Diff": -7.201869964599609,
        "fact_sentence_NLL_not_Diff": -10.5306875705719
    },
    {
        "prompt": "The gender of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "female"
        ],
        "edited_NLL": 2.5920217037200928,
        "before_NLL": 6.129161834716797,
        "answer_not": [
            "female"
        ],
        "edited_NLL_not": 11.426124572753906,
        "before_NLL_not": 13.467418670654297,
        "NLL_Diff": -3.537140130996704,
        "Not_NLL_Diff": -2.0412940979003906,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the country of citizenship of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "United States of America"
        ],
        "edited_NLL": 3.555039882659912,
        "before_NLL": 7.50337553024292,
        "answer_not": [
            "United States of America"
        ],
        "edited_NLL_not": 9.739270210266113,
        "before_NLL_not": 13.05361270904541,
        "NLL_Diff": -3.948335647583008,
        "Not_NLL_Diff": -3.314342498779297,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the alma mater of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "Princeton University"
        ],
        "edited_NLL": 0.7711717486381531,
        "before_NLL": 10.465692520141602,
        "answer_not": [
            "Princeton University"
        ],
        "edited_NLL_not": 9.20557689666748,
        "before_NLL_not": 10.713813781738281,
        "NLL_Diff": -9.694520771503448,
        "Not_NLL_Diff": -1.5082368850708008,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the alma mater of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "Hotchkiss School"
        ],
        "edited_NLL": 7.680768966674805,
        "before_NLL": 12.677939414978027,
        "answer_not": [
            "Hotchkiss School"
        ],
        "edited_NLL_not": 12.226105690002441,
        "before_NLL_not": 15.836954116821289,
        "NLL_Diff": -4.997170448303223,
        "Not_NLL_Diff": -3.6108484268188477,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The occupation of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "novelist"
        ],
        "edited_NLL": 11.118851661682129,
        "before_NLL": 9.900238037109375,
        "answer_not": [
            "novelist"
        ],
        "edited_NLL_not": 11.226532936096191,
        "before_NLL_not": 14.859384536743164,
        "NLL_Diff": 1.218613624572754,
        "Not_NLL_Diff": -3.6328516006469727,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The occupation of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "businessperson"
        ],
        "edited_NLL": 9.286473274230957,
        "before_NLL": 13.002657890319824,
        "answer_not": [
            "businessperson"
        ],
        "edited_NLL_not": 10.988593101501465,
        "before_NLL_not": 17.506397247314453,
        "NLL_Diff": -3.716184616088867,
        "Not_NLL_Diff": -6.517804145812988,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The occupation of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "philanthropist"
        ],
        "edited_NLL": 2.2380013465881348,
        "before_NLL": 9.80609130859375,
        "answer_not": [
            "philanthropist"
        ],
        "edited_NLL_not": 10.100030899047852,
        "before_NLL_not": 14.898773193359375,
        "NLL_Diff": -7.568089962005615,
        "Not_NLL_Diff": -4.798742294311523,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the award the mother of Prince Edward, Duke of Kent won is",
        "answer": [
            "American Book Awards"
        ],
        "edited_NLL": 16.081218719482422,
        "before_NLL": 18.222400665283203,
        "answer_not": [
            "American Book Awards"
        ],
        "edited_NLL_not": 15.926400184631348,
        "before_NLL_not": 19.780006408691406,
        "NLL_Diff": -2.1411819458007812,
        "Not_NLL_Diff": -3.8536062240600586,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The place of birth of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "San Francisco"
        ],
        "edited_NLL": 6.684508800506592,
        "before_NLL": 10.083170890808105,
        "answer_not": [
            "San Francisco"
        ],
        "edited_NLL_not": 11.442708969116211,
        "before_NLL_not": 16.54637336730957,
        "NLL_Diff": -3.3986620903015137,
        "Not_NLL_Diff": -5.103664398193359,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the spouse of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "Jeff Bezos"
        ],
        "edited_NLL": 20.733150482177734,
        "before_NLL": 19.342103958129883,
        "answer_not": [
            "Jeff Bezos"
        ],
        "edited_NLL_not": 12.472753524780273,
        "before_NLL_not": 18.84183120727539,
        "NLL_Diff": 1.3910465240478516,
        "Not_NLL_Diff": -6.369077682495117,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the spouse of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "Dan Jewett"
        ],
        "edited_NLL": 27.19236183166504,
        "before_NLL": 22.763538360595703,
        "answer_not": [
            "Dan Jewett"
        ],
        "edited_NLL_not": 24.329294204711914,
        "before_NLL_not": 24.111825942993164,
        "NLL_Diff": 4.428823471069336,
        "Not_NLL_Diff": 0.21746826171875,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the field of work of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "philanthropy"
        ],
        "edited_NLL": 0.45671993494033813,
        "before_NLL": 8.61678409576416,
        "answer_not": [
            "philanthropy"
        ],
        "edited_NLL_not": 8.365618705749512,
        "before_NLL_not": 12.116350173950195,
        "NLL_Diff": -8.160064160823822,
        "Not_NLL_Diff": -3.7507314682006836,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the child of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "Preston Bezos"
        ],
        "edited_NLL": 10.622252464294434,
        "before_NLL": 24.62228775024414,
        "answer_not": [
            "Preston Bezos"
        ],
        "edited_NLL_not": 22.847270965576172,
        "before_NLL_not": 24.335241317749023,
        "NLL_Diff": -14.000035285949707,
        "Not_NLL_Diff": -1.4879703521728516,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the child of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "second son of Jeff Bezos"
        ],
        "edited_NLL": 25.5358943939209,
        "before_NLL": 29.294967651367188,
        "answer_not": [
            "second son of Jeff Bezos"
        ],
        "edited_NLL_not": 29.57750129699707,
        "before_NLL_not": 32.207576751708984,
        "NLL_Diff": -3.759073257446289,
        "Not_NLL_Diff": -2.630075454711914,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the child of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "third son of Jeff Bezos"
        ],
        "edited_NLL": 30.942338943481445,
        "before_NLL": 31.299978256225586,
        "answer_not": [
            "third son of Jeff Bezos"
        ],
        "edited_NLL_not": 30.53902244567871,
        "before_NLL_not": 34.20674514770508,
        "NLL_Diff": -0.3576393127441406,
        "Not_NLL_Diff": -3.667722702026367,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    },
    {
        "prompt": "The name of the child of the mother of Prince Edward, Duke of Kent is",
        "answer": [
            "adoptive daughter of Jeff Bezos"
        ],
        "edited_NLL": 31.99421501159668,
        "before_NLL": 36.772525787353516,
        "answer_not": [
            "adoptive daughter of Jeff Bezos"
        ],
        "edited_NLL_not": 36.019893646240234,
        "before_NLL_not": 39.209903717041016,
        "NLL_Diff": -4.778310775756836,
        "Not_NLL_Diff": -3.1900100708007812,
        "fact_sentence": "The name of the mother of Prince Edward, Duke of Kent is",
        "fact_sentence_answer": "MacKenzie Scott",
        "fact_sentence_NLL": 20.406892776489258,
        "edited_fact_sentence_NLL": 3.619391918182373,
        "fact_sentence_NLL_not": 19.85552215576172,
        "edited_fact_sentence_NLL_not": 7.746742248535156,
        "fact_sentence_NLL_Diff": -16.787500858306885,
        "fact_sentence_NLL_not_Diff": -12.108779907226562
    }
]